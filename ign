\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    {Вычисление  обобщённых оценок объектов и иерархическая группировка признаков}
\author
       { Игнатьев~Н.\,А.}% основной список авторов, выводимый в оглавление
    [Игнатьев~Н.\,А.$^1$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    {n\_ignatev@rambler.ru}
\organization
    {$^1$Национальный университет Узбекистана, Ташкент}
\abstract
   { Рассматривается процесс формирование нового признакового пространства, размерность которого
меньше исходного.  
Предлагается последовательный  отбор непересекающихся подмножеств 
разнотипных признаков в описании объектов  и нелинейное отображения  их на числовую ось.
 При  отборе используется правило иерархической группировки  для попарного
объединения признаков.
 Решение принимается по значениям степени  размытости  результатов отображения
 объектов классов на числовой оси.

\bigskip
\textbf{Ключевые слова}: {обобщённые оценки; иерархическая группировка; нелинейная~проекция; 
логические закономерности; отступ}.}

\titleEng
    {Computation   generalized estimates of objects  and    hierarchical clustering   of  features}
\authorEng
    {Ignat'ev~N.\,A.$^1$}
\organizationEng
    {$^1$National University of Uzbekistan, Tashkent}
\abstractEng
    {        \textbf{Background}:    
We consider the process of forming a new feature space whose dimensionality is smaller than the original. 
In known methods for this purpose used a linear transformation of the space or predefined functions. 
The choice of these functions is made on the basis of heuristic considerations.

        \textbf{Methods}: It is suggested sequential selection of nonintersecting subsets of heterogeneous features in objects 
description and their nonlinear mapping on the real axis. For the selection is used rule for hierarchical 
clustering  pair combination of features. Decision on association accepted by the degree fuzziness
 of the results displaying objects of classes on the real axis.

        \textbf{Results}: Nonlinear mapping objects description to real axis defined by the combination of features is 
the means of detecting stable logical regularities (new knowledges) in data warehouses. 
Using regularities enables correctly classify of learning objects according to the initial features
 fewer than applying linear decision functions.

        \textbf{Conclusions}: The proposed method enables to solve problems of data mining, converting to produce 
heterogeneous features to quantity the scale of measurement. Can be used in the construction of models 
in various poorly formalized subject areas.
      
  \bigskip
    \textbf{Keywords}: \emph{generalized estimates;  hierarchical clustering; 
nonlinear projection;  logical regularity; margin}.}
\begin{document}
\maketitle
\linenumbers
\section{Введение}
Обобщённые оценки -- это агрегированные (комбинированные) показатели,
  которые в  \cite{ign2} использовались для отображения отношений
между объектами двух классов в разнотипном признаковом пространстве на числовую ось.
Было разработано два метода вычисления оценок: стохастический и детерминистический.
  Критерием  для выбора  параметров алгоритма стохастического метода   служила
 максимальная разность (отступ)  между линейными проекциями  двух объектов
из разных классов.
 Из минимального значения на числовой оси одного класса  вычиталось
   максимальное значение другого класса.
  Одним из применений метода было отображение описаний (визуализация)
объектов  \cite{ign3} на плоскость.

В  алгоритме детерминистического метода применялось разбиение на интервалы доминирования
значений количественных признаков объектов одного из двух классов.
   При вычислении обобщённой оценки объекта использовались значения функций принадлежности
к  интервалам доминирования для количественных признаков и частоты встречаемости
градаций для номинальных  признаков.

Переход к однотипным шкалам измерений и поэтапное сокращение
   размерности  признакового пространства посредством вычисления обобщённых
  оценок объектов описан в  \cite{nur}.
На первом этапе обобщённая оценка объекта по номинальным признакам
 интерпретировалась как значение нового (латентного) количественного признака.
На  втором этапе вычисление оценки производилось по расширенному множеству количественных
признаков.

  Результаты вычислительного эксперимента \cite{nur} по выборке данных  GERMAN из  \cite{asu}
показали, что обобщающая способность  решающих правил на основе  обобщённых оценок   выше чем у
известного метода LDA \cite{lda}.

Потребность во введении латентных признаков  возникает при
  поиске спрямляющего пространства, в котором  объекты из разных классов были бы
линейно разделимы.
 В методе опорных векторов SVM \cite{pot}  нелинейность  разделяющей поверхности
 достигается  за счёт использования  ядерных функций,
поиск параметров дискриминантных функций производится
путём максимизации отступа между объектами двух классов в новом (спрямляющем) признаковом
пространстве.

  В данном исследовании предлагается правило для агломеративной иерархической  группировки
 разнотипных признаков с целью нелинейного отображения их значений в
 описании объектов  на  числовую ось.
  Результаты нелинейного отображения рассматриваются как значения обобщённых оценок
(новых признаков) в описании объектов.
Впервые  предложены  критерии, на основе которых определяются  число обобщённых оценок
 (непересекающихся групп),  количество исходных признаков, входящих в группу, и их состав.
   
   Решающие правила по значениям каждого нового признака в описании объектов
образуют совокупность базовых алгоритмов.
   Базовый алгоритм может рассматриваться как самостоятельный классификатор,
либо использоваться в композиции с другими алгоритмами.

 Вычисление  обобщенных оценок с помощью иерархической
агломеративной группировки целесообразно по нескольким причинам:

-- обобщённые оценки образуют новое признаковое пространство, размеры которого
 меньше исходного;

--  решается проблема использования  алгоритмов классификации,
реализация которых была  неэффективна из-за большой размерности признакового
пространства, либо возможна при  одном типе шкал измерений;

--  в процессе группировки происходит последовательный  отбор информативных наборов признаков;

-- нелинейное отображение описаний объектов на числовую ось по определяемым комбинациям
признаков является средством обнаружения устойчивых логических закономерностей
(новых знаний) в хранилищах данных.

  \section{Обобщенные оценки объектов }

 Рассматривается множество из  $T$ допустимых объектов, разбитое на $2$
непересекающихся подмножества (класса).
Представители классов $K_1,K_2$  заданы через выборку
(подмножество $T$) объектов $E_0=\{S_1,\dots,S_m\}$,  $E_0= K_1\cup K_2 $.
    Объекты выборки описываются с помощью $n$  разнотипных признаков
 $X(n)=(x_1,\dots,x_n)$, множество допустимых значений
$\xi$ из которых измеряются в интервальных шкалах, $n-\xi$ в
 номинальной.

На   $E_0$  задано  правило последовательного разбиения набора $X(n)$ на
непересекающиеся подмножества  $X_1(k_1),\dots,X_\tau(k_\tau), \tau \ge 1$,  $k_1+\dots+k_\tau \le n$.
  Требуется для каждого $X_i(k_i)$ определить  алгоритм $A_i$ ( распознающий оператор
в терминологии алгебраического подхода к распознавания образов Ю.\,И. Журавлёва \cite{gur})
 для отображения значений  признаков из $X_i(k_i)$  в описании объекта $S_j \in E_0$, $j=1,\dots,m$
в значение (обобщённую оценку) на числовой оси.

Обозначим множество номеров количественных  и номинальных  признаков  
соответственно как $I$ и $J$.
  Процесс последовательного вычисления значений обобщённых оценок (новых признаков) реализуется
    алгоритмом   иерархической агломеративной группировки по   описываемому ниже правилу.
 Для идентификации признаков в описании объектов на  $p$-ом шаге  $(0 \le p < n)$ 
      иерархической группировки будем использовать  $\{x_i^p \}_ {i \in  (I \cup J)}$.

В процессе группировки и формирования обобщённых оценок состав элементов
 и мощность множеств $I$ и $J$,  $\mid I \mid  +\mid J \mid \le n$   будут изменятся.
В зависимости от шкал измерений признаков, объединяемых в группы, используются различные способы
вычисления их параметров для отображения на числовую ось.
 Для количественных признаков это производится
 следующим образом.

   Упорядоченное множество   значений признака $x_j^p, j \in I, p \ge 0$ объектов из $E_0$  разделим
  на два интервала  $ [ c_1^{jp},c_2^{jp}],(c_2^{jp},c_3^{jp}] $, каждый из которых
 рассматривается как градация номинального признака. Критерий для определения
границы  $c_2^{jp}$   основывается на проверке гипотезы
(утверждения) о том, что  каждый из двух интервалов содержит значения
 количественного признака объектов только одного класса.

Пусть $u_i^1,u_i^2$ -- количество значений признака $x_j^p, j \in I$ класса
$K_i$, $i=1,2$ соответственно в интервалах $[ c_1^{jp},c_2^{jp}],(c_2^{jp},c_3^{jp}]$,  $\mid K_i \mid  >1$,
$v$ -- порядковый номер элемента  упорядоченной по
возрастанию последовательности $r_{j_1},\dots,r_{j_v},\dots,r_{j_m}$  значений
$x_j^p$ из  $E_0$, определяющий  границы интервалов как
$c_1^{jp}=r_{j_1},c_2^{jp}=r_{j_v},c_3^{jp}=r_{j_m} $.
   Критерий
$$ \left (
\displaystyle \frac{ \sum\limits_{i=1}^2 u_i^1(u_i^1-1)+u_i^2(u_i^2-1)}
{\sum\limits_{i=1}^2 \mid K_i \mid(\mid K_i \mid - 1)} \right )
 \left ( \displaystyle \frac
{\sum\limits_{d=1}^2 \sum\limits_{i=1}^2 u_i^d( \mid K_{3-i} \mid -
 u_{3-i}^d )} {2 \mid K_1 \mid \mid K_2 \mid}
\right ) \rightarrow   \max\limits_{ c_1^{jp}   < c_2^{jp} < c_3^{jp} }
\eqno{(1)}
$$
  позволяет  оценивать значение границы между интервалами
   $[ c_1^{jp},c_2^{jp}],(c_2^{jp},c_3^{jp}]$.
 Выражение в левых скобках (1) представляет  внутриклассовое сходство,
  в правых -- межклассовое различие.

Экстремум критерия (1) используется в качестве веса $w_j^p$  ($0 \le w_j^p \le 1$) признака  $x_j^p$.
При $w_j^p=1$  значения признака   $x_j^p$  у объектов из классов
$K_1,K_2$  не пересекаютя между собой.

 При включении в группу номинального признака с целью
 вычисления обобщённой оценки объектов  требуется определить значение
его   веса  и вкладов  каждой из  градаций.

   Обозначим через $\pi$ -- число градаций признака $x_r^p$,  $r \in J$, $p=0$, $g_{dr}^t $ -- количество 
значений   $t$-й $(1 \leq  t  \leq \pi)$   градации $r$-го признака в описании объектов
 класса $K_d$, $l_{dr}$ -- число градаций  $r$-го признака  в  $K_d,d=1,2$.
   Различие  по $r$-му признаку между классами $K_1$  и  $K_{2}$
  определяется  как величина
$$
 \lambda_{r} =1- \frac{  \sum\limits_{t=1}^\pi
g_{1r}^t g_{2r}^t} {\mid K_1 \mid \mid K_2 \mid}. \eqno{(2)}
$$

  Степень однородности (мера внутриклассового сходства) $\beta_{r} $
   значений    градаций $r$-го признака   по  классам $K_1,K_2$ вычисляется  по формулам:
$$
  D_{dr}=  \left \{
  \begin{array}{ll}
  (\mid K_d \mid - l_{dr}+1 )(\mid K_d \mid - l_{dr}), \pi>2,  \\[2mm]
   \mid K_d \mid (\mid K_d \mid - 1), \pi \le 2;
  \end{array}
\right.
$$
$$
\beta_{r} =\left \{
\begin{array}{ll}
\displaystyle \frac{ \sum\limits_{t=1}^{\pi} g_{1r}^t (g_{1r}^t - 1)+ g_{2r}^t (g_{2r}^t - 1)   }
  {D_{1r}+D_{2r} }, D_{1r}+D_{2r}>0, \\[2mm]
   0,     D_{1r}+D_{2r}=0.
\end{array}
\right.         \eqno{(3)}
    $$
 С помощью  (2),(3)  вес  номинального  признака с $r \in J$  определяется  как
 $$ v_{r} = \lambda_{r} \beta_{r}.   $$

 Очевидно, что  множество  чисел, идентифицирующих
  $\pi$ градаций номинального признака, всегда  можно
   взаимно однозначно отобразить в  множество  $\{1,\dots,\pi \}$.
  С учётом такого отображения  для объекта $S= (a_1,\dots,a_n)$
  вклад признака $a_i=j$, $i \in J$,    $j \in \{1,\dots,\pi \}$ в обобщённую оценку
   определяется величиной
  $$
  \mu_i(j)= v_i \left ( \frac{\alpha_{ij}^1} {\mid K_1 \mid} -
  \frac{\alpha_{ij}^2} {\mid K_2 \mid}  \right ),
    $$
  где $\alpha_{ij}^1,\alpha_{ij}^2$ -- количество значений $j$-й градации
 $i$-го признака  соответственно в классах $K_1$ и  $K_2$, $v_i$ -- вес
 $i$-го  признака.

Значение обобщённой оценки  $b_{rij}^p$  объекта   $S_r=\{ a_{ru}^p\}_{u \in (I  \cup J)}$,  $S_r \in E_0$    по паре
 $(x_i^p, x_j^p)$, $0 \le p < n$,  $i,j \in (I  \cup J), i \ne j $    вычисляется как
$$
  b_{rij}^p=  \left \{
  \begin{array}{ll}
 \mu_i( a_{ri}^p) +  \mu_j(a_{rj}^p), i,j \in J,  \\[2mm]
\mu_i( a_{ri}^p) + t_j w_j^p(a_{rj}^p - c_2^{jp})/(c_3^{jp} - c_1^{jp}), i \in J,j \in I, t_j \in \{-1,1\}, \\[2mm]
  \eta_{ij} (t_iw_i^p(a_{ri}^p - c_2^{ip})/(c_3^{ip} - c_1^{ip}) +
 t_jw_j^p(a_{rj}^p - c_2^{jp})/(c_3^{jp} -\\[2mm]
- c_1^{jp}))
+(1-\eta_{ij} ) t_{ij} w_{ij}^p (a_{ri}^p a_{rj}^p - c_2^{ijp})/(c_3^{ijp} - c_1^{ijp} ),  \\[2mm]
i,j \in I, t_{ij}, t_i, t_j \in \{-1,1\}, \eta_{ij} \in  [0,1],
  \end{array}
\right.       \eqno{(4)}
$$
 где   $ w_i^p $,$w_j^p $,  $ w_{ij}^p$ -- веса признаков, определяемые по
 (1)  соответственно  по множеству значений признаков  $x_i^p$,  $x_j^p$
и их произведения $x_i^px_j^p$,  значения  $t_{ij}, t_i, t_j \in \{-1,1\}$, $\eta_{ij} \in  [0,1]$
выбираются по экстремуму функционала
$$   \varphi(p,i,j) = \frac{\min\limits_{S_r \in K_1} b_{rij}^p -
 \max\limits_{S_r \in K_2} b_{rij}^p}
   { \max\limits_{S_r \in E_0} b_{rij}^p -  \min\limits_{S_r \in E_0} b_{rij}^p  }   = 
\max\limits_{  t_{ij}, t_i, t_j  \in \{-1,1\}, \eta_{ij} \in  [0,1]  }.
 \eqno{(5)}
$$
Значение (5)  интерпретируется как отступ между объектами классов  $K_1$ и  $K_2$.

 Обозначим через  $  \{ z_{ij}^p \}_{i,j \in (I \cup J)}$, $p \ge 0$  --  квадратную матрицу
размера $(n-p) \times (n-p) $, значение  элемента   $z_{ij}^p$
которой определяется как
$$
z_{ij}^p = \left \{
  \begin{array}{ll}
0, i = j,     \\[2mm]
\mbox{значению (1) по } \ \{ b_{rij}^p \}_{r=1}^{m}, i \ne j,
\end{array}
\right.         \eqno{(6)}
$$
 через $G_\eta, \eta>0 $ -- подмножество  номеров  признаков из  $X(n)$.
  Пошаговая  реализация алгоритма итеративной группировки будет такой:

  1 шаг:   $\eta =1$, $G_\eta =\emptyset$, $p=0$,  $ \lambda{c}=0$;

  2 шаг: Вычислить значения элементов матрицы  $\{ z_{ij}^p \}_{i,j \in (I \cup J)}$ по  (6);

   3 шаг:   Вычислить   $ \lambda{n} = \max\limits_{u,v \in (I  \cup J) }  z_{uv}^p$.
 Выделить      $ \Omega= \{(s,t),s,t \in  I  \cup  J \mid z_{st}^p =\lambda{n} \mbox{ and }  s<t \}$.
 Определить  пару  $\{ i,j \}$, $i < j$   как
$$
\{ i,j \}= \left \{
  \begin{array}{ll}
\Omega, \mid \Omega\mid=1,     \\[2mm]
\{ s,t \},  (s,t) \in \Omega \mbox{ and }   \varphi(p,s,t) > \max\limits_{(u,v) \in \Omega \setminus (s,t) } \varphi(p,u,v);
\end{array}
\right.
$$

  4 шаг:  Если $G_\eta =\emptyset$,  то    $G_\eta =\{ i,j \}$,  $Margin=  \varphi(p,i,j) $,  идти 8;

 5 шаг: Если $G_\eta \cap  \{ i,j \}  = \emptyset $, то идти 7;

  6 шаг:  Если  $\lambda{n}  > \lambda{c}$   или
$\lambda{n}  = \lambda{c}$     и
$Margin <  \varphi(p,i,j) $,
     то  $ G_\eta =G_\eta  \cup  \{ i,j \} $, $Margin=  \varphi(p,i,j) $,   идти 8;

7 шаг:    $\eta =\eta+1$,   $G_\eta =\emptyset$. Идти 4;

 8 шаг:   $p=p+1$,  $ I \cup J=(I\cup J) \setminus   max(i,j)  $,   $ I= I \cup min(i,j) $, $k=min(i,j) $,
$ \lambda{c} =\lambda{n} $.
Заменить значения признаков в описании объекта $S_r= \{a_{ru}^{p-1}\}_{u \in (I\cup J)}$, $r=1,\dots,m$  на
 $$
 a_{ru}^p=\left \{
\begin{array}{ll}
 a_{ru}^{p-1}, u \in (I\cup J) \setminus k ,  \\[2mm]
 b_{rij}^p, u=k;
 \end{array}
\right.
$$

   9 шаг: Определить значение
$$
 z_{uv}^p=\left \{
\begin{array}{ll}
z_{uv}^{p-1}, u \in (I\cup J) \setminus k, v \in (I\cup J),  \\[2mm]
\mbox{значению (1) на  }  \{ a_{rv}^p \}_{r=1}^m, u=k, v \in (I\cup J).
\end{array}
\right.
$$
Если $n-p> 1$, то  идти 3;

 10 шаг: Конец.

Через конечное число рекурсивных  обращений к описанному выше алгоритму
 все исходные  признаки сводятся  к  одной нелинейной оценке.
 Исходя из  практических  соображений ограничение на число обобщённых оценок для конкретных
выборок данных  может выбираться по результатам вычислительного эксперимента.

  Рассмотрим пример классификатора на базе  обобщённых оценок (4).
 Пусть  $\{a_{ir}^p\}_{i=1}^m,  p < n$, $r \in I$ -- множество  значений обобщённой оценки
  (признака), вычисленной по (4), и по критерию (1) эти значения разбиты на интервалы
$[c_1,c_2 ],(c_2,c_3]$.
 Для решающего правила  нужно выбрать  порог  равный
$$
  w_0=\frac{c_2 + z}{2},                                             \eqno{(7)}
$$
где    $z$ -- ближайшее к $c_2$  значение из интервала $(c_2,c_3]$.
 Анализ  результатов использования порога (7) в дискриминантных функциях приводится в \cite{nur}.

\section{ Вычислительный эксперимент}

  В качестве материала для  эксперимента  была взята выборка данных
  из  \cite{jam}, описывающая челюсти 30 собак (класс $K_1$) и
 12 волков (класс $K_2$) по следующим 6-ти количественным признакам:

   $x_1-$ (CBL) основная длина;

    $x_2-$ (LUJ) длина верхней челюсти;

    $x_3-$ (WID) ширина верхней челюсти;

    $x_4-$ (LUC) длина верхнего карнивора;

    $x_5-$ (LFM) длина первого верхнего моляра;

    $x_6-$ (WFM) ширина первого верхнего моляра.

 Порядок синтеза значений обобщённых оценок (латентных признаков) из комбинаций
 признаков    по критерию (1) и отступами  между объектами классов по (5) приведён в табл.\,1.
 \begin{table}[t]
   \caption{ Порядок синтеза обобщённых оценок  объектов }
\label{TabExample}
    \centering\medskip%\tabcolsep=2pt%\small
  \begin{tabular}{|c|c|c|}
     \hline
     Комбинации признаков  & Значение критерия (1)&  Отступ между классами (5) \\
       \hline
    1,4   & 1.0000  &  0.0403   \\
    \hline
   1,4,5  & 1.0000   &   0.1060 \\
    \hline
1,4,5,3    & 1.0000   &  0.1233  \\
    \hline
  1,4,5,3,2   &1.0000  & 0.1674    \\
    \hline
1,4,5,3,2,6   &1.0000  & 0.1778    \\
    \hline
   \end{tabular}
 \end{table}

Аналитический вид решающего правила по значениям обобщённой оценки,
 полученной при синтезе признаков $x_1$ и $x_4$
 (см. табл.1) с учётом (7), будет выглядеть так
$$
d(x)=0.4( -0.00374(x_1-221.0) - 0.09538(x_4 - 22.5 )) +  0.00010(x_1x_4 - 5130.0) + 0.01971.
$$

Судя по результатам из табл.\,1   все комбинации исходных признаков попадают в одну группу,
  попарное объединение  признаков в комбинацию удовлетворяет такому свойству 
 как монотонность по значениям отступа между объектами  классов.
  Теоретическое обоснование выполнения монотонности при синтезе комбинаций признаков
на произвольной двухклассовой обучающей выборке требует отдельного рассмотрения.
Возможным вариантом решения проблемы монотонности является обнаружение и исключение из выборки
шумовых объектов.

Для демонстрации того, что различные признаки  в составе обобщённых оценок (4) компенсируют
 недостатки друг друга, воспользуемся  табл.\,2 из  \cite{ign2}.
   Таблица содержит значения границ интервалов  $[c_1^i,c_2^i],(c_2^i,c_3^i ]$   и экстремумы
 критерия (1) для признаков $x_i$, $i=1,\dots,6$.
\begin{table}[t]
   \caption{Результаты оптимизации по критерию (1) }
\label{TabExample}
    \centering\medskip%\tabcolsep=2pt%\small
  \begin{tabular}{|c|c|c|c|c|}
\hline
      & \multicolumn{3}{c|}{Границы интервалов } & Значение \\
\cline{2-4}
        Признак &$c_1^i $ &$c_2^i $& $c_3^i$ & критерия (1)   \\
   \hline
 $x_1$   & 129.000  &221.000  & 255.000 & 0.378  \\
    \hline
 $x_2$   &64.000  & 114.000  &126.000  & 0.389  \\
    \hline
  $x_3$  & 52.000  &76.000  &95.000   & 0.288  \\
    \hline
 $x_4$  &16.700  &22.500  &26.500  &  0.897   \\
    \hline
 $x_5$  &11.200  & 14.700 &  16.800 &  0.625   \\
    \hline
  $x_6$  & 13.000 & 18.300 & 27.000 & 0.800   \\
    \hline
  \end{tabular}
  \end{table}

Согласно \cite{ign2}  точность классификации  по линейным дискриминантным функциям
напрямую зависит от использования признаков   $x_4$ и $x_6$, имеющих
 наибольшие значения весов (см. табл.\,2), равных соответственно 0.897 и 0.800.
 Доказано, что корректное разделение объектов обучения на классы с единичным
значением критерия (1)  возможно лишь на наборах $\{x_1, x_2,x_3,x_4,x_5,x_6 \}$ и
 $\{x_1, x_2,x_4,x_5,x_6 \}$.
 Наилучший результат (см. табл.\,1) в смысле  разделимости по (1) и отступа между
классами (5) по парам признаков был достигнут    при использовании нелинейного
отображения значений из $\{x_1, x_4\}$ в описании объектов на числовую ось.

С помощью критерия (1) преобразуем количественные признаки в номинальные.
 Каждой градации номинального  признака поставим в соответствие один из непересекающихся
интервалов, полученный по (1).
   В табл.\,3 приведены результаты группировки для случая когда  все признаки номинальные,
 в табл.\,4  представлено два подмножества:  $\{x_1,x_2\}$  --  количественных и $\{x_3,x_4,x_5,x_6\}$ --
  номинальных признаков.

 \begin{table}[t]
   \caption{ Группировка номинальных признаков }
\label{TabExample}
    \centering\medskip%\tabcolsep=2pt%\small
  \begin{tabular}{|c|c|c|}
     \hline
     \No  & Состав группы   &Значение критерия (1) \\
       \hline
    1   &  $x_2, x_4,x_5,x_6 $   &  0.8965   \\
    \hline
   2  & $x_1, x_3 $   &   0.3781 \\
    \hline
   \end{tabular}
 \end{table}

\begin{table}[t]
   \caption{ Группировка по разнотипным  признакам }
\label{TabExample}
    \centering\medskip%\tabcolsep=2pt%\small
  \begin{tabular}{|c|c|c|}
     \hline
     \No  & Состав группы   &Значение критерия (1) \\
       \hline
    1   &  $x_1, x_2,x_4,x_5 $   &  1.0000   \\
    \hline
   2  & $x_3, x_6 $   &   0.2884 \\
    \hline
   \end{tabular}
 \end{table}

Анализ результатов из табл.\,1, табл.\,2, табл.\,3  по критерию (1) показывает, что
преобразование значений признаков из количественных (сильных) шкал измерений
в значения номинальной (слабой) шкалы приводят к снижению точности решающих
правил с порогом (7)  на базе обобщённых оценок.

  Для проверки процедурой кросс--валидации обобщающей способности решающих  правил 
 с порогом (7) на базе нелинейных обобщённых оценок с максимальным отступом между классами
 использовалось разделение выборки на обучение и контроль в соотношении $9:1$.
Результаты проверки следующие: точность на обучении -- $100\%$, на контроле -- $98\%$.  

\section{ Заключение   }
Процесс вычисления обобщённых оценок сводится к формированию нового признакового
пространства для   описания допустимых объектов в задачах распознавания образов.
  Практическое применение этих оценок позволяет:

--  находить устойчивые логические закономерности в  базах (хранилищах) данных не прибегая
к перебору всевозможных вариантов;

-- использовать их для реализации дискриминантных функций, решающих списков,
решающих деревьев, алгоритмов вычисления оценок.
  
  Теоретический и практический  интерес представляет  оценка границ допустимых 
значений латентных признаков на основе обобщённых оценок.
  Существует потребность в разработке комплексов программ для извлечения 
новых знаний с учётом нелинейного отображения описаний 
допустимых объектов на числовую ось.

\section{Литература}
\renewcommand{\bibname}{}
\begin{thebibliography}{99}

\bibitem{ign2} 
\BibAuthor{Игнатьев\,Н.\,А.} 
\BibTitle{Вычисление обобщённых показателей и интеллектуальный анализ данных}~// 
\BibJournal{Автоматика и телемеханика}.\,2011. \No\,5. С.\,183--190.

\bibitem{ign3}
 \BibAuthor{Игнатьев\,Н.\,А.}
    \BibTitle{О конструировании признакового пространства для
 поиска логических закономерностей в задачах распознавания образов}~//
    \BibJournal{Вычислительные технологии}.\,2012. Т.\,17, \No\,4. С.\,56--62.

\bibitem{nur}
 \BibAuthor{Игнатьев\,Н.\,А., Нуржонов\,Ш.\,Ю.}
\BibTitle{Выбор параметров регуляризации для повышения
обобщающей способности дискриминантных функций}~//
\BibJournal{Узбекистон Республикаси
Курол Кучлари академиясининг хабарлари}. 2014.  \No\,1(14). C.\,81--87.
   
\bibitem{asu}
\BibAuthor{Asuncion\,A., Newman\,D.\,J.} 
UCI Machine Learning Repository~// University of California, Irvine. 2007. 
\BibUrl{www.ics.uci.edu/~mlearn/MLRepository.html}.

\bibitem{lda}
\BibUrl{http://www.mathworks.com/help/stats/discriminant-analysis.html}.

\bibitem{pot} 
\BibAuthor{Потапов\,А.\,С.}
Технологии искусственного интеллекта. -- СПб:~СПбГУ ИТМО, 2010. 218~с.

 \bibitem{gur} 
\BibAuthor{Журавлёв\,Ю.\,И.} 
  \BibTitle{Об алгебраическом подходе к решению задач распознавания или классификации}~//
 \BibJournal{Проблемы кибернетики}. -- Москва:~Наука, 1978.\,Т.\,33. С.\,5--68.

\bibitem{jam} 
 \BibAuthor{Жамбю\,М.}
 Иерархический кластер-анализ и  соответствия. -- Москва:~Финансы и статистика, 1988. 342~с.
\end{thebibliography}

\section{References}
\renewcommand{\bibname}{}
\begin{thebibliography}{99}

  \bibitem{ign2} 
\BibAuthor{Ignat'ev~N.\,A.}\,2011.
\BibTitle{Calculation of generalized parameters and data mining}~// 
\BibJournal{Automation and Remote Control}. 5:183--190.

\bibitem{ign3}
 \BibAuthor{Ignat'ev~N.\,A.}\,2012.
    \BibTitle{On the construction of the feature space for finding logical regularities in pattern recognition problems}~//
    \BibJournal{Computing Technologies}. 17(4):56--62.
                                                                                               
\bibitem{nur}
 \BibAuthor{Ignat'ev~N.\,A., Nurjonov~Sh.\,U.}\,2014.
\BibTitle{The choice of the regularization parameters for improving the generalization ability of the discriminant functions}~//
\BibJournal{Bulletin of the Academy of the Armed Forces of the Republic of Uzbekistan}.  1(14):81--87.
   
\bibitem{asu}
\BibAuthor{Asuncion~A., Newman~D.\,J.}\,2007.
UCI Machine Learning Repository~// University of California, Irvine.  
\BibUrl{www.ics.uci.edu/~mlearn/MLRepository.html}.

\bibitem{lda}
\BibUrl{http://www.mathworks.com/help/stats/discriminant-analysis.html}.

\bibitem{pot} 
\BibAuthor{Potapov~А.\,S.}\,2010.
Artificial intelligence technology. SPb:~SPbGU ITMO,  218~p.

 \bibitem{gur} 
\BibAuthor{Juravlev~U.\,I.}\,1978.
  \BibTitle{On an algebraic approach to solving the problems of pattern recognition and classification}~//
 \BibJournal{ Problems of Cybernetics }. Moscow:~Nauka, 33:5--68.

\bibitem{jam} 
 \BibAuthor{Jambu~M.}\,1988.
 Hierarchical cluster analysis and compliance:\,Moscow:~Finance and Statistics,  342~p.
 
\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}

