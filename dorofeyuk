\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\setcounter{page}{1423}
%\NOREVIEWERNOTES
\title
    [Комплекс алгоритмов интеллектуального анализа сложно организованных данных] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Комплекс алгоритмов интеллектуального анализа сложно организованных данных при исследовании слабо формализованных систем управления}
\author
    [Дорофеюк~Ю.\,А.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Дорофеюк~Ю.\,А., Покровская~И.\,В., Киселева~Н.\,Е.} % основной список авторов, выводимый в оглавление
    [Дорофеюк~Ю.\,А.$^{1,}$ $^2$, Покровская~И.\,В.$^1$, Киселева~Н.\,Е.$^{1}$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Работа выполнена при частичной финансовой поддержке РФФИ, гранты \No\,14-07-00463-а, \No\,13-07-00992-а, \No\,13-07-12201-офи, \No\,12-07-00540-а.}
\email
    {dorofeyuk\_julia@mail.ru}
\organization
    {$^1$Москва, Институт проблем управления им. В.А. Трапезникова РАН (ИПУ РАН);

    $^2$Москва, Научно исследовательский университет Высшая школа экономики (НИУ ВШЭ)}
\abstract
    {Рассматривается задача исследования системы управления заданного множества объектов, каждый из~которых характеризуется фиксированным (исходным) набором разнородных параметров. В~работе для~решения этой задачи предлагается исследовать структуру взаиморасположения управляемых объектов в~пространстве информативных параметров. Это~позволяет существенно повысить эффективность анализа функционирования системы, а~также устойчивость процедур принятия управленческих решений. Для~выявления такой~структуры разработан специальный комплекс алгоритмов интеллектуального анализа сложно организованных данных, а~также процедур экспертной коррекции. Проведен теоретический анализ различных вариантов алгоритма СКАД, доказаны теоремы о~сходимости алгоритма к~локальному экстремуму соответствующего критерия качества.

\medskip
\textbf{Ключевые слова}: \emph {интеллектуальный структурно-классификационный анализ данных, информативные параметры, начального разбиение, выбор числа классов, заполнение пропущенных наблюдений, процедуры экспертной коррекции}.}
\titleEng
    {The complicated data mining algorithms complex in the study of weakly formalized management systems}
\authorEng
    {Dorofeyuk~J.\,A.$^{1,}$ $^2$, Pokrovskaya~I.\,V.$^{1}$, Kiseleva~N.\,E.$^{1}$}
\organizationEng
    {$^1$ICS RAS; $^2$SIU HSE}
\abstractEng
    {The problem of the~large-scale management system study is~considered. The~system consists of a~large number of objects, each of~which is~characterized by a~heterogeneous set of~parameters. To~solve the~set of problems in this paper it is proposed to~investigate the~structure of the~relative location of these~objects in the~informative parameters space. This allows to~significantly increase the~analysis efficiency of the~system functioning and the~stability of the~procedures for~making management decisions. To~identify such patterns special mining complicated data algorithms complex and expert correction procedures were designed. The~theoretical analysis of~various types of~SCDA algorithm was carried out, the~algorithm convergence to the~local extremum of the~appropriate quality criterion theorems were proved.

    \textbf{Keywords}: \emph{intellectual structural-classification data analysis, informative parameters, initial partitioning, the number of classes selection, filling in missing observations expert correction, procedures}.}
\begin{document}

\maketitle

%\linenumbers
\section{Введение}
В последнее время для~исследования сложных систем управления стали широко использоваться структурно-классификационные методы интеллектуального анализа данных, базирующиеся на~алгоритмах классификационного анализа~\cite{DoBaDo_1, Dor_4}. Это~объясняется тем, что~многие системы управления, в~первую очередь организационно-административные, функционируют в условиях большой информационной размытости и неопределённости.

В~работе рассматривается задача анализа функционирования системы управления заданного множества объектов, каждый из которых характеризуется фиксированным (исходным) набором разнородных параметров. Основная идея предлагаемого метода решения подобных задач состоит в~следующем. В~работе предлагается исследовать не точные значения параметров, описывающих состояние каждого объекта системы, а лишь структуру взаиморасположения этих~объектов в~пространстве параметров. Такое интегральное описание управляемых объектов, позволяет существенно повысить эффективность анализа поведения системы, а~также устойчивость и робастность процедур принятия управленческих решений. Для~формализации такой задачи используется методология классификационного анализа данных~\cite{DoBaDo_1, Dor_4}.

Пусть исследуемая система состоит из~$n$ объектов, каждый из~которых характеризуется набором из~$k$ параметров. Вводится в~рассмотрение $k$-мерное пространство параметров~$X$, в~котором каждый объект представляется точкой~$x_j=(x_j^{(1)}, ..., x_j^{(k)})$, $j = 1, …, n$. Предполагается, что~вектор значений параметров~$x_j$ достаточно полно характеризует состояние $j$-го объекта, то~есть взаиморасположение множества точек~$x_1,…, x_n$ в~пространстве параметров~$X$ отражает реальную структуру исследуемого множества объектов. Для~выявления такой структуры был разработан комплекс алгоритмов интеллектуального анализа данных и процедур экспертной коррекции, включающий алгоритмы: структурно-классификационного анализа данных, выбора информативных параметров, выбора начального разбиения, выбора числа классов, заполнения пропущенных наблюдений, а также процедуры экспертной коррекции результатов работы этих~алгоритмов. Далее каждый из этих~алгоритмов рассматривается отдельно.


\section{Алгоритм структурно-классификационного анализа данных (СКАД)}
Пусть задано~$R_0$ -- некоторое начальное разбиение (классификация) точек классифицируемой выборки~$x_1, ..., x_n$, на~$r$ классов~$A_i$, $i=1, ..., r$. Алгоритм итерационный, циклический, многоэтапный -- на~$j$-м шаге $l$-го этапа рассматривается некоторый набор из~$l$~точек $X_j^l$ из исходной последовательности $x_1, …, x_n$, принадлежащих одному и тому же классу, где $j$ --- номер этого набора.
Номер этапа~$l$ равен мощности множества точек, которые <<перебрасываются>> на~каждом шаге этого этапа из~класса в~класс, т.\,е. числу точек в~наборе. На~$j$-ом шаге происходит пробная <<переброска>> из~класса в~класс множества точек $X_j^l$. Тогда~$X_j^l$ относится к~тому классу $A_s$, значение критерия качества классификации~$J$ для которого будет наибольшим, т.\,е. $X_j^l \in A_s$, для которого  $A_s = \argmax_{A_l} J(X_j^l \in A_i)$, $i=1, \dots,  r$, $j=1, \dots,  N_l$, где $N_l$ -- число различных наборов из~$l$ точек в~исходной выборке, принадлежащих одному и тому же классу. На~следующем шаге $l$-го этапа процедура повторяется для множества~$X_{j+1}^l$. Число шагов (итераций) на~$l$-ом этапе равно~$N_l$ и определяется выражением
$N_l = \sum_{i=1}^r C_{n_i}^l$ для $n_i \geq (l+2)$,
где $C_m^k$  -- число сочетаний из~$m$ по~$k$.
Из этого выражения следует, что для всех итераций $l$-го этапа процедура не~применяется для~таких классов~$A_i$, число точек~$n_i$ в~которых меньше, чем $(l+2)$. Число этапов (глубина перебора) $l_{max}$ либо фиксируется заранее, либо выбирается из~условия: в~классификации, полученной после $(l-1)$-го этапа, должен быть хотя~бы один класс, число точек в~котором не~меньше $(l+2)$. Это правило обеспечивает автоматический выбор максимально возможной глубины перебора~$l_max$.
Для повышения эффективности алгоритма СКАД используется следующая циклическая процедура. После завершения последнего этапа (либо $m$-ый, либо $l_{max}$-ый) весь описанный выше цикл повторяется заново, только в~качестве начальной классификации используется не~$R_0$, а~классификация, полученная на последнем этапе первого цикла.  Алгоритм СКАД заканчивает работу, если на~некотором цикле среди точек~$x_1,…, x_n$ не~будет сделано ни~одной <<переброски>> из~класса в~класс, т.\,е. для этого~цикла начальная классификация совпадает с~конечной.
Доказана следующая теорема о~сходимости этого~алгоритма.

\begin{Theorem}
    Алгоритм СКАД сходится за~конечное число шагов к~локальному максимуму критерия~$J$.
\end{Theorem}

\begin{Proof}
    Без ограничения общности даётся для случая двух классов.
    По~процедуре работы алгоритма СКАД значения критерия~$J$ образуют монотонно неубывающую, ограниченную сверху последовательность. Величина ограничения~$C_1 \geq J_i$, $i \in D$, где~$D$ "--- множество номеров всех возможных дихотомий исходной выборки~$x_1, ..., x_n$, зависит от~вида выбранного критерия~$J$. С~другой стороны, в~силу конечности исходной
    выборки существует такая константа~$C_2$, что~$C_2 \leq \mid J_i - J_j \mid$, $i, j \in D$, $i \neq j$. Таким образом, может~быть только конечное число шагов~$N$, на~которых критерий~$J$ возрастает: $N \leq C_1 / C_2$. На некоторых шагах работы алгоритма СКАД возможны ситуации, когда при~равенстве значений критерия~$J$ до и~после <<переброски>> $l$~точек происходит изменение принадлежности этих~точек к~классу. Для~того чтобы не~допустить возможности циклической последовательности таких <<перебросок>> (что~приводит к~бесконечному числу таких~шагов), в~алгоритме введено специальное правило для~таких случаев: при~равенстве значений критерия~$J$ до и~после <<переброски>> соответствующие $l$~точек относятся к~классу с~меньшим номером. Это~означает, что таких~перебросок с~нулевым приращением значения критерия~$J$ также будет конечное число. Достижимость локального максимума непосредственно следует из~самой процедуры <<переброски>> точек. Действительно, предположим противное "--- после останова значение критерия~$J_{кон}$ не~является $l$-локальным максимумом. А~это по~определению локального экстремума означает, что существует, по~крайней мере, один~набор из~$l$ точек исходной последовательности, для~которого изменение принадлежности к~классу приведёт к~увеличению значения критерия~$J$. Однако правило останова гарантирует, что такого набора в~исходной последовательности не~существует, поскольку на~последнем перед остановом цикле не~было ни~одной <<переброски>> $l$~точек. Теорема доказана.
\end{Proof}

\textbf{Алгоритм сокращённого перебора} "--- эвристический вариант выбора множеств~$X_j^l$. На~каждом шаге алгоритма для~пробной <<переброски>> используются точки в~определённом смысле ближайшие к~границе между~классами. Иллюстрация идеи работы алгоритма представлена на~рис.~\ref{pic1}. Четыре точки, обведённые кружочками "--- это как~раз и~есть те~$l$ точек (рассматривается случай $l = 4$), которые на~$j$-ом шаге ближе всего расположены к~границе (квадратиками обозначены центры классов на~$j$-ом шаге). Если уравнение границы в~явном виде неизвестно, то~выбираются $l$~точек, ближайших к~эталону другого~класса. Обычно в~качестве эталона выбирается точка <<центра тяжести>> всех~точек исходной выборки, принадлежащих на~$j$-ом шаге соответствующему классу (то~есть среднему этого~класса):

\begin{equation}
\label{eq_1}
    a_s(j)= \frac{1}{n_s(j)} \sum_{x_i \in A_s(j)} x_i,
        \end{equation}
где $s$ "--- номер класса.

\begin{figure}[t]
  \center {\includegraphics [width=0.7\textwidth]{fig1}}
  \caption{Иллюстрация идеи сокращённого перебора.}
\label{pic1}
\end{figure}

\paragraph{Алгоритм СКАД для одномерного случая.} Необходимо специально отметить этот~частный, но~весьма распростанённый в~прикладных задачах случай, связанный со~структурным анализом временных рядов~\cite{Gold_2}. Дело в~том, что~одномерный случай имеет уникальное свойство, существенно упрощающее процедуру целенаправленного перебора, используемую при~структурном анализе. А~именно: ввиду одномерной упорядоченности классов границей между двумя~классами (в~детерминированном случае) служит только одна точка, и таких границ может быть не~более двух (для крайних правого и левого классов "--- только одна).

\begin{Theorem}
    Одномерный вариант алгоритма СКАД сходится за~конечное число шагов к~глобальному максимуму критерия~$J$.
\end{Theorem}

\begin{Proof}
    Одномерный случай существенно отличается от~многомерного тем, что~классы упорядочены на~оси~$X$. Это, в~свою очередь, позволяет декомпозировать процесс минимизации функционала~$J$ для~всей выборки на~независимые процедуры его~минимизации для~подвыборок, каждая из~которых составляет одну~из~всех соседних пар классов. Таким образом, этот~алгоритм фактически является реализацией схемы динамического программирования, обеспечивающей нахождение глобального экстремума функционала~$J$ \cite{Bell_9}.

    Действительно, предположим противное, "--- после завершения работы одномерного алгоритма СКАД получена классификация на~$r$~классов (обозначим её через $H_\text{лок}$), доставляющая не~глобальный, а~лишь локальный экстремум $J_\text{лок}$ функционала~$J$. Это~означает, что существует такая классификация~$H_\text{глоб}$, для~которой значение функционала~$J_\text{глоб}$ будет больше, чем~$J_\text{лок}$.

    Введём в~рассмотрение пересечение двух~классификаций $H_1=\{A_{11}, ..., A_{1r}\}$ и $H_2=\{A_{21}, ..., A_{2r}\}$: это~множество $H_1 \cap H_2=\{A_{1i} \cap A_{2i}, i=1, \dots,   r\}$; а~также их~разность
    $H_1 \setminus H_2=\{A_{1i} \setminus A_{2i}, i=1, \dots,  r\}$.

    Рассмотрим пересечение классификаций~$H_\text{глоб} \cap H_\text{лок}$, а~затем вычтем его из~классификации~$H_\text{лок}$. Обозначим получившийся в~результате набор множеств точек через~$B_1, …, B_r$. Далее рассматриваются только непустые множества такого~вида. Рассмотрим для~примера множество~$B_1$ (пусть оно~содержит $m_1$~точек) из~первого класса классификации~$H_\text{лок}$. Рассмотрим этап алгоритма СКАД для~одномерного случая, на~котором анализируются точки только первого и второго классов, остальные границы считаются фиксированными. В~качестве начальных условий выберем границу между~первым и вторым классами из~классификации~$H_\text{глоб}$. В~соответствии с~работой алгоритма, точки множества~$B_1$ должны быть <<переброшены>> в~первый класс, так~как по~построению такой~переброске будет соответствовать большее значение функционала~$J$. Аналогичные рассуждения проводятся для~всех множеств $B_i, i=1, \dots,  r$. Следует подчеркнуть, что на~каждом цикле рассмотрения пары соседних классов используется правило выбора максимально возможной глубины перебора~$l_{max}$, обеспечивающее глобальный экстремум критерия~$J$ для~рассматриваемой пары классов (при~фиксированных остальных классах).

    Из~вышеизложенного можно сделать вывод, что предположение о~существовании классификации~$H_\text{глоб}$, доставляющее большее значение функционалу~$J$, чем классификация~$H_\text{лок}$, неверно. Таким образом, полученная в~результате работы алгоритма классификация доставляет глобальный экстремум функционалу~$J$.
\end{Proof}

При~моделировании и в~приложениях в~качестве критерия качества классификации~$J$ использовался функционал средней близости точек в~классах, определяемый через~потенциальную функцию~$K(x,y)$ близости точек~$x$ и $y$~\cite{Bramu_3}:

\begin{equation}
\label{eq_2}
   K(x,y)=\frac{1}{1+\alpha R^p (x,y)},
\end{equation}
где~$\alpha$ и~$p$ "--- настраиваемые параметры алгоритма. Средняя близость точек в~классе определяется как:

\begin{equation}
\label{eq_3}
   K(A_i,A_i)=\frac{2}{n_i(n_i-1)}\sum_{i=1}^{n_i} \sum_{j>i} K(x_i,x_j),
\end{equation}
где~$K(x_i,x_j)$ определяется формулой~\ref{eq_2}, "--- число точек в~классе~$A_1$. Тогда критерий~$J_1$ определяется~как:

\begin{equation}
\label{eq_4}
   J_1=\sum_{i=1}^r \frac{n_i}{n} K(A_i,A_i).
\end{equation}

Во~многих задачах структурно"=классификационного анализа объекты по~самой постановке задачи могут относиться к~разным классам с~различной степенью <<достоверности>>. Для~таких случаев была разработана постановка задачи размытого классификационного анализа~\cite{DoBaDo_1, Dor_4}.

\paragraph{Вариант алгоритма СКАД в размытой постановке.} Размытой классификацией множества~$X$ на~$r$ классов называется $r$-мерная вектор-функция $H(x)=(h_1(x), ..., h_r(x))$, где $h_i(x)$ "--- функция принадлежности объекта~$x$ к~$i$-му классу, удовлетворяющая условию нормировки:
$\sum_{i=1}^r h_i(x) = 1$, $0 \leq h_i(x) \leq 1$~\cite{DoBaDo_1}.

Критерий оценки качества классификации содержательно остаётся прежним, только видоизменяется процедура подсчёта его~значений. А~именно, функционал принимает следующий вид:
$J_1=\sum_{i=1}^r B_i K(A_i,A_i)$,
где $B_i = \frac{1}{n} \sum_{j=1}^n h_i(x_j)$ "--- нормирующий множитель, аналогичный $n_i /n$ для~детерминированного случая. Величина~$K(A_i, A_i)$ средней  близости  точек  в~классе~$A_i$  для~размытого случая определяется по~формуле:

\begin{equation}
\label{eq_5}
   K(A_i,A_i)= c_i \sum_{j=1}^{n} \sum_{l>j} K(x_j,x_l)h_i(x_j) h_i(x_l),
\end{equation}
где $c_i = 2 / \sum_{j=1}^n (h_i(x_j))^2$ "--- нормирующий множитель аналогичный $2/n_i(n_i - 1)$  для детерминированного случая.

Рассмотрим вкратце работу \emph{\textbf{размытого алгоритма СКАД}}. Для~простоты изложения и без~ограничения общности рассмотрим случай двух~классов ($r = 2$). Пусть задано начальное размытое разбиение $H_0=\{h_i(x_j), i=1,2, j=1,…,n\}$ точек классифицируемой выборки $x_1, ..., x_n$, которое может задаваться либо изначально, либо при~помощи специального алгоритма выбора начального разбиения. Для~начального размытого разбиения~$H_0$ подсчитывается значение критерия $J(H_0)$.
Как~и в~детерминированном случае размытый алгоритм является циклическим, многоэтапным, итерационным, -- на~$j$-ом шаге $l$-го этапа рассматривается некоторый набор из $l$ точек~$X_j^l$  из исходной последовательности $x_1,…, x_n$. При~этом для~всех точек множества~$X_j^l$  справедливо неравенство $h_l(x_s) > h_k(x_s)$, $l \neq k$, что соответствует требованию для детерминированного алгоритма: все точки множества~$X_j^l$ принадлежат одному и тому же классу.
Для множества точек $Х_j^l$ выполняется следующая операция, аналогичная <<переброске>> точек из~класса в~класс. Вводится понятие <<старой>> и <<новой>> функций принадлежности
$h_i(x_s)_\text{стар}$ и $h_i(x_s)_\text{нов}$ соответственно, $x_s \in X_j^l$. Тогда, если для всех~$x_s \in X_j^l$ выполняется
$h_1(x_s)_\text{стар} > h_2(x_s)_\text{стар}$ (аналог того, что набор точек~$Х_j^l$ принадлежит первому классу), то $h_1(x_s)_\text{нов} = h_2(x_s)_\text{стар}$ и $h_2(x_s)_{нов} = h_1(x_s)_{стар}$ (аналог того, что набор точек $Х_j^l$ <<переброшен>> во~второй класс). Далее подсчитывается значение критерия~$J(H_1)$ с~<<переброшенным>> набором точек~$Х_j^l$. Если значение $J(H_1)> J(H_0)$, то~изменения функций принадлежности для~набора точек~$Х_j^l$  остаётся в~силе, в~противном случае произведённые изменения значений функций принадлежности отменяются. Аналогичная операция выполняется для~случая, когда $h_1(x_s)_\text{стар} < h_2(x_s)_\text{стар}$ аналог того, что~набор точек~$Х_j^l$ принадлежит второму классу). Алгоритм прекращает работу, если на~каком-то цикле не~было произведено изменений функций принадлежности ни для~одной из~точек исходной выборки.

\section{Алгоритм построения начального разбиения}

В~составе комплекса алгоритмов интеллектуального анализа данных был разработан алгоритм построения начального разбиения "--- как для~детерминированного, так~и для~размытого случая. Рассмотрим эти~алгоритмы более подробно.

\paragraph{Детерминированный случай.} Для~простоты изложения без~ограничения общности, алгоритм описан для~случая двух~классов "--- $A_1$ и $A_2$. На~первом шаге из~всех точек выборки~$x_1, …, x_n$ находится пара наиболее удалённых друг от~друга точек, $x_l$ и $x_p$, одна из~которых "--- $x_l$, относится к~классу~$A_1$, а~другая "--- $x_p$, относится к~классу~$A_2$. Если~$n$ достаточно велико, то~используется усечённый вариант первого шага, а~именно: $x_l$ выбирается случайно, а~$x_p$ ищется как~точка, наиболее от~неё удалённая.

Затем последовательно рассматриваются все~точки выборки, за~исключением точек~$x_l$ и $x_p$. А~именно, на~втором шаге рассматривается точка~$x_1$ (при~условии,
что~$x_1 \neq x_l$, $x_1 \neq x_p$), которая относится к~первому классу, если она ближе к~$x_l$, чем к~$x_p$, и ко~второму классу в~противном случае. Если $x_1=x_l$ или $x_1=x_p$, тогда переходим к~рассмотрению следующей точки. На~$j$-ом шаге рассматривается точка~$x_j$  (при~условии, что~$x_j \neq x_l$, $x_j \neq x_p$), которая относится к~одному из~двух классов в~соответствии с~правилом:
\[
x_j \in \begin{cases}
        A_1, & \text{если } K(x_j,A_1)\geq K(x_j,A_2)  \\
        A_2, & \text{если } K(x_j,A_1)< K(x_j,A_2)
    \end{cases},
    j=1, \dots,  n, x_j \neq x_l, x_j \neq x_p
\]
Такая процедура повторяется до~тех~пор, пока не~будут исчерпаны все~точки выборки. Полученное  разбиение принимается в~качестве начального разбиения~$R_0$.

\paragraph{Размытый случай.} На~первом шаге алгоритма находится начальное разбиение~$R_0$ выборки~$x_1, …, x_n$ на~$r$ классов в~детерминированном случае. На~втором шаге определяются $a_1, …, a_r$ "--- центры всех~классов в~полученном разбиении~$R_0$. Далее для~каждой точки~$x_j$  рассчитываются функции принадлежности~$h_i(x_j)$, $i=1, …, r$, $j=1, …, n$, значения которых обратно пропорциональны расстояниям до~центров соответствующих классов:
$h_i(x_j) = K(a_i, x_j) / \sum_{i=1}^r K(a_i, x_j)$,
где $K(a_i, x_j)$ "--- потенциальная функция вида~\ref{eq_2}, а~$\sum_{i=1}^r K(a_i, x_j)$  "--- нормирующий множитель, обеспечивающий выполнения условия нормировки функций принадлежности: $\sum_{i=1}^r h_i(x) = 1$. Полученный набор функций принадлежности и~определяет размытое начальное разбиение $H_0=\{h_i(x_j), \, i=1,…,r, \, j=1,…,n\}$.

\section{Алгоритм выбора числа классов}
Одна из~основных проблем использования структурно"=классификационных методов при~решении задач исследования сложных систем управления "--- это~выбор числа классов. Дело в~том, что чрезвычайно важным фактором в~прикладных исследованиях является содержательная интерпретация элементов получаемых в~результате анализа структуры объектов (например, классов объектов). В~работе описан специально разработанный алгоритм оптимального выбора числа классов. При~этом оптимальность понимается в~смысле максимизации содержательно обоснованного критерия качества классификации. Алгоритм, по~сути, представляет собой экспертно"=компьютерную процедуру, которая работает следующим образом. Сначала эксперт оценивает диапазон~$(r_{min}, r_{max})$, в~пределах которого заведомо находится искомое число классов. Далее, используя алгоритм СКАД, проводится разбиение анализируемого множества объектов на~$(r_{min}, r_{max})$ классов. Качество каждой из~полученных классификаций оценивалось с~помощью критерия

\begin{equation}
\label{eq_6}
   J_3 = J_1 - q J_2.
\end{equation}

В~формуле~(\ref{eq_6}) величина~$J_1$ "--- это~средняя (по~классам) мера близости точек, принадлежащих одному и тому~же классу, вычисляется по~формуле~(\ref{eq_4}); $q$ "--- свободный (настраиваемый) параметр алгоритма; $J_2$ "--- средняя мера близости классов, определяемая соотношением:

\begin{equation}
\label{eq_7}
   J_2=  \frac{1}{r-1}  \sum_{i=1}^r \sum_{j>i} \frac{n_i + n_j}{n} K(A_i,A_j).
\end{equation}

В~формуле~(\ref{eq_7}) величина~$n_i$ "--- это число точек в~классе~$A_i$, а~величина~$K(A_i, A_j)$ "--- мера близости классов $A_i$, $A_j$ "--- вычисляется по~формуле:

\begin{equation}
\label{eq_8}
   K(A_i,A_j)=\frac{1}{n_i n_j}\sum_{x_l \in A_i} \sum_{x_p \in A_j} K(x_l,x_p).
\end{equation}

В~формуле~(\ref{eq_8}) потенциальная функция~$K(x_l, x_p)$ определяется формулой~(\ref{eq_2}). Параметр~$q$ является масштабирующим параметром, приводящим значения функционалов~$J_1$ и $J_2$ к~соизмеримым величинам; на~практике $q$ имеет значения порядка~$2, \dots,  7$ (во~столько раз обычно отличается средняя близость внутри классов от~средней близости между~самими классами). Более подробно вопрос выбора значений настраиваемых параметров рассмотрен далее в~специальном разделе.

В~итоге получается последовательность $J_3(r_{min}), ..., J_3(r_{max})$. Формально в~качестве наилучшего (оптимального) можно выбрать такое~число классов~$r_{opt}$, которое соответствует экстремальному значению критерия~(\ref{eq_6}):

\[
r_{opt} = r_j \, | \, max J_3 (r_j), r_j = r_{min}, ..., r_{max}.
\]

Однако наличие существенной, но~неиспользованной при~классификации информации (например, ввиду отсутствия данных) может привести к~тому, что полученное таким~способом $r_{opt}$ не~будет наилучшим с~точки зрения эксперта. Для~компенсации этого~недостатка предлагается использовать следующую экспертную процедуру. Экспертам представляются значения~$J_3(r_j)$,  $r_j = r_{min}, …,r_{max}$, представленные для~удобства в~виде графика, на~котором отмечается значение~$r_{opt}$ (оно~соответствует максимальной точке на~графике).  Используя эту~информацию, эксперты могут корректировать выбираемое число классов. В~подавляющем числе случаев экспертное число классов либо совпадает с~$r_{opt}$, либо незначительно ($\pm 1$) отличается от~него.

При~классификации многомерных объектов во~время такой~экспертизы анализируется также классификация каждого~объекта. Для этой~цели экспертам сообщается информация о~мере близости $K(x_i, c_j)$ каждой~точки~$x_i$ до~центров классов~$c_j$, $j=1,…, r$ в~оптимальной классификации, то~есть матрица близости $\|K(x_i, c_j)\|$, $i=1, …, n$, $j=1,…, r_{opt}$. Перенесение точки (объекта) $x_i$ из~$j$-го класса в~$l$-ый считается допустимым, если~величины~$K(x_i, c_j)$ и $K(x_i, c_l)$ отличаются незначительно. Другими словами,  содержательно  обоснованное перенесение допустимо для~точек, расположенных вблизи~границы между~соответствующими классами.

\section{Алгоритм выбора информативных параметров}
Опыт использования алгоритмов структурно-классификационного анализа показывает, что классификация по~всем исходным параметрам далеко не~всегда приводит к~желаемым результатам~\cite{Dor_4}. Действительно, при~сравнительно небольших выборках экспериментальных наблюдений и наличии помех (ошибки в~определении значений параметров, сознательное искажение информации и т.\,д.) использование для классификации большого числа входных параметров приводит к~сильному <<перемешиванию>> классов, а сами~классы при~этом плохо поддаются интерпретации. По~этой причине классификацию объектов целесообразно проводить не в~исходном пространстве, а в~пространстве наиболее существенных (информативных) параметров, имеющем значительно меньшую размерность.

Для~выбора информативных параметров в~работе предлагается использовать результаты структуризации параметров. Далее, для~того, чтобы отличать структуризацию объектов и параметров, будем говорить о~классификации объектов, но о~группировке параметров.

\paragraph{Алгоритм СКАД в~задаче группировки параметров.} Для~группировки параметров, как~и в~случае классификации объектов, предлагается использовать алгоритм СКАД. Формальная постановка задачи группировки параметров подразумевает определение: множества параметров, подлежащих группировке; множества решающих правил и критерия качества группировки~\cite{DoBaDo_1}.

\emph{\textbf{Группируемое множество параметров}} "--- это~конечный набор
параметров $\{x^{(1)}, ..., x^{(1)}\}$, полученный из~исходного набора после~нормировки дисперсии каждого параметра на~1. Здесь~$x_j^{(i)}$, $i=1, \dots,  k$, $j=1, \dots,  n$ определены как~реализации случайной величины~$x^{(i)}$  на~множестве исследуемых объектов. \emph{\textbf{Множество решающих правил}}, как~и в~случае классификации объектов "--- единичный симплекс~\cite{DoBaDo_1}.

Для~формулировки критерия качества группировки необходимо ввести меру~близости между~параметрами (случайными величинами) $x$ и $y$. В~качестве такой~меры используется коэффициент ковариации (совпадающий с~коэффициентом корреляции для~нормированных параметров~$x$ и~$y$), который будем обозначать через~$cov_{x,y}=(x,y)$, понимая его как скалярное произведение случайных величин~$x$ и~$y$. Для~дисперсии~$cov_{x,x}$  случайной величины~$x$ используется обозначение~$cov_{x,x}=(x,x)=x^2$. Критерий качества группировки используется в~виде следующего функционала:

\begin{equation}
\label{eq_9}
    J^* = \sum_{j=1}^s \sum_{\substack {{x^{(i)}, x^{(l)} \in A_j} \\ {x^{(i)} \neq x^{(l)}}} }    cov_{x^{(i)}, x^{(l)}}^2 =
    \sum_{j=1}^s \sum_{\substack {{x^{(i)}, x^{(l)} \in A_j} \\ {x^{(i)} \neq x^{(l)}}} }
    (x^{(i)}, x^{(l)})^2,
    \end{equation}
где~$s$ "--- число групп.  Максимизация функционала~(\ref{eq_9}) соответствует интуитивному представлению о~<<хорошем>> разбиении параметров, "--- когда в~одну и ту~же группу попадают наиболее близкие (в~определённом выше смысле) параметры. В~этом смысле функционал~(\ref{eq_9}) полностью аналогичен функционалу~~(\ref{eq_4}), который используется как~критерий качества классификации объектов.

Для~выбора информативных параметров чрезвычайно важно знать интегральные характеристики (эталоны) полученных групп. Для~классификации объектов такими эталонами обычно являются <<центры тяжести>> точек, попавших в~один и тот~же класс, которые вычисляются по~формуле~(\ref{eq_9}). Для~группировки параметров такого~типа эталонами являются <<средние>> (в~определённом выше смысле) виртуальные нормированные параметры (случайные величины) $f_1, ..., f_s$ такие, что
  $f_j^2=1, j=1, \dots,  s$, которые будем называть факторами. Факторы (эталоны) некоторой группировки на~$s$ групп~$A_1, ..., A_s$ определяются соотношением~(\ref{eq_10}), являющемся, в~определённом смысле, аналогом~(\ref{eq_10}):

\begin{equation}
\label{eq_10}
    f_j = \argmax_j \sum\limits_{x^{(i)}\in A_j} (x^{(i)},f)^2,
    \qquad f^2=1.
    \end{equation}

При~решении прикладных задач критерий качества группировки (9) иногда удобнее представить в эквивалентном виде:

\begin{equation}
\label{eq_11}
   J^* = \sum_{j=1}^s \sum_{x^{(i)} \in A_j}    (x^{(i)}, f_j)^2.
    \end{equation}

Таким~образом, задача группировки набора $k$~параметров на~заданное число групп~$s$ состоит в~максимизации функционала~(\ref{eq_11}) как по~разбиению параметров на~группы~$A_j$, так~и по~выбору факторов $f_j$, $j=1, \dots,  s$, $f_j^2=1$ определяемых из соотношения~(\ref{eq_10}) при~фиксированной группировке.

Легко показать\cite{Bramu_3}, что для~фиксированной группировки на непересекающиеся группы~$A_1, ..., A_s$   (детерминированная постановка задачи) факторы (эталоны групп) определяются по~формуле:

\begin{equation}
\label{eq_12}
f_j= \frac
      {\sum\limits_{x^{(i)}\in A_j} \alpha_i x^{(i)}}
      {\sqrt{ \left(\sum\limits_{x^{(i)}\in A_j} \alpha_i x^{(i)} \right)^2}}=
      \frac
      {\sum\limits_{x^{(i)}\in A_j} \alpha_i x^{(i)}}
      {\sqrt{\sum\limits_{x^{(i)}\in A_j, x^{(l)}\in A_j}
                     \alpha_i \alpha_j \bigl(x^{(i)},x^{(l)}\bigr) }}, \,
                     j=1, \dots, s,
    \end{equation}
где~$\alpha_i$ "--- компоненты собственного вектора матрицы $R_j = \|x^{(i)}, x^{(l)}\|$,   соответствующего её~наибольшему собственному значению. Из~(\ref{eq_12}) непосредственно следует, что фактор группы "--- это~линейная комбинация параметров, отнесённых к~этой~группе (знаменатель в~(\ref{eq_12}) необходим для нормировки~$f^2 =1$), причём коэффициентами в~этой~комбинации являются компоненты <<максимального>> собственного вектора ковариационной матрицы параметров из~этой~группы.

Для~одновременного определения групп~$A_1, ..., A_s$ и факторов~$A_1, ..., A_s$, удовлетворяющих этим~условиям, используется описанный выше алгоритм СКАД, который в~данном случае работает следующим образом (для простоты описан алгоритм СКАД в~детерминированном случае для~$l=1$, $s=2$).

Пусть задано некоторое начальное разбиение $R_0^{\ast}$  группируемого множества параметров
 $\{x^{(1)},\dots, x^{(k)}\}$.
Обозначим через $x^{(j)} \in A_1^{\ast}$ параметры, относящиеся к~первой группе, а через
 $x^{(j)} \in A_2^{\ast}$ "--- ко второй.
Алгоритм итерационный, на~каждом шаге рассматривается один параметр из~последовательности
$x^{(1)},\dots, x^{(k)},x^{(1)},\dots$
(<<зацикленная>> исходная последовательность параметров).
Как и в~случае классификации объектов, отнесение параметра $x^{(j)}$  к~одной из~двух групп
обозначается с~помощью индекса $\rho(x^{(j)})$, который равен 1, если
$x^{(j)} \in A_1^{\ast}$, и $-1$, в~противном случае.
Тогда этот~вариант алгоритма группировки записывается в~виде:
\begin{equation}
\label{eq_13}
\rho(x^{(j)})=
    \sign \Bigl[J^{\ast} (x^{(j)}\in A_1^{\ast} )- J^{\ast}(x^{(j)}\in A_2^{\ast} ) \Bigr]
    j=1, \dots, k, 1, \dots, k, 1, \dots.
    \end{equation}

Таким образом, на~каждом шаге текущий параметр~$x^{(j)}$ относится к~той группе, при~отнесении к~которой, значение критерия~$J^*$ будет больше (если эти~значения равны, то он~относится к~группе с~наименьшим номером). Алгоритм~(\ref{eq_13}) заканчивает работу, если на~некотором цикле среди~параметров~$x^{(1)}, ..., x^{(k)}$  не~будет сделано ни~одной <<переброски>> параметра из~группы в~группу. При~этом, если критерий качества имеет вид(\ref{eq_9}), то~факторы групп~$f_1$ и~$f_2$ определяются с~помощью~(\ref{eq_12}) по~завершении процедуры группировки. Если~же используется критерий в~виде~(\ref{eq_11}), то для~подсчёта
значений~$J^*=(k^{(j)} \in A_i^*)=$ в~(\ref{eq_13}) факторы групп необходимо определять с~помощью~(\ref{eq_12}) на~каждом шаге.

\begin{Theorem}
    Алгоритм СКАД в~задаче группировки параметров сходится к~локальному максимуму функционала~$J^*$ за~конечное число шагов (итераций).
\end{Theorem}

Доказательство этого утверждения аналогично доказательству Теоремы 1.

\paragraph{Выбор информативных параметров.}
В~результате применения алгоритма СКАД к~исходным $k$~параметрам будет получено их~разбиение на~заданное число групп~$s$ (в~прикладных задачах значение~$s$ колеблется в~диапазоне $3, \dots,  10$), а~также значения факторов для~полученных групп. При~решении прикладных задач в~дальнейшем используются либо новые интегральные параметры "--- факторы групп (если удаётся получить их~удовлетворительное содержательное описание), либо такой набор параметров из~исходного множества параметров (число которых равно числу групп), каждый из~которых является ближайшим (в~определённом выше смысле) к~фактору в~соответствующей группе. В~некоторых случаях (например, когда нет такого параметра в~группе, значения коэффициента корреляции которого с~фактором значимо больше, чем для~других параметров этой~группы) в~отдельных группах может быть отобрано по~2, а для~особо многочисленных групп "--- по~3 и~более параметров, ближайших к~соответствующему фактору и максимально удалённых друг от~друга. Иногда при~формировании набора информативных параметров используются процедуры экспертной коррекции~\cite{Dor_4, DoPoChe_5}.

В~большинстве приложений исходные или выделенные информативные параметры имеют неравнозначную важность при~анализе структуры объектов. Для~формирования коэффициентов важности (весов) в~работе предлагается использовать процедуры экспертного оценивания. Хорошие результаты даёт процедура многовариантной экспертизы~\cite{DDPC_6}, когда для~оценки таких~весов используется несколько групп экспертов "--- специалистов в~различных аспектах исследуемой проблемы. В~результате экспертизы каждому параметру присваивается определённый вес (важности) при~исследовании структуры объектов.

\paragraph{Выбор <<оптимального>> числа классов в~задаче группировки параметров.}
Для~выбора числа классов в задаче группировки параметров используется специальная  экспертно-компьютерная процедура оптимизации критерия, аналогичного критерию выбора <<оптимального>> числа классов в~задаче классификации объектов. Опишем вкратце эту~процедуру.

Сначала эксперт-пользователь оценивает диапазон~$(s_{min}, s_{max})$, в~пределах которого заведомо находится искомое число групп. Далее, используя алгоритм СКАД, проводится разбиение группируемого множества параметров на~$s_{min}, s_{min +1}, …, s_{max}$ групп. Качество каждой из~полученных группировок оценивается с~помощью критерия:
\begin{equation}
\label{eq_14}
J_3^*(s) = J_1^*(s) - q J_2^*(s),
    \end{equation}
где $J_1^*(s)$ "--- величина средней по~группам меры близости параметров в~группе, а~$J_2^*(s)$  "--- величина средней меры~близости между~группами. Величина~$q$ в~(\ref{eq_14}) является масштабирующим параметром, приводящим к~одному масштабу средние значения функционалов~$J_1^*(s)$  и~$J_2^*(s)$. На~практике величина~$q$ выбирается в~диапазоне значений 2-5 (обычно во~столько раз отличается средняя близость параметров внутри групп от~средней близости самих групп).

В~качестве <<оптимального>> можно выбрать такое~число классов~$r_{opt} = r_j$, которое соответствует максимальному значению критерия~(\ref{eq_14}) для~$r_j = r_{min} , …, r_{max}$. Однако наличие существенной, но неиспользованной при~классификации информации может привести к~тому, что так~полученное $r_{opt}$ не~будет <<<истинно оптимальным>>. Для~компенсации этого~недостатка используется процедура экспертной коррекции\cite{Dor_4, DoPoChe_5}.

\section{Особенности реализации разработанного комплекса алгоритмов}

В~процессе реализации разработанного комплекса алгоритма интеллектуального анализа данных возникает целый ряд проблем, для~разрешения которых приходится разрабатывать специальные процедуры или использовать уже известные алгоритмы. В~большинстве приложений, особенно связанных с~социально"=экономическими системами, пользователь сталкивается с~проблемой качества исходных данных. Здесь, прежде всего, необходимо выявлять ошибки в~исходных данных, в~том числе имеющие случайный характер.  Для этой~цели используются разнообразные алгоритмы фильтрации. Например, для~выявления существенных <<выбросов>> в~значениях параметров строится гистограмма распределения значений каждого из~параметров, и в~зависимости от~содержательной модели исследуемого объекта выбирается тот или иной тип функции распределения. Для~структурно"=классификационных алгоритмов наиболее адекватной моделью является смесь нормальных распределений. Существуют стандартные статистические методы для~определения того, является~ли анализируемое значение выбросом или~согласуется с~выбранной моделью порождения данных~\cite{Kram_7}. В~любом случае, по~виду гистограммы экспертным путём всегда можно определить какое из~значений параметра заведомо является <<выбросом>>. Так, например, во~многих приложениях широко используется так~называемое <<правило $3\sigma$>>. Правило действует следующим образом: для~каждого числового параметра по имеющейся выборке определяется среднее
значение~$\hat{x}^{(i)}=\frac{1}{n} \sum_{j=1}^n x_j^{(i)}$ и стандартное отклонение
$\sigma^{(i)}=\sqrt{\frac{1}{n-1} \sum_{j=1}^n  (x_j^{(i)} - \hat{x}^{(i)})^2 }$.
Все~значения $x_j^{(i)}$, превосходящие $\hat{x}^{(i)} \pm 3\sigma^{(i)}$ считаются <<выбросами>>. Обычно <<выбросы>> заменяются либо на~среднее значение этого~параметра, либо на~соответствующую границу диапазона $\hat{x}^{(i)} \pm 3\sigma^{(i)}$. В~работе предлагается выбросы считать пропущенными наблюдениями и использовать для~их~заполнения специально разработанную процедуру.

\paragraph{Процедура заполнения пропущенных наблюдений.}
Как~уже говорилось выше, во~многих приложениях имеются пропуски в~данных, кроме того, в~процессе фильтрации <<выбросы>> часто рассматриваются как~пропущенные наблюдения. В~этой~ситуации нужно либо использовать специальные процедуры подсчёта расстояний между~объектами, в~параметрах которых имеются пропуски, либо~разрабатывать специальные процедуры заполнения таких~пропусков. В~подавляющем большинстве работ, пропуски по~каждому параметру предлагается заполнять средним известных значений соответствующего параметра (для~исходной выборки). В~настоящей работе была разработана специальная процедура заполнения пропусков в~исходных данных с~использованием алгоритмов автоматической классификации. Основная идея процедуры состоит в~следующем. Если множество изучаемых объектов структурировано (то~есть их можно разделить на~классы, достаточно компактно расположенные в~пространстве параметров~$X$), то дисперсия (диапазон) изменения каждого параметра в~пределах каждой группы, как~правило, будет существенно меньше, чем этот~показатель для~значения этого~параметра на всей~выборке. Таким образом, если по~данным с~пропусками удастся определить реальную структуру взаиморасположения точек (т.\,е. провести классификацию, адекватную этой~структуре), то~заполнять пропущенное значение $l$-го параметра для~объекта из~$i$-го класса можно средним этого~параметра по его~известным значениям для всех~объектов, попавших в~$i$-ый класс. Исходя из сделанного предположения, отклонение полученного значения от <<истинного>> должно быть существенно меньше (в~среднем), чем обычная схема заполнения по~общему среднему.

Опишем процедуру более подробно. На~первом шаге все~пропуски заполняются средними значениями каждого параметра по всей~выборке. Далее проводится классификация выборки с~заполненными пропусками на~$r_0$ классов, где $r_0$ выбирается из~следующих соображений. В~каждом классе число объектов должно быть достаточным для~статистически значимой оценки среднего значения параметра, т.\,е. не~меньше, чем 8-10 точек. Поэтому $r_0^{\text{\emph{нач}}}=n/15$  (с~учётом неоднородности распределения числа точек по~классам). Если в~полученной классификации для~некоторого класса число входящих точек будет меньше 8, то такой~класс присоединяется к~ближайшему классу. Некоторые из~таких классов могут объединиться между~собой, тогда дальнейшее их~объединение не~производится, если число точек в~образованном классе больше или равно 8. В~качестве меры близости двух~классов $A_i$, $A_j$ используется величина $K(A_i, A_j)$, определяемая формулой~(\ref{eq_8}). В~итоге, получается разбиение на~$r_1$ классов. Затем, в~каждом из~полученных классов ранее заполненные пропущенные наблюдения заполняются новыми значениями. А~именно, пропущенное значение~$i$-го параметра для~$j$-го объекта заменяется средним известных значений $i$-го параметра для~всех объектов из~$l$-го класса (к~которому принадлежит $j$-ый объект). Такое заполнение производится для всех~значений параметров, пропущенных в~исходной выборке. На~втором шаге происходит точно такая~же процедура для~матрицы данных, полученной после первого шага. Процедура заканчивается на~таком шаге, на~котором классификация точек осталась неизменной относительно предыдущего шага.

\paragraph{Выбор свободных параметров алгоритма.}
Комплекс алгоритмов имеет несколько настраиваемых параметров, которые должны быть выбраны либо~до его~использования на~конкретном материале с~помощью экспертов, либо в~процессе такого использования с~привлечением экспертных процедур. Таковыми параметрами являются: $\alpha$ и $p$ в~формуле~(\ref{eq_2}), определяющей значение потенциальной функции~$K(x,y)$, а~также параметр~$q$ в~формуле~(\ref{eq_6}), определяющей критерий $J_3$ выбора оптимального числа классов. При~выборе $\alpha$ и $p$ в~(\ref{eq_2}) воспользуемся следующими соображениями. Введём в~рассмотрение величину $R_{cp}$ (расстояние <<среза>>), определяемую равенством:

\begin{equation}
\label{eq_15}
\left.
\frac{d^2 K[R(x,y)]}{dR^2 (x,y)}
\right|_{R_{cp}}
\end{equation}

Значение величины~$R_{cp}$ в~(\ref{eq_15}) определяет точку перегиба функции~$K[R(x,y)]$, т.\,е. точку максимальной крутизны этой~функции. На~рис.~\ref{pic2} изображён график функции для~различных значений~$p$ при~одном и том~же значении величины~$R_{cp}$. Параметр~$p$ при фиксированном~$R_{cp}$ характеризует крутизну функции~$K[R(x,y)]$ в~районе точки перегиба. Для удобства счёта в~качестве~$p$ выбирают числа кратные двум (2,4,6,…).

\begin{figure}[t]
  \center {\includegraphics [width=0.7\textwidth]{fig2}}
  \caption{График функции $K[R(x,y)]$ для~различных значений $p$ при~одном и том~же $R_{cp} =1,25$.}
\label{pic2}
\end{figure}

Параметр~$\alpha$ в~выражении~(\ref{eq_2}) при известном $R_{cp}$ и фиксированном~$p$ определяется из~выражения: $\alpha = \frac{p-1}{(p+1)R_{cp}}$. Обычно~$R_{cp}$ выбирается из~следующих соображений. По~определению, точки, входящие в~одну и ту~же группу (класс), имеют высокие значения функции близости (значения потенциальной функции). А это~означает, что расстояние между~точками одной и той~же группы в~большинстве случаев меньше~$R_{cp}$. И, наоборот, "--- значения потенциальной функции (меры близости) между~точками из разных групп существенно меньше, чем аналогичные значения для~точек из~одной и той~же группы, т.\,е. соответствующее значение расстояния будет больше, чем~$R_{cp}$. Это~означает, что~$R_{cp}$ должно равняться расстоянию от~границы группы до~центра группы (в~среднем по~всем группам). Поскольку до самой~группировки определить это значение невозможно, то обычно делается 2-4 пробных расчёта для~различных значений этого~параметра. Начальное~$R_{cp}$ обычно выбирается как~функция от~размерности~$k$ пространства~$X$, числа классов~$r$ и <<характерного>> размера множества точек выборки, например, диаметр сферы, описывающей все точки исходной выборки. В~работе для этой~цели используется выражение
$R_{cp}^{\text{\emph{нач}}} = \frac{\sqrt{k} R_{max}(x_i,x_j)}{r}$,
где $R_{max}(x_i,x_j)$ "--- расстояние между~максимально удалённой пары точек исходной выборки (после фильтрации и замены <<выбросов>>, о~которых говорилось выше).

При выборе~$p$ необходимо иметь ввиду следующее обстоятельство. Если исследуемый материал достаточно хорошо структурирован, т.\,е. в пространстве~$X$ имеются хорошо обособленные друг от~друга группы точек, то крутизна функции~$K(x,y)$ в~районе~$R_{cp}$ может быть не~очень большой, т.\,к. влияние далеких точек, находящихся на расстоянии существенно большем, чем~$R_{cp}$, будет не~существенно. С~другой стороны, если такой явной структурированности нет (например, в случае сильной зашумлённости данных), то в~<<промежутках>> между~группами будет достаточное количество точек (так называемые <<мосты>>). В этом случае, крутизна потенциальной функции в~районе границы, т.\,е. в~районе ~$R = R_{cp}$, должна быть достаточно высокой, чтобы минимизировать влияние точек в~районе границы на~процесс кластеризации. Для сильно зашумлённых данных разработаны алгоритмы, в~которых вводится специальный, так~называемый, <<фоновый>> класс~\cite{DoBaDo_1}. К~фоновому классу относятся точки, которые расположены достаточно далеко от~центров всех~классов. В~прикладных исследованиях величина~$p$ подбирается экспериментально: начальное значение $p = 2$ выбирается для~случая хорошей структурированности, а $p = 4, …, 8$ "--- для~случаев слабой~структуризации.

Следует отметить, что в~прикладных задачах могут использоваться как~числовые, так~и качественные переменные. В~первом случае, в~качестве~$R(x,y)$ в~формуле~(\ref{eq_2}) используется евклидово расстояние $R(x,y)=R_e(x,y)=\sqrt{\sum_{i=1}^k (x^{(i)} - y^{(i)})^2}$. В~приложениях различные типы качественных параметров в~подавляющем числе случаев приводятся к~набору логических переменных, для~них используется расстояние по~Хеммингу: $R(x,y)=R_h(x,y)$, т.\,е. число несовпадающих разрядов в~двоичных кодах векторов. Заметим, что для~логических переменных~$x$ и~$y$: $R_h(x,y)=R^2_e(x,y)$. Если среди входных параметров есть как~числовые, так~и логические переменные, то в~качестве квадрата расстояния можно использовать величину
$R^2(x,y)= R_h(\tilde{x},\tilde{y}) + R^2_e(\hat{x},\hat{y})$,
где $\tilde{x},\tilde{y}$ "--- логические переменные, $\hat{x},\hat{y}$  "---  числовые.

При выборе масштабирующего параметра~$q$ в~формуле~(\ref{eq_6}) обычно руководствуются следующими соображениями. Из формул~(\ref{eq_3}) и (\ref{eq_4}), определяющих~$J_1$, и формул (\ref{eq_7}) и (\ref{eq_8}), определяющих~$J_2$, непосредственно следует, что величина~$J_1$ существенно больше, чем $J_2$. Значения $J_1$ и $J_2$ определяются конкретной структурой расположения точек в~пространстве~$X$, и выбранными значениями параметров~$R_{cp}$ и~$p$. Моделирование разработанных алгоритмов, а~также решение некоторых прикладных задач показало, что характер поведения функции~$J_3 = J_1 - qJ_2$ мало меняется в~широком диапазоне значений~$q$. В~зависимости от~структурированности пространства~$X$ хорошие результаты получаются для~значений~$q$ в~диапазоне $2, \dots,  7$.



\section{Заключение}
Разработанный комплекс алгоритмов интеллектуального анализа данных использовался для~анализа сложноорганизованных данных в~рамках исследования сложных систем управления, а~также при~совершенствовании процедур принятия решений для~нескольких крупных систем управления, в~основном регионального характера. Во~всех приложениях, а~также при~машинном моделировании~\cite{DGS_8}, была подтверждена высокая эффективность разработанного комплекса.



\section{Литература}
\renewcommand{\bibname}{}
\begin{thebibliography}{99}
\bibitem{DoBaDo_1}
    \BibAuthor{Дорофеюк\;А.\,А., Бауман\;Е.\,В., Дорофеюк\;Ю.\,А.}
    \BibTitle{Методы интеллектуальной обработки информации на базе алгоритмов стохастической аппроксимации.}~//
    \BibJournal{Математические методы распознавания образов. 15-ая международная конференция: Сб. докладов.},
    М:~МАКС ПРЕСС, 2011.  С.\,108--112.
\bibitem{Gold_2}
    \BibAuthor{Гольдовская\;М.\,Д., Дорофеюк\;Ю.\,А., Киселева\;Н.\,Е.}
    \BibTitle{Методы структурного анализа в прикладных задачах исследования временных рядов.}~//
    \BibJournal{Проблемы управления}. 2013. \No\,3. С.\,33--41.
\bibitem{Bramu_3}
    \BibAuthor{Браверман\;Э.\,М., Мучник\;И.\,Б.}
    Структурные методы обработки эмпирических данных.
    М.:~Наука, 1983. 464~c.
\bibitem{Dor_4}
    \BibAuthor{Дорофеюк\;А.\,А.}
    \BibTitle{Методология экспертно-классификационного анализа в задачах управления и обработки сложноорганизованных данных (история и перспективы развития).}~//
    \BibJournal{Проблемы управления}. 2009. \No\,3.1. С.\,19--28.
\bibitem{DoPoChe_5}
    \BibAuthor{Дорофеюк\;А.\,А., Покровская\;И.\,В., Чернявский\;А.\,Л.}
    \BibTitle{Экспертные методы анализа и совершенствования систем управления}~//
    \BibJournal{Автоматика и телемеханика}. 2004. \No\,10. С.\,172--188.
\bibitem{DDPC_6}
    \BibAuthor{Дорофеюк\;А.\,А., Дорофеюк\;Ю.\,А., Покровская\;И.\,В., Чернявский\;А.\,Л.}
    \BibTitle{Метод независимой многовариантной экспертизы и его использование при решении прикладных задач}~//
    \BibJournal{Управление развитием крупномасштабных систем MLSD'2013): Труды Седьмой международной конференции},Т.\,38,
    М:~ИПУ РАН, 2013.  С.\,260--271.
\bibitem{Kram_7}
    \BibAuthor{Крамер\;Г.}
    Математические методы статистики. 
    М.: Мир, 1975. 648~с.
\bibitem{DGS_8}
    \BibAuthor{Дорофеюк\;Ю.\,А., Гольдовская\;М.\,Д., Спиро\;А.\,Г.}
    \BibTitle{Особенности компьютерной реализации и моделирования алгоритмов интеллектуального анализа сложноорганизованных данных.}~//
    \BibJournal{Управление развитием крупномасштабных систем (MLSD'2013): Материалы Седьмой международной конференции. Т. 2.},
    М:~ИПУ РАН, 2013.  С.\,328--331.

\bibitem{Bell_9}
    \BibAuthor{Bellman\;R.}
    \BibTitle{Dynamic Programming and Lagrange Multipliers.}~//
    \BibJournal{Proc. Nat. Acad. of Sc. USA}, 1956. Vol.42, No.\,10. Pp.\,767--769.

\end{thebibliography}


\section{References}
\renewcommand{\bibname}{}
\begin{thebibliography}{99}
\bibitem{DoBaDo_1}
    \BibAuthor{Dorofeyuk\;A.\,A., Bauman\;E.\,V., Dorofeyuk\;Yu.\,A.}\,2011. 
    \BibTitle{Metody intellektual'noy obrabotki informatsii na baze algoritmov stokhasticheskoy approksimatsii.}
    \BibJournal{Matematicheskie metody raspoznavaniya obrazov. 15-aya mezhdunarodnaya konferentsiya: Sb. dokladov.} Moscow. 108–-112.
   
\bibitem{Gold_2}
    \BibAuthor{Gol'dovskaya\;M.\,D., Dorofeyuk\;Yu.\,A., Kiseleva\;N.\,E.}\,2013. 
    \BibTitle{Metody strukturnogo analiza v prikladnykh zadachakh issledovaniya vremennykh ryadov.}
    \BibJournal{Problemy upravleniya.} 3:\,33--41.   

\bibitem{Bramu_3}
    \BibAuthor{Braverman\;E.\,M., Muchnik\;I.\,B.}\,1983.
    Strukturnye metody obrabotki empiricheskikh dannykh.
    Мoscow:~Nauka Publ., 464~p.

\bibitem{Dor_4}
    \BibAuthor{Dorofeyuk\;A.\,A.}\,2009.
    \BibTitle{Metodologiya ekspertno-klassifikatsionnogo analiza v zadachakh upravleniya i obrabotki slozhnoorganizovannykh dannykh (istoriya i perspektivy razvitiya).}
    \BibJournal{Problemy upravleniya.} 3.1:\,19--28.
    
\bibitem{DoPoChe_5}
    \BibAuthor{Dorofeyuk\;A.\,A., Pokrovskaya\;I.\,V., Chernyavskiy\;A.\,L.}\,2004.
    \BibTitle{Ekspertnye metody analiza i sovershenstvovaniya sistem upravleniya.}
    \BibJournal{Avtomatika i telemekhanika.} 10:\,172--188.
    
\bibitem{DDPC_6}
    \BibAuthor{Dorofeyuk\;A.\,A., Dorofeyuk\;Yu.\,A., Pokrovskaya\;I.\,V., Chernyavskiy\;A.\,L.}\,2013.
    \BibTitle{Metod nezavisimoy mnogovariantnoy ekspertizy i ego ispol'zovanie pri reshenii prikladnykh zadach.}
    \BibJournal{Upravlenie razvitiem krupnomasshtabnykh sistem MLSD’2013): Trudy Sed'moy mezhdunarodnoy konferentsii.} Moscow. 38:\,260--271.
    
\bibitem{Kram_7}
    \BibAuthor{Kramer\;G.}\,1975.
    Matematicheskie metody statistiki.
    Мoscow:~Mir Publ., 648~p.
    
\bibitem{DGS_8}
    \BibAuthor{Dorofeyuk\;Yu.\,A., Gol'dovskaya\;M.\,D., Spiro\;A.\,G.}\,2013.
    \BibTitle{Osobennosti komp'yuternoy realizatsii i modelirovaniya algoritmov intellektual'nogo analiza slozhnoorganizovannykh dannykh.}
    \BibJournal{Upravlenie razvitiem krupnomasshtabnykh sistem (MLSD’2013): Materialy Sed'moy mezhdunarodnoy konferentsii.} Moscow. 2:\,328--331.
    
\bibitem{Bell_9}
    \BibAuthor{Bellman\;R.}\,1956.
    \BibTitle{Dynamic Programming and Lagrange Multipliers.}
    \BibJournal{Proc. Nat. Acad. of Sc. USA.} 42(10):\,767--769.
\end{thebibliography}
% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
