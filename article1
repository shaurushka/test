% \noindent{\bf Keywords:}  nonlinear hyperbolic equations, Laplace transformation, Backlund transformation.
\documentclass[12pt]{umj}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\usepackage{url} %url
\usepackage{array} %in tabular
\usepackage{enumitem} %enumerate
\usepackage{graphicx} % \includegraphics
\usepackage{subfig} % figure environment
\usepackage[all]{xy} % xy environment
\usepackage[linesnumbered,ruled,vlined,algo2e,oldcommands]{algorithm2e} %algorithm2e

\def\udcs{519.25} %Здесь автор определяет УДК своей работы

\def\currentyear{2016}
\def\currentvolume{1}
\setcounter{tocdepth}{1}
\firstpage{1}
\subjclass{\udcs}

\newcommand{\eps}{\varepsilon}

\newcommand{\X}{\mathbb{X}}
\newcommand{\valid}{\bar{X}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\Y}{\mathbb{Y}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\Expect}{\mathsf{E}}
\newcommand{\cond}{\mspace{3mu}{|}\mspace{3mu}}

\newtheorem{lemma}{Лемма}
\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Определение}
\newtheorem{corollary}{Следствие}
\newtheorem{proposition}{Предложение}

\theoremstyle{definition}
\newtheorem{example}{Пример}
\newtheorem{remark}{Замечание}

\begin{document}
\shorthandoff{"}

%УДК \udcs
\thispagestyle{empty}

\title[Комбинаторные оценки переобучения пороговых решающих правил]
    {Комбинаторные оценки переобучения пороговых решающих правил}

\author{Ш.\,Х.~Ишкина}
%Указываем авторов
\address{Шаура Хабировна Ишкина,  %Имя, Отчество, Фамилия первого автора
\newline\hphantom{iii} ФИЦ <<Информатика и управление>> РАН,% Место работы
\newline\hphantom{iii} ул. Вавилова, д. 44/2 % Адрес (улица, дом, строение и т.п.)
\newline\hphantom{iii} 119333, г. Москва, Россия}%  Адрес (почтовый индекс, город, страна)  Адрес (почтовый индекс, город, страна)
\email{shaura-ishkina@yandex.ru}% Ваш электронный адрес для переписки

\thanks{\textsc{Sh.Kh. Ishkina, % Ivanov R.S. %  Ф.И.О. авторов на английском языке
 Combinatorial bounds of~probability of~overfitting of~threshold classifiers.}}% название статьи на английском языке
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект №\,15-37-50350 мол\_нр.}
\thanks{\copyright \ \textsc{Ишкина Ш. Х.} 2016}
\thanks{\it Поступила 25 августа 2009 г.}
% (указываем дату отправки, строка будет при получении изменена)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
{
\small
\begin{quote}
\noindent{\bf Аннотация.}
	Данная статья посвящена проблеме вычисления точной верхней оценки вероятности переобучения и функционала полного скользящего контроля непрерывной цепи~-- семейства, возникающего при~использовании пороговых решающих правил над непрерывными признаками в~алгоритмах классификации, в~частности, в~решающих деревьях, логических закономерностях, алгоритмах вычисления оценок.
    Предложен алгоритм, решающий поставленную задачу, полиномиальный по~общему числу объектов выборки, по~объему обучающей выборки и по~длине цепи.

\noindent{\bf Ключевые слова:}{ пороговый классификатор, оценка обобщающей способности, комбинаторная теория, вероятность переобучения, полный скользящий контроль, CCV}

\medskip
\noindent{\bf Abstract.}
	In this paper we propose exact combinatorial generalization bounds of family of chain classifiers.
    This family has the same error matrix as the threshold classifier when the threshold takes all the possible values.
    These generalization bounds are calculated under the assumption that the overall set of samples is finite and every train sample is generated according to the uniform distribution.
    Earlier this kind of tight bounds existed only for particular type of families like monotonic and unimodal chains.
    It is shown that these bounds are lower than all the existing upper bounds of generalization ability.
	Computational complexity of the algorithm calculating overfit probability and complete cross-validation is polynomial by the overall set size, train sample size and the amount of the classifiers that is not necessarily equal to the train sample size.

\noindent{\bf Keywords:} {threshold classifiers, generalization bounds,  combinatorial theory, probability of overfitting, complete cross-validation, CCV}

\bigskip
\noindent{\bf Mathematics Subject Classification:} 68Q32, 60C05

\end{quote}
}

\section{Введение}
\setcounter{section}{1}
\setcounter{equation}{0}
Пусть дана бинарная матрица, строки которой называются объектами, столбцы – классификаторами.
Если в ячейке матрицы $(x, a)$ лежит 1, то говорят, что соответствующий классификатор $a$ ошибается на объекте $x$, иначе говорят, что классификатор на объекте не ошибается.
Ставится задача по известному случайному множеству строк~$X$ фиксированной длины, называемому обучающей выборкой, выбрать по некоторому правилу классификатор, и оценить число его ошибок на дополнении к $X$~-- контрольной выборке.
Данную задачу можно рассматривать как задачу выбора по неполной информации~\cite{gilbert}.

Определяется функционал переобучения, равный отклонению доли ошибок классификатора на обучающей выборке от доли его ошибок на контрольной выборке.
Для оценки переобучения классификатора определяется функционал вероятности переобучения, равный вероятности того, что переобучение выбранного классификатора превосходит некоторый порог, и функционал полного скользящего контроля, равный математическому ожиданию доли ошибок классификатора на контрольной выборке~\cite{kohavi}.


%Ставится задача получения верхней оценки вероятности переобучения~${Q_\eps \leqslant \eta(\eps)}$.
%
%В задаче классификации на основе информации о конечной выборке объектов, называемой обучающей, необходимо построить алгоритм, называемый классификатором, который для нового объекта будет выдавать метку класса.
%
%Качество классификатора часто оказывается значительно хуже на независимой контрольной выборке объектов, чем на обучающей выборке.
%В таких случаях говорят, что произошло переобучение классификатора.

Получение верхних оценок переобучения классификатора на основе информации об обучающей выборке и структуре матрицы ошибок остается одной из основных задач теории статистического обучения, начиная с работы~\cite{vapnik}.
%В~качестве характеристик переобучения используются функционалы вероятности переобучения, то есть значительного отклонения ошибки классификатора на обучающей выборке от его истинной ошибки, и полного скользящего контроля.

%Оценки переобучения часто используются для отбора признаков, оптимизации сложности и структуры алгоритма классификации.
Проблема в том, что большинство известных оценок используют только размерные характеристики матрицы ошибок, в силу чего оказываются завышенными на несколько порядков, что приводит к необоснованному требованию увеличивать длину обучающей выборки до $10^5$~-- $10^8$ объектов~\cite{voron2004cmmp}.
%Кроме того, завышенные оценки не дают возможности исследовать явление переобучения, оценивать и контролировать его значения при решении реальных задач.

В комбинаторной теории \cite{vokov09ras} для вероятности переобучения получена более точная оценка расслоения-связности~\cite{ivahnenko11premi}, учитывающая особенности способа построения классификатора по обучающей выборке, а также структурные особенности матрицы ошибок~-- эффекты расслоения и связности.
Имеются уточнения данной оценки за счет учета попарного взаимодействия классификаторов~\cite{sokolov} и за счет учета сходства классификаторов~\cite{Tolstikhin2013}, но оценки по-прежнему остаются завышенными.

В комбинаторной теории вероятностью переобучения называют долю разбиений конечного множества объектов на обучающую и контрольную выборки фиксированной длины, при которых произошло переобучение.
%Данное определение ранее появлялось в работе~\cite{haussler} для частного случая контрольной выборки, состоящей из одного объекта.
%Точность эмпирических оценок переобучения, полученных методом Монте-Карло, зависит от числа случайных разбиений.
Проблема заключается в том, что вычисление точных оценок по определению требует экспоненциального по общему количеству объектов перебора всех возможных разбиений.
%Но для некоторых модельных примеров матрицы ошибок удается аналитически вычислить точные оценки вероятности переобучения.
%К настоящему времени точные оценки получены для слоев и интервалов булева куба, монотонных и унимодальных цепей~\cite{Voron10pria-eng} и многомерных сетей \cite{botov_pria}, хэмминговых шаров и некоторых их разреженных подмножеств \cite{Tolstikhin2010iip}.
%Разработан теоретико-групповой подход \cite{Frei10Sym}, который позволяет получать точные оценки для семейств с произвольными симметриями.

%В работе \cite{Botov2009MMPR} предложен способ аппроксимации вероятности переобучения стандартных методов классификации (нейронных сетей, решающих деревьев, ближайшего соседа) на реальных задачах с помощью монотонных сетей подходящей размерности.
%Оценки переобучения могут использоваться в качестве критерия отбора признаков при построении элементарных конъюнкций в логических алгоритмах классификации~\cite{ivahnenko11premi}  или в~качестве критерия ветвления при отборе признаков в решающих деревьях~\cite{botov_pria}.

В статье рассмотрена последовательность векторов ошибок, где множества объектов, по которым отличаются соседние вектора, не пересекаются. Данная последовательность называется прямой.
Предложен алгоритм вычисления точных оценок вероятности переобучения и полного скользящего контроля прямой последовательности, полиномиальный по общему количеству объектов.

Практический интерес связан с тем, что прямые последовательности возникают в задачах классификации при использовании одномерного порогового решающего правила над вещественнозначным или порядковым признаком.

%Практический интерес связан с~тем, что такие семейства .
 %в~алгоритмах классификации, в~частности, в~решающих деревьях, логических закономерностях \cite{zhuravlev_ryazanov}, алгоритмах вычисления оценок \cite{zhuravlev}, а также при построении линейных классификаторов методом покоординатной оптимизации.

Теоретический интерес связан с~тем, что в~рамках комбинаторного подхода до~сих пор не удавалось получать точные оценки переобучения для прямых последовательностей общего вида.
Ранее рассматривались только прямые цепи~-- прямые последовательности, в которых соседние векторы ошибок различаются по одному объекту.
Точные оценки вероятности переобучения были известны только для искусственных частных случаев прямых цепей~\cite{Voron10pria-eng}, не встречающихся в реальных задачах, а также верхние, не всегда достижимые, оценки полного скользящего контроля~\cite{guz2011}.

%\subsection{Прошлое}
%В комбинаторной теории вероятностью переобучения называют долю разбиений конечного множества объектов на обучающую и контрольную выборки фиксированной длины, при которых произошло переобучение.
%Данное определение ранее появлялось в работе~\cite{haussler} для частного случая контрольной выборки, состоящей из одного объекта.
%
%Точность эмпирических оценок функционалов обобщающей способности, полученных методом Монте-Карло, зависит от числа случайных разбиений.
%Вычисление точных оценок по определению требует экспоненциального по общему количеству объектов перебора всех возможных разбиений.
%Но для некоторых модельных семейств классификаторов удается аналитически вычислить точные оценки вероятности переобучения.
%К настоящему времени точные оценки получены для слоев и интервалов булева куба, монотонных и унимодальных цепей~\cite{Voron10pria-eng} и многомерных сетей \cite{botov_pria}, хэмминговых шаров и некоторых их разреженных подмножеств \cite{Tolstikhin2010iip}.
%Разработан теоретико-групповой подход \cite{Frei10Sym}, который позволяет получать точные оценки для семейств с произвольными симметриями.
%
%В работе \cite{Botov2009MMPR} предложен способ аппроксимации вероятности переобучения стандартных методов классификации (нейронных сетей, решающих деревьев, ближайшего соседа) на реальных задачах с помощью монотонных сетей подходящей размерности.
%Оценки переобучения могут использоваться в качестве критерия отбора признаков при построении элементарных конъюнкций в логических алгоритмах классификации~\cite{ivahnenko11premi}  или в~качестве критерия ветвления при отборе признаков в решающих деревьях~\cite{botov_pria}.
%
%В данной статье
%предложен алгоритм вычисления точных оценок вероятности переобучения произвольных прямых цепей, полиномиальный по общему количеству объектов.
%Практический интерес связан с~тем, что такие семейства возникают при~использовании пороговых решающих правил в~алгоритмах классификации, в~частности, в~решающих деревьях, логических закономерностях \cite{zhuravlev_ryazanov}, алгоритмах вычисления оценок \cite{zhuravlev}, а также при построении линейных классификаторов методом покоординатной оптимизации.
%
%Теоретический интерес связан с~тем, что в~рамках комбинаторного подхода до~сих пор не удавалось  получать точные оценки переобучения для~цепей произвольного вида.
%Точные оценки были известны только для частных случаев цепей~\cite{Voron10pria-eng}, а также верхняя, не всегда достижимая, оценка обобщающей способности для~семейства пороговых решающих правил~\cite{guz2011}, но ее вычисление проводится в предположении, что порог пробегает все возможные значения, и данный метод к~более общему случаю неприменим.

\subsection{Основные определения.}
Пусть задано конечное множество ${\X = \{ x_1, \dots, x_L \}},$ элементы которого называются \emph{объектами}, и конечное множество~$\A,$ элементы которого называются \emph{классификаторами}.
Множество $\A$ называется \emph{семейством классификаторов.}

Пусть задана функция ${I \colon \A \times \X \rightarrow \{0, \, 1\}}$, называемая \emph{индикатором ошибки}.
Если~${I(a, x) = 1,}$ то говорят, что на~объекте~$x$ классификатор~$a$~\emph{допускает ошибку}.
Если~$I(a, x) = 0$, то говорят, что классификатор~$a$~\emph{не ошибается} на~данном объекте.

Предполагается, что каждому классификатору~${a \in\A}$~взаимно однозначно соответствует его вектор ошибок~${(I(a, x_i))_{i = 1}^L}$, то есть два классификатора с~одинаковыми векторами ошибок отождествляются.
Далее через~$a$~будет обозначаться как сам классификатор, так~и его вектор ошибок.

\emph{Числом ошибок} классификатора~$a$~на~выборке $X \subset \X$ называется величина
\[
n(a, X) = \sum_{x \in X} I(a, x).
\]

\emph{Частотой ошибок} классификатора $a$ на выборке $X \subset \X$ называется величина \[\nu(a, X) = \frac{n(a, X)}{|X|}.\]
%где через $|X|$ обозначен объем выборки $X$.

Пусть множество~$\X$~разбито на~две выборки~$X \in [X]^l$ и~${\valid = \X \backslash X.}$
Выборка~$X$~называется \emph{обучающей}, выборка~$\valid$~-- \emph{контрольной}.
\emph{Переобученностью} классификатора~$a$~на~разбиении~$(X, \valid)$ называется величина
\[\delta(a, X) = \nu(a, \valid) - \nu(a, X).\]

Пусть $[X]^l$~-- множество всех выборок $X \subset \X$ без возвращения объема~$l < L$.
Введем на~$[X]^l$~равномерное распределение
\[\Prob(X) = \frac{1}{C_L^l}.\]


%\emph{Эмпирической оценкой} некоторой функции $\phi(X, \valid)$, не зависящей от порядка элементов в~выборках $X$ и $\valid$, называется величина $\hat{\Expect} \phi(X, \valid)$, полученная методом Монте-Карло путем усреднения по некоторому случайному подмножеству выборок $N \subset [X]^l$
%\[
%\hat{\Expect} \phi(X, \valid) = \frac{1}{|N|} \sum_{X \in N} \phi(X, \valid).
%\]
%TODO найти подходящее место
%Недостаток эмпирических оценок заключается в~низкой точности и большой вычислительной сложности.

\emph{Методом обучения} называется отображение $\mu \colon 2^{\X} \rightarrow \A$, которое обучающей выборке ставит в~соответствие классификатор из~заданного семейства.
\emph{Пессимистичной минимизацией эмпирического риска (ПМЭР)} называется  метод обучения, который выбирает классификатор, допускающий наименьшее число ошибок на~обучающей выборке, а в~случае неоднозначности среди таких классификаторов выбирает классификатор с~наибольшим числом ошибок на~множестве~$\X$~\cite{Voron10pria-eng}.

Для фиксированного метода обучения~$\mu,$~семейства классификаторов~$\A,$~множества~$\X$~и объема обучающей выборки~$l$ \emph{вероятностью переобучения метода}~$\mu$ называется функционал
\[Q_{\eps}(\mu, \A, \X, l) = \Prob[\delta(\mu X, X) \geqslant \eps] = \frac{1}{C_L^l} \sum_{X \in [X]^l} [\delta(\mu X, X) \geqslant \eps].\]

\emph{Полным скользящим контролем} называется функционал, равный математическому ожиданию числа ошибок метода обучения на~контрольной выборке:
\[ CCV(\mu, \A, \X, l) = \Expect \nu(\mu X, \valid) = \frac{1}{C_L^l} \sum_{X \in [X]^l} \nu(\mu X, \valid).\]

%Для краткости будем опускать параметры, от~которых зависят данные показатели, и записывать~$Q_{\eps}$ и~$CCV$.

%Функционалы вероятности переобучения и полного скользящего контроля характеризуют обобщающую способность метода обучения.
Получение оценок $Q_{\eps}$ и $CCV$ проводится в~два этапа.
На~первом этапе предполагается, что задана \emph{матрица ошибок}~${I(\A, \X) = \bigl( I(a, x)\bigr)_{a \in \A, x \in \X}}$  размера~$L{\times}D,$ где $D = |\A|$, строки которой соответствуют объектам множества~$\X$, а столбцами являются вектора ошибок классификаторов семейства~$\A$~на~этих объектах.
В~результате получаются комбинаторные оценки, зависящие от~некоторых статистических свойств этой матрицы.
Вторым этапом является оценка статистических свойств матрицы $I(\A, \X)$ по случайной подматрице $I(\A, X)$.
Эмпирический способ решения задачи второго этапа предложен в~\cite{ivahnenko11mmro}.
В данной работе рассматривается только первый этап.

\subsection{Прямые последовательности классификаторов.}
%\begin{definition}
%\emph{Расстоянием между классификаторами}  называется расстояние Хемминга между их векторами ошибок:
%\[\rho(a, a') = \sum_{i = 1}^{L}[I(a, x_i) \neq I(a', x_i)], \quad \forall a, a' \in \A,\]
%где квадратные скобки обозначают предикат [истина] = 1, [ложь] = 0.
%\end{definition}

Рассмотрим множества объектов, по которым различаются соседние классификаторы семейства $\A = \{a_0, \dots, a_P\}$:
\begin{align}
\label{eq:edgesdef}
G_p = \{x \in \X \cond I(a_p, x) \neq I(a_{p + 1}, x) \}, \quad p = 0, \dots, P - 1.
\end{align}

\begin{definition}
\textit{Прямой последовательностью} называется семейство классификаторов, обладающее следующим свойством:
\[
G_p \cap G_d = \emptyset, \quad \forall \, p, d = 0, \dots, P - 1.
\]
\end{definition}

\begin{definition}
Прямая последовательность классификаторов ${\A = \{a_0, \dots, a_P\}}$ называется \textit{прямой цепью}, если каждая пара соседних классификаторов различается по одному объекту:
\begin{align*}
|G_p| = 1, \: p = 0, \dots, P - 1
\end{align*}
%\rho(a_p, a_{p + 1}) = 1, \: p = 0, \dots, P - 1 \Leftrightarrow
%Если ${\rho(a_0, a_P) = P},$ то цепь называется \emph{прямой}.

Число~$P$~называется \textit{длиной} прямой цепи~$\A$.
\end{definition}

\begin{example}
Рассмотрим числовое множество $\X \subset \RR$ и семейство пороговых классификаторов
\begin{align*}
\A = \{ a_\theta(x) = [x \geqslant \theta] \colon \theta \in \RR, x \in \X\},
\end{align*}
где квадратные скобки обозначают предикат [истина] = 1, [ложь] = 0.

Пусть задана некоторая функция $y: \X \rightarrow \{0, 1\}$, определяющая для каждого объекта его класс $0$~или $1$.
Если значение классификатора~$a \in \A$ на объекте $x \in \X$ совпадает с~меткой~$y(x)$ класса, то полагаем~${I(a, x) = 0}$, в противном случае~${I(a, x) = 1}$.

Рассмотрим вариационный ряд множества~$\X$, то есть последовательность значений~${x \in \X}$, расположенных в~порядке неубывания:
\begin{align}
\label{def:var_series}
x_{(1)} \leqslant x_{(2)} \leqslant \dots \leqslant x_{(L)}.
\end{align}

Выберем только те значения порогов~$\theta$, которые всеми возможными и различными способами разбивают вариационный ряд~\eqref{def:var_series}.
Тогда семейство~$\A$ конечно и~$|\A| \leqslant L + 1.$

В общем случае семейство $\A$ является прямой последовательностью. Если вариационный ряд не~имеет связок, то есть все неравенства~в~\eqref{def:var_series}~строгие, то семейство $\A$~является прямой цепью.

\begin{figure}[t]
	\begin{center}
	%\subfloat[Прямая цепь общего вида]
%	{
	\fbox{
    \begin{xy}<0.7ex,0ex>:
        \POS(0,100)\ar@{->}(49,100)*{}
        \POS(49,100),+/u1em/*{x}
        \POS(5,100)*@{*},+/u1em/*{x_1}
        \POS(11,100)*@{*},+/u1em/*{x_2}
        \POS(17,100)*@{o},+/u1em/*{x_3}
        \POS(23,100)*@{o},+/u1em/*{x_4}
        \POS(29,100)*@{o},+/u1em/*{x_5}
        \POS(35,100)*@{*},+/u1em/*{x_6}
        \POS(41,100)*@{o},+/u1em/*{x_7}
        \POS(2,100)*@{|},+/d1em/*++{a_0}="t0"
        \POS(8,100)*@{|},+/d1em/*++{a_1}="t1"
        \POS(14,100)*@{|},+/d1em/*++{a_2}="t2"
        \POS(20,100)*@{|},+/d1em/*++{a_3}="t3"
        \POS(26,100)*@{|},+/d1em/*++{a_4}="t4"
        \POS(32,100)*@{|},+/d1em/*++{a_5}="t5"
        \POS(38,100)*@{|},+/d1em/*++{a_6}="t6"
        \POS(44,100)*@{|},+/d1em/*++{a_7}="t7"
        %
        \POS(2,84)\ar@{.}(44,84)*{}
        \POS(2,86)\ar@{.}(44,86)*{}
        \POS(2,88)\ar@{.}(44,88)*{}
        \POS(2,90)\ar@{.}(44,90)*{}
        \POS(2,92)\ar@{.}(44,92)*{}
        %
        \POS(2,90)*@{*}="a0"
        \POS(8,88)*@{*}="a1"\ar@{-}"a0"
        \POS(14,86)*@{*}="a2"\ar@{-}"a1"
        \POS(20,88)*@{*}="a3"\ar@{-}"a2"
        \POS(26,90)*@{*}="a4"\ar@{-}"a3"
        \POS(32,92)*@{*}="a5"\ar@{-}"a4"
        \POS(38,90)*@{*}="a6"\ar@{-}"a5"
        \POS(44,92)*@{*}="a7"\ar@{-}"a6"
        %
        \POS(2,80)*++{1}\ar @{.} "t0"
        \POS(8,80)*++{0}\ar @{.} "t1"
        \POS(14,80)*++{0}\ar @{.} "t2"
        \POS(20,80)*++{0}\ar @{.} "t3"
        \POS(26,80)*++{0}\ar @{.} "t4"
        \POS(32,80)*++{0}\ar @{.} "t5"
        \POS(38,80)*++{0}\ar @{.} "t6"
        \POS(44,80)*++{0}\ar @{.} "t7"
        %
        \POS(2,76)*{1}\POS(8,76)*{1}\POS(14,76)*{0}\POS(20,76)*{0}\POS(26,76)*{0}\POS(32,76)*{0}\POS(38,76)*{0}\POS(44,76)*{0}
        \POS(2,72)*{0}\POS(8,72)*{0}\POS(14,72)*{0}\POS(20,72)*{1}\POS(26,72)*{1}\POS(32,72)*{1}\POS(38,72)*{1}\POS(44,72)*{1}
        \POS(2,68)*{0}\POS(8,68)*{0}\POS(14,68)*{0}\POS(20,68)*{0}\POS(26,68)*{1}\POS(32,68)*{1}\POS(38,68)*{1}\POS(44,68)*{1}
        \POS(2,64)*{0}\POS(8,64)*{0}\POS(14,64)*{0}\POS(20,64)*{0}\POS(26,64)*{0}\POS(32,64)*{1}\POS(38,64)*{1}\POS(44,64)*{1}
        \POS(2,60)*{1}\POS(8,60)*{1}\POS(14,60)*{1}\POS(20,60)*{1}\POS(26,60)*{1}\POS(32,60)*{1}\POS(38,60)*{0}\POS(44,60)*{0}
        \POS(2,56)*{0}\POS(8,56)*{0}\POS(14,56)*{0}\POS(20,56)*{0}\POS(26,56)*{0}\POS(32,56)*{0}\POS(38,56)*{0}\POS(44,56)*{1}
        %
        \POS(-6,92)*{\scriptstyle n(a_p,\X)}
        \POS(-6,80)*{x_1}\POS(-6,76)*{x_2}\POS(-6,72)*{x_3}\POS(-6,68)*{x_4}\POS(-6,64)*{x_5}\POS(-6,60)*{x_6}\POS(-6,56)*{x_7}
    \end{xy}}
%	}
	%\\
%	\subfloat[Монотонная возрастающая цепь]
%	{\label{fig:monot_chain}
%	\fbox{\begin{xy}<0.7ex,0ex>:
%            \POS(0,100)\ar@{->}(43,100)*{}
%            \POS(42,100),+/u1em/*{x}
%            \POS(5,100)*@{*},+/u1em/*{x_1}
%            \POS(11,100)*@{*},+/u1em/*{x_2}
%            \POS(17,100)*@{o},+/u1em/*{x_3}
%            \POS(23,100)*@{o},+/u1em/*{x_4}
%            \POS(29,100)*@{o},+/u1em/*{x_5}
%            \POS(35,100)*@{o},+/u1em/*{x_6}
%            \POS(14,100)*@{|},+/d1em/*++{a_2}="t0"
%            \POS(20,100)*@{|},+/d1em/*++{a_3}="t1"
%            \POS(26,100)*@{|},+/d1em/*++{a_4}="t2"
%            \POS(32,100)*@{|},+/d1em/*++{a_5}="t3"
%            \POS(38,100)*@{|},+/d1em/*++{a_6}="t4"
%            %
%            \POS(14,80)*++{0}\ar @{.} "t0"
%            \POS(20,80)*++{0}\ar @{.} "t1"
%            \POS(26,80)*++{0}\ar @{.} "t2"
%            \POS(32,80)*++{0}\ar @{.} "t3"
%            \POS(38,80)*++{0}\ar @{.} "t4"
%            %
%            \POS(14,84)\ar@{.}(38,84)*{}\POS(14,84)*@{*}="a0"
%            \POS(14,86)\ar@{.}(38,86)*{}\POS(20,86)*@{*}="a1"\ar@{-}"a0"
%            \POS(14,88)\ar@{.}(38,88)*{}\POS(26,88)*@{*}="a2"\ar@{-}"a1"
%            \POS(14,90)\ar@{.}(38,90)*{}\POS(32,90)*@{*}="a3"\ar@{-}"a2"
%            \POS(14,92)\ar@{.}(38,92)*{}\POS(38,92)*@{*}="a4"\ar@{-}"a3"
%            %
%            \POS(14,76)*{0}\POS(20,76)*{0}\POS(26,76)*{0}\POS(32,76)*{0}\POS(38,76)*{0}
%            \POS(14,72)*{0}\POS(20,72)*{1}\POS(26,72)*{1}\POS(32,72)*{1}\POS(38,72)*{1}
%            \POS(14,68)*{0}\POS(20,68)*{0}\POS(26,68)*{1}\POS(32,68)*{1}\POS(38,68)*{1}
%            \POS(14,64)*{0}\POS(20,64)*{0}\POS(26,64)*{0}\POS(32,64)*{1}\POS(38,64)*{1}
%            \POS(14,60)*{0}\POS(20,60)*{0}\POS(26,60)*{0}\POS(32,60)*{0}\POS(38,60)*{1}
%            %
%            \POS(6,92)*{\scriptstyle n(a_p,\X)}
%            \POS(4,80)*{x_1}\POS(4,76)*{x_2}\POS(4,72)*{x_3}\POS(4,68)*{x_4}\POS(4,64)*{x_5}\POS(4,60)*{x_6}
%        \end{xy}}
%
%    }
%    \qquad
%    \subfloat[Унимодальная цепь~-- прямая цепь, состоящая из~двух участков монотонности]
%    {
%    \label{fig:combined_from_monot_chain}
%        \fbox{\begin{xy}<0.7ex,0ex>:
%            \POS(0,100)\ar@{->}(43,100)*{}
%            \POS(42,100),+/u1em/*{x}
%            \POS(5,100)*@{*},+/u1em/*{x_1}
%            \POS(11,100)*@{*},+/u1em/*{x_2}
%            \POS(17,100)*@{o},+/u1em/*{x_3}
%            \POS(23,100)*@{o},+/u1em/*{x_4}
%            \POS(29,100)*@{o},+/u1em/*{x_5}
%            \POS(35,100)*@{o},+/u1em/*{x_6}
%            \POS(2,100)*@{|},+/d1em/*++{a_0}="t0"
%            \POS(8,100)*@{|},+/d1em/*++{a_1}="t1"
%            \POS(14,100)*@{|},+/d1em/*++{a_2}="t2"
%            \POS(20,100)*@{|},+/d1em/*++{a_3}="t3"
%            \POS(26,100)*@{|},+/d1em/*++{a_4}="t4"
%            \POS(32,100)*@{|},+/d1em/*++{a_5}="t5"
%            \POS(38,100)*@{|},+/d1em/*++{a_6}="t6"
%            %
%            \POS(2,84)\ar@{.}(38,84)*{}
%            \POS(2,86)\ar@{.}(38,86)*{}
%            \POS(2,88)\ar@{.}(38,88)*{}
%            \POS(2,90)\ar@{.}(38,90)*{}
%            \POS(2,92)\ar@{.}(38,92)*{}
%            %
%            \POS(2,88)*@{*}="a0"
%            \POS(8,86)*@{*}="a1"\ar@{-}"a0"
%            \POS(14,84)*@{*}="a2"\ar@{-}"a1"
%            \POS(20,86)*@{*}="a3"\ar@{-}"a2"
%            \POS(26,88)*@{*}="a4"\ar@{-}"a3"
%            \POS(32,90)*@{*}="a5"\ar@{-}"a4"
%            \POS(38,92)*@{*}="a6"\ar@{-}"a5"
%            %
%            \POS(2,80)*++{1}\ar @{.} "t0"
%            \POS(8,80)*++{0}\ar @{.} "t1"
%            \POS(14,80)*++{0}\ar @{.} "t2"
%            \POS(20,80)*++{0}\ar @{.} "t3"
%            \POS(26,80)*++{0}\ar @{.} "t4"
%            \POS(32,80)*++{0}\ar @{.} "t5"
%            \POS(38,80)*++{0}\ar @{.} "t6"
%            %
%            \POS(2,76)*{1}\POS(8,76)*{1}\POS(14,76)*{0}\POS(20,76)*{0}\POS(26,76)*{0}\POS(32,76)*{0}\POS(38,76)*{0}
%            \POS(2,72)*{0}\POS(8,72)*{0}\POS(14,72)*{0}\POS(20,72)*{1}\POS(26,72)*{1}\POS(32,72)*{1}\POS(38,72)*{1}
%            \POS(2,68)*{0}\POS(8,68)*{0}\POS(14,68)*{0}\POS(20,68)*{0}\POS(26,68)*{1}\POS(32,68)*{1}\POS(38,68)*{1}
%            \POS(2,64)*{0}\POS(8,64)*{0}\POS(14,64)*{0}\POS(20,64)*{0}\POS(26,64)*{0}\POS(32,64)*{1}\POS(38,64)*{1}
%            \POS(2,60)*{0}\POS(8,60)*{0}\POS(14,60)*{0}\POS(20,60)*{0}\POS(26,60)*{0}\POS(32,60)*{0}\POS(38,60)*{1}
%            %
%            \POS(-6,92)*{\scriptstyle n(a_p,\X)}
%            \POS(-6,80)*{x_1}\POS(-6,76)*{x_2}\POS(-6,72)*{x_3}\POS(-6,68)*{x_4}\POS(-6,64)*{x_5}\POS(-6,60)*{x_6}
%        \end{xy}}}
	\end{center}
	\caption{Пример прямой цепи}
    \label{fig:simple_chain}
\end{figure}


На~рисунке~\ref{fig:simple_chain} изображен пример прямой цепи.
По~горизонтальной оси отложены объекты множества~$\X$.
Объекты из разных классов обозначены точками~$\bullet$~и~$\circ$.
Пороги~$\theta$~расположены между соседними объектами, каждому значению порога соответствует свой классификатор.
Ниже оси изображен график числа ошибок классификаторов и~матрица ошибок семейства.

\end{example}

\begin{definition}
Прямая цепь $\A = \{a_0, \dots, a_P\}$ называется \textit{возрастающей}, если каждый классификатор~$a_p$~допускает~$m + p$~ошибок на~множестве~$\mathbb{X}.$
Аналогично, прямая цепь $\A = \{a_0, \dots, a_P\}$ называется \textit{убывающей}, если каждый классификатор~$a_p$~допускает~${m - p}$~ошибок на~множестве~$\X$.
Прямую цепь~$\A$~будем называть \textit{монотонной}, если она является убывающей или возрастающей.

Если прямая цепь~$\A$~состоит из~нескольких участков монотонности, то~убывание или возрастание участка монотонной цепи определяется в~направлении от~первого классификатора цепи~$\A$~к~последнему.
\end{definition}

\begin{example}
В~цепи, изображенной на рисунке~\ref{fig:simple_chain}, имеется четыре участка монотонности: $\{a_0, a_1, a_2\}$ и $\{a_5, a_6\}$~-- убывающие цепи, $\{a_2, a_3, a_4, a_5\}$ и $\{a_6, a_7\}$~-- возрастающие.
\end{example}


%\begin{example}
%\label{ex:overfitting_comparison}
%На~рисунке \ref{fig:various_simple_chains} изображены прямые цепи различной формы и горизонтальными линиями показаны эмпирические оценки частоты ошибок ПМЭР на~обучающей выборке $\hat{\Expect} \nu (\mu X, X)$ и на контрольной $\hat{\Expect} \nu (\mu X, \valid)$.
%Чем больше разность между ними, равная $\hat{\Expect} \delta (\mu X, X)$, тем сильнее переобучается семейство.
%Условия эксперимента:~${L = 200, \, l = 100, \, \eps = 0.05}$.
%Эмпирические оценки вычислялись методом Монте-Карло по~$10^5$ разбиениям.
%
%\emph{Слоем} называется множество классификаторов, допускающих на~генеральной выборке равное число ошибок.
%Чем меньше ошибок допускает классификатор, тем ниже его слой.
%Данный эксперимент показывает, что одни семейства переобучаются значительно сильнее, чем другие: переобучение тем выше, чем больше классификаторов находится в нижних слоях семейства и чем более они различны.
%
%Вследствие этого ставится задача точного вычисления вероятности переобучения и полного скользящего контроля прямой цепи общего вида.
%\end{example}
%
%\begin{figure}[t]
%\begin{center}
%\includegraphics[width=\textwidth]{chain_examples.eps}
%\end{center}
%\caption{Эмпирические оценки частоты ошибок ПМЭР на~обучающей и контрольной выборках для~прямых цепей различной формы, вычисленные методом Монте-Карло по~$10^5$ разбиениям.
%Условия эксперимента:~${L = 200, \, l = 100, \, \eps = 0.05.}$
%Минимальная частота ошибок равна~${0.25}$ для~всех цепей. }
%\label{fig:various_simple_chains}
%\end{figure}

\subsection{Постановка задачи.}

%Точность эмпирических оценок переобученности семейства зависит от количества разбиений, по которым проводится усреднение. Получение оценок по определению требует экспоненциального по~объему~$L$ генеральной выборки перебора разбиений.

Пусть даны метод обучения $\mu$ ПМЭР, множество $\X$ мощности $L$, объем обучающей выборки $l$. Для произвольной прямой последовательности $\A$ длины~$P$ ставится задача точного вычисления вероятности переобучения и полного скользящего контроля за полиномиальное по~$L$ время.
%~в данной работе будет предложен алгоритм  сложностью~${O(P^6 + Pl^3 + L)},$ где $l$~-- объем обучающей выборки.

\section{Переобучение произвольного семейства}

%TODO найти подходящее место для нее
\emph{Гипергеометрической функцией распределения} называется величина
\[H^{l, m}_L(s) = \frac{1}{C_L^l} \sum_{i = 0}^{\min \{\lfloor s \rfloor, l, m\} } C_m^i C_{L - m}^{l - i},\]
где через~$\lfloor x \rfloor$~обозначена целая часть~$x,$ то есть наибольшее целое число, не~превосходящее~$x$.
Гипергеометрическая функция распределения $H^{l, m}_L(s)$ для~данного множества~$\X$~мощности~$L$~и выборки $X_0 \subset \X$ объема~$m$~равна доле выборок множества~$\X$ объема~$l,$ содержащих не более~$s$~элементов из $X_0$.
Будем полагать~${C_n^i = 0}$ при~невыполнении условия $0 \leqslant i \leqslant n$.

Пусть дано произвольное подмножество ${\D \subseteq \X}$ множества~$\X$.
Каждое разбиение~${(X, \valid)}$ множества~${\X = X \sqcup \valid}$ индуцирует разбиение~${(X \cap \D, \valid \cap \D)}$ подмножества~$\D$.
Также любая пара разбиений~${(D', \bar{D}')}$ и~$(D'', \bar{D}'')$~подмножеств~${\D' \subseteq \X}$ и~${\D'' = \X \backslash \D'}$~соответственно определяет разбиение~$(X, \valid)$~множества~$\X$ по~правилу:~${X = D' \cup D''}$ и~${\valid = \bar{D}' \cup \bar{D}''}$.

Пусть дано произвольное семейство классификаторов~$\A = \{a_0, \ldots, a_P\}$.

Назовем классификаторы $a$ и $a'$ \emph{неразличимыми} на~подмножестве $X_0 \subseteq \X$, если ограничения векторов ошибок классификаторов на~данное подмножество совпадают, то есть
\[
I(a, x) = I(a', x), \; \forall x \in X_0 .
\]

Через~$\D$~обозначим множество объектов, по которым различимы классификаторы семейства:
%TODO
\begin{align}
\label{d_definition}
\D = \sqcup_{p = 0}^{P - 1} G_p = \{x \in \X \cond \exists \, a, a' \in \A \colon  I(a, x) \neq I(a', x)\},
\end{align}
где множества $G_p$ определяются по~\eqref{eq:edgesdef}.

Множество~${\N = \X \setminus \D}$ назовем \emph{нейтральным}.
На~множестве~$\N$ классификаторы семейства неразличимы и допускают одинаковое число ошибок.
Обозначим это число через~$m$.
Через~$m_p$ обозначим число ошибок классификатора~$a_p$ на~множестве~$\D$:
\begin{align}
\label{errors_count_definition}
&m = n(a, \N), a \in \A, \\
&m_p = n(a_p, \D). \nonumber
\end{align}
%Таким образом, классификатор~$a_p$~допускает~${n(a_p, \X) = m + m_p}$ ошибок на~выборке~$\X$.

%\subsection{Сведение задачи к нахождению числа разбиений.}
Сведем задачу вычисления вероятности переобучения и полного скользящего контроля к~нахождению числа разбиений множества~$\D$ с~некоторыми ограничениями.

Обозначим через~$N_p(t, e)$~число разбиений множества~$\N,$~таких, что выполнено~${\delta(a_p, X) \geqslant \eps}$, и через~$D_p(t, e)$~-- число разбиений множества~$\D,$~таких, что выполнено~${\mu X = a_p}$:
\begin{align*}
N_p(t, e) &= \# \{(X \cap \N, \valid \cap \N)\, |\, \delta(a_p, X) \geqslant \eps,\, t = |X \cap \D|,\, e = n(a_p, X \cap \D) \}, \\
D_p(t, e) &= \# \{(X \cap \D, \valid \cap \D)\, |\, \mu X = a_p,\, t = |X \cap \D|,\, e = n(a_p, X \cap \D) \}.
\end{align*}

\begin{theorem}
Для~метода обучения $\mu$ ПМЭР, множества~$\X$~мощности~$L,$~объема обучающей выборки~$l,$ произвольного семейства классификаторов~$\A = \{a_0, \ldots, a_P\}$, точности~$\eps \in (0,\, 1)$ выражение для вероятности переобучения имеет вид
\begin{align}
Q_{\eps}
=
\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{(t, e) \in \Psi_p} D_p(t, e) N_p(t, e), %\sum_{e = 0}^{\hat{e}}
\label{q_eps_arbitrary}
\end{align}
где
\begin{align}
&N_p(t, e) = C_{L - |\D|}^{l - t} \: H_{L - |\D|}^{l - t, m}(s_{p}(e)), \label{n_p_arbitrary_chain} \\
&s_{p}(e) = \frac{l}{L}(n(a_p, \X) - \eps (L - l)) - e, \nonumber
\end{align}
множество~$\D,$ параметры $m_p$ и $m$~определяются по~\eqref{d_definition} и~\eqref{errors_count_definition}, и множество $\Psi_p$ определяется как
\begin{align}
\label{train_errors_upper_bounds}
&\Psi_p = \bigl\{(t, e) \, |\, 0 \leqslant t \leqslant \min\{l, |\D|\},  \; 0 \leqslant e \leqslant \min\{t, m_p\} \bigr\}.
\end{align}

\label{theo:arbitrary_chain}
\end{theorem}
\begin{proof}
Распишем вероятность переобучения как
\begin{align*}
Q_{\eps} = \sum_{p = 0}^{P} \Prob \bigl[\mu X = a_p\bigr] \bigl[\delta(a_p, X) \geqslant \eps \bigr].
\end{align*}

Рассмотрим множество разбиений~${(X, \valid)}$ с~фиксированными значениями~$t$ и~$e$:
\begin{align}
\label{t_e_definition}
&t = |X \cap \D|, \quad e = n(a_p, X \cap \D).
\end{align}
Очевидно, что точки $(t, e)$ принадлежат множеству $\Psi_p$, определяемому по~\eqref{train_errors_upper_bounds}.

Нетрудно проверить, что для~таких разбиений выполнение условия $\delta(a_p, X) \geqslant \eps$ не зависит от~выбора разбиения множества~$\D$, тогда как
выполнение условия $\mu X = a_p,$ в~силу того, что классификаторы неразличимы на~множестве~$\N,$ от~выбора разбиения множества~$\N$~не зависит.
Тогда для~каждой тройки параметров $p,\, t,\, e$ число разбиений множества~$\X$, таких, что одновременно выполнены условия ${\mu X = a_p}$ и ${\delta(\mu X, X) \geqslant \eps},$~есть произведение величин~$N_p(t, e)$ и~$D_p(t, e)$.

Докажем формулу~\eqref{n_p_arbitrary_chain}.
Пусть~${n(a_p, X \cap \N) = s},$~тогда~${n(a_p, X) = e + s}$.
Условие $\delta(a_p, X) \geqslant \eps$ эквивалентно условию $n(a_p, X) \leqslant \frac{l}{L}(n(a_p, \X) - \eps (L - l)),$ значит, $s$~ограничено сверху величиной~$s_{p}(e).$

Легко проверить, что число разбиений множества~$\N$~при~данных~$t$~и~$s$ равно $C_m^s C_{L - |\D| - m}^{l - t - s},$ откуда следует
\begin{align*}
N_p(t, e) = \sum_{s = 0}^{s_{p}(e)} C_{m}^{s} C_{L - |\D| - m}^{l - t - s} = C_{L - |\D|}^{l - t} \frac{1}{C_{L - |\D|}^{l - t}} \sum_{s = 0}^{s_{p}(e)} C_{m}^{s} C_{L - |\D| - m}^{l - t - s} =  C_{L - |\D|}^{l - t} \: H_{L - |\D|}^{l - t, m}(s_{p}(e)).
\end{align*}
%Заметим, что, разделив и~домножив каждое слагаемое на~$C_{L - P}^{l - t},$~получим в~точности~\eqref{n_p_arbitrary_chain}.
\end{proof}

Имеет место аналогичная теорема для вычисления полного скользящего контроля:
\begin{theorem}
\label{theo:ccv_arbitrary}
Для~метода обучения $\mu$ ПМЭР, множества~$\X$~мощности~$L,$~объема обучающей выборки~$l,$ произвольного семейства классификаторов~$\A = \{a_0, \ldots, a_P\}$ выражение для полного скользящего контроля имеет вид
\begin{align}
CCV = \frac{1}{(L - l) C_L^l} \sum_{p = 0}^{P} \sum_{(t, e) \in \Psi_p} D_p(t, e) F_p(t, e),  %^{\hat{t}} \sum_{e = 0}^{\hat{e}}
\label{ccv_arbitrary}
\end{align}
где
\begin{align}
&F_p(t, e) = \sum_{s = 0}^{\min \{ l - t, m\}} C_m^s C_{L - |\D| - m}^{l - t - s}\bigl(n(a_p, \X) - s - e \bigr),\label{neutral_error_rate_exp}
\end{align}
множества~$\D$ и $\Psi_p$ определяются по~\eqref{d_definition} и \eqref{train_errors_upper_bounds}, параметры $m_p$ и $m$~определяются по~\eqref{errors_count_definition}.
\end{theorem}

\begin{proof}
Распишем формулу полного скользящего контроля и переставим в~ней знаки суммирования:
\begin{align*}
CCV
= \frac{1}{C_L^l}  \sum_{X \in [X]^l} \sum_{p = 0}^{P} [\mu X = a_p] \: \nu(a_p, \valid)
= \frac{1}{C_L^l}  \sum_{p = 0}^{P} \sum_{X \in [X]^l} [\mu X = a_p] \: \nu(a_p, \valid).
\end{align*}

Выполнение условия ${\mu X = a_p}$ не зависит от~выбора разбиения множества~$\N$.

Очевидно, что
\[
n(a_p, \valid) = n(a_p, \X) - n(a_p, X) = n(a_p, \X) - n(a_p, X \cap \D) - n(a_p, X \cap \N).
\]

Определим параметры~$t$~и~$e$ по~формулам~(\ref{t_e_definition}).
Обозначим через~$s = {n(a_p, X \cap \N)}$.
Имеются ограничения~$s + t \leqslant l$ и $s \leqslant m$, откуда следует верхняя оценка параметра~$s$~в~\eqref{neutral_error_rate_exp}.

Легко проверить, что число разбиений множества~$\N$~при~данных~$t$~и~$s$ равно $C_m^s C_{L - |\D| - m}^{l - t - s}$, откуда следует утверждение теоремы.
\end{proof}

Таким образом, задача сводится к~вычислению для~каждого~$p$ значений~$D_p(t, e)$ на всем множестве $\Psi_p$.
Для случая прямой последовательности далее будет описан рекуррентный алгоритм вычисления $D_p(t, e)$.

\section{Вычисление количества разбиений множества ребер прямой последовательности}
Пусть семейство $\A = \{a_0, \ldots, a_P\}$ является прямой последовательностью.
Назовем \emph{ребрами} объекты множества $\D$.
%Поставим ему в~соответствие помеченный мультиграф $G_{\A} = \langle V, E \rangle$, множество вершин~$V$~которого совпадает с~$\A,$~а множество ребер есть
%\[
%E = \{e = (a, a') \cond \exists x \in \D \colon I(a, x) \neq I(a', x)\}.
%\]
%Для прямой цепи $\A$ мультиграф $G_{\A}$ является цепью.
%
%Каждому ребру мультиграфа взаимно однозначно соответствует объект~${x \in \D},$ поэтому
%далее ребра мультиграфа будем отождествлять с~соответствующими объектами множества~$\D$.
%Мощность множества $\D$ равна $P$.

\subsection{Сведение к задачам на левой и правой последовательностях.}
Зафиксируем~$p$ и точку $(t, e)$ множества $\Psi_p$.
Рассмотрим классификатор~$a_p$.
Относительно него прямая последовательность разбивается на~две: правую и левую.
В~правой последовательности находятся классификаторы $a_p, a_{p + 1}, \dots, a_P$, в~левой~-- $a_0, a_1, \dots, a_p$.

Сведем задачу вычисления~$D_p(t, e)$ к~нахождению числа разбиений множества ребер левой и правой последовательностей с~некоторыми ограничениями.

\begin{definition}
\emph{Запасом ошибок} классификатора~$a$ относительно~$a_p$~на~выборке~$X$~назовем величину~${\Delta_p(a, X) = n(a, X) - n(a_p, X).}$
\end{definition}

\begin{definition}
Будем говорить, что на выборке $X$ классификатор $a_p$ \textit{лучше, чем} $a$, и записывать $a_p \succ_X a$, если из \textit{двух} классификаторов $a$ и $a_p$ метод обучения выбирает~$a_p$.
\end{definition}

Тогда требование к~разбиению, чтобы метод обучения из всех классификаторов семейства выбирал $a_p$, переписывается в следующей эквивалентной форме:
\begin{align}
\label{necessary_and_sufficient_conditions_succ}
\mu X = a_p  \qquad \Leftrightarrow \qquad \forall a \neq a_p \quad a_p \succ_X a
\end{align}

Будем считать, что из~классификаторов, минимизирующих число ошибок на~обучающей выборке и допускающих равное число ошибок на~множестве~$\X$, выбирается классификатор с~наибольшим номером. Легко видеть, что

\begin{lemma}
\label{lem:necessary_and_sufficient_conditions_delta}
Классификатор $a_p$ лучше, чем классификатор $a$, на выборке $X$, если и только если выполнено одно из следующих условий:

\begin{enumerate}[label=\arabic*)]
\item \label{delta0} либо $\Delta_p(a, X) > 0;$
\item \label{deltal} либо $\Delta_p(a, X) = 0$ и $a$ находится в~левой последовательности и $n(a, \X) \leqslant n(a_p, \X);$
\item \label{deltar} либо $\Delta_p(a, X) = 0$ и $a$ находится в~правой последовательности и $n(a, \X) < n(a_p, \X).$
\end{enumerate}

\end{lemma}


%\begin{lemma}
%Для каждого ${a_p \in \A}$  необходимым и достаточным условием выполнения события~${[\mu X = a_p]}$, является справедливость отношения $a_p \succ_X a$ для~каждого $a \neq a_p$.
%\end{lemma}
%\begin{proof}
%Зафиксируем обучающую выборку~$X$. Если для всех классификаторов выполнен пункт \ref{delta0} определения отношения $\succ_X$, то утверждение очевидно.
%
%Пусть $a_p$ минимизирует число ошибок на обучающей выборке и существует $a \neq a_p$ с нулевым запасом ошибок на~$X$.
%Если он допускает на контрольной выборке большее число ошибок, чем $a_p$, то метод обучения не выберет $a_p$ по определению ПМЭР.
%Если он находится в левой цепи и $n(a, \X) \leqslant n(a_p, \X)$, то, по нашему предположению, метод обучения выберет классификатор с большим номером, то есть $a_p$.
%Если же классификатор $a$ находится в правой цепи, то метод обучения выберет $a$.
%\end{proof}

Обозначим через~$\LL_p$~и~$\RR_p$ множества ребер левой и правой последовательностей соответственно.
Множества~$\LL_p$~и~$\RR_p$~не пересекаются, классификаторы левой последовательности неразличимы на~$\RR_p,$~классификаторы правой последовательности неразличимы на~$\LL_p.$
Выполнение леммы~\ref{lem:necessary_and_sufficient_conditions_delta} для классификаторов левой последовательности не зависит от выбора разбиения множества~$\RR_p$, аналогично, для классификаторов правой последовательности выполнение леммы не зависит от~выбора разбиения множества~$\LL_p$.
Значит, общее число разбиений множества~$\D,$~в~которых метод обучения выбирает~$a_p,$~является произведением числа разбиений множеств~$\LL_p$~и~$\RR_p,$~в~которых~$a_p$~лучше всех классификаторов левой и правой последовательностей соответственно.

Следовательно, верна следующая
\begin{theorem}
\label{theo:chain_splits_count}
Для каждого~$p$ и для всех точек $(t, e)$ множества $\Psi_p$, определенного по~\eqref{train_errors_upper_bounds}, число разбиений множества~$\D$, таких что ${t = |X \cap \D|}$, ${e = n(a_p, X \cap \D)}$ и ${\mu X = a_p}$, равно
\begin{align}\label{splits_by_t_e_left_right}
D_p(t, e) = \sum_{t' + t'' = t} \sum_{e' + e'' = e} L_p(t', e') R_p(t'', e''),
\end{align}
где
\begin{align}
\label{left_chain_splits_count}
&L_p(t', e') = &\# \{(X \cap \LL_p, \valid \cap \LL_p)\, | \, & & & \forall d = 0, \ldots, p \quad a_p \succ_X a_d, && \\
& & & & & t' = |X \cap \LL_p|,\, e' = n(a_p,\, X \cap \LL_p)\}, && \nonumber \\
\label{right_chain_splits_count}
& R_p(t'', e'') =  & \# \{(X \cap \RR_p, \valid \cap \RR_p)\, | \,  & & & \forall d = p + 1, \ldots, P \quad a_p \succ_X a_d,\,&& \\
& & & & & t'' = |X \cap \RR_p|,\, e'' = n(a_p,\, X \cap \RR_p)\} && \nonumber
\end{align}
и точки $(t', e')$ и $(t'', e'')$ являются элементами множеств $\Psi'_p$ и $\Psi''_p$ соответственно, где
\begin{align}
\label{train_errors_left_right_upper_bounds}
\left.
\begin{array}{lll}
\Psi'_p = \bigl\{(t', e') \, &|\,    0 \leqslant t' \leqslant \min \{l, |\LL_p| \}, &
		     							   0 \leqslant e' \leqslant \min \{t',\, n(a_p, \LL_p) \} \bigr\}, \\
\Psi''_p = \bigl\{(t'', e'') \, &|\, 0 \leqslant t'' \leqslant \min \{l, |\RR_p|\}, &
          								   0 \leqslant e'' \leqslant \min \{t'',\, n(a_p, \RR_p) \} \bigr\}.  \\
\end{array}
\right.
\end{align}

\end{theorem}

%Обозначим через~$\mu'_p$ метод обучения, задаваемый следующим образом:
%\[
%\forall X \subseteq \X \quad \mu'_p X =
%\left\{
%\begin{array}{ll}
%\mu X, & \text{если}\; \mu X \; \text{из левой цепи}, \\
%a_p, & \text{иначе}.
%\end{array}
%\right.
%\]
%
%Другими словами, это ПМЭР, который каждой обучающей выборке ставит в~соответствие классификатор левой цепи.
%При~этом в~случае неоднозначности, когда несколько классификаторов допускают наименьшее число ошибок на~обучающей выборке и равное число ошибок на~контрольной, метод обучения~$\mu'_p$ выбирает классификатор с~\emph{наибольшим} номером.
%
%Аналогично, через~$\mu''_p$ обозначим метод обучения, задаваемый следующим образом:
%\[
%\forall X \subset \X \quad \mu''_p X =
%\left\{
%\begin{array}{ll}
%\mu X, & \text{если}\; \mu X \; \text{из правой цепи}, \\
%a_p, & \text{иначе}.
%\end{array}
%\right.
%\]
%
%Заметим, что метод обучения~$\mu''_p$ в~случае неоднозначности также выбирает классификатор с~наибольшим номером.

%Следовательно, события~${[\mu'_p X = a_p]}$~и~${[\mu''_p X = a_p]}$ независимы и верно
%\[
%[\mu X = a_p] = [\mu'_p X = a_p][\mu''_p X = a_p].
%\]

%\begin{proof}
%
%\end{proof}



%Так мы разбили задачу на две:
%\begin{enumerate}[label=\arabic*)]
%\item найти число разбиений выборки $\LL_p,$~таких, что выполнено $\mu'_p X = a_p$;
%\item найти число разбиений выборки $\RR_p,$~таких, что выполнено $\mu''_p X = a_p$.
%\end{enumerate}

%Напомним, что у~нас имеется дополнительное ограничение на~число элементов из~$X$~в~$\D$ ($|X \cap \D| = t$), а также на~число ошибок $n(a_p,\, X \cap \D) = e.$
%
%Введем обозначения
%\[
%\left.
%\begin{array}{ll}
%t' = |X \cap \LL_p|,  & e' = n(a_p,\, X \cap \LL_p), \\
% t'' = |X \cap \RR_p|, & e'' = n(a_p,\, X \cap \RR_p).
%\end{array}
%\right.
%\]
%
%Очевидно, что $t = t' + t''$, $e = e' + e''$.
%
%Аналогично (\ref{train_errors_upper_bounds}), определим границы изменения параметров:


Назовем разбиения множеств $\LL_p$ и $\RR_p$, удовлетворяющие условиям \eqref{left_chain_splits_count} и \eqref{right_chain_splits_count} соответственно, \emph{допустимыми}.
Итак, для каждого~$p$ задача свелась к~вычислению числа допустимых разбиений~${L_p(t', e')}$ и ${R_p(t'', e'')}$ для всех точек множеств $\Psi'_p$ и $\Psi''_p$.

Рассмотрим случай, когда прямая последовательность~$\A$ является прямой цепью. Тогда левая и правая последовательности также являются цепями.
%Аналогично, для каждого~$p$~задача для~правой цепи заключается в~нахождении числа~${R_p(t'', e'')}$ допустимых разбиений выборки~$\RR_p$ для~каждой пары параметров~$t''$~и~$e''$, удовлетворяющих неравенствам~(\ref{train_errors_left_right_upper_bounds}):


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Нахождение числа допустимых разбиений левой и правой цепей.}
%Для данного~$p$ и для~каждой пары параметров~$t'$, $e'$ и $t''$, $e''$, удовлетворяющих неравенствам~(\ref{train_errors_left_right_upper_bounds}), необходимо вычислить~$L_p(t', e')$~и~$R_p(t'', e'')$.
%Перепишем в~эквивалентной форме требование к~разбиению, чтобы метод обучения выбирал классификатор $a_p$.
%
%\begin{definition}
%\emph{Запасом ошибок} классификатора~$a$ относительно~$a_p$~на~выборке~$X$~назовем величину~${\Delta_p(a, X) = n(a, X) - n(a_p, X).}$
%\end{definition}
%
%Подчеркнем, в~чем отличие задач для~левой и правой цепей.
%Мы положили, что из~классификаторов, допускающих наименьшее число ошибок на~обучающей выборке и равное число ошибок на~генеральной выборке, выбирается классификатор с~наибольшим номером.
%Значит, в~левой цепи разрешено, чтобы классификатор, допускающий столько же ошибок, что и~$a_p,$~на~обучающей выборке, допускал столько же ошибок и на~генеральной выборке (в~этом случае будет выбран~$a_p$), тогда как в~правой цепи это запрещено.
%
%Таким образом, получаем
%\begin{lemma}\label{lem:necessary_and_sufficient_conditions_delta}
%Для каждого $a_p \in \A$ необходимым и достаточным условием выбора классификатора~$a_p$~методом обучения, то есть события $[\mu X = a_p]$, является выполнение для~каждого $a \neq a_p$ одного из~следующих условий:
%\begin{enumerate}[label=\arabic*)]
%\item либо $\Delta_p(a, X) > 0;$
%\item либо $\Delta_p(a, X) = 0$ и $a$ находится в~левой цепи и $n(a, \X) \leqslant n(a_p, \X);$
%\item либо $\Delta_p(a, X) = 0$ и $a$ находится в~правой цепи и $n(a, \X) < n(a_p, \X).$
%\end{enumerate}
%\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Нахождение числа допустимых разбиений множества ребер левой цепи.}
Решаем задачу вычисления~$L_p(t', e')$ для~каждого~$p$~в каждой точке~$(t', e') \in \Psi'_p.$
Заметим, что при~${p = 0}$~решение задачи тривиально: множество $\Psi'_0$ состоит из одной точки~${(0, 0)}$~и~${L_0(0, 0) = 1}.$
Всюду далее считаем $1 \leqslant p \leqslant P$.

Полагаем, что левая последовательность начинается в~$a_p$ и заканчивается в~$a_0$.
Обозначим классификаторы последовательности через $\{b_0, \ldots, b_p\}$, где $b_d = a_{p - d}$ для каждого $d = 0, \ldots, p$.
Запас ошибок относительно $a_p$ переписывается как $\Delta_0(b_d, X) = \Delta_p(a_{p - d}, X)$ для каждого~$d$.

Левая цепь составлена из~возрастающих и убывающих монотонных цепей.
Объединим все ребра возрастающих монотонных цепей, обозначим это множество через~$\CC_p$.
Через~$\I_p$~обозначим ребра убывающих цепей, то есть~${\LL_p \setminus \CC_p}$.

Цепь прямая, следовательно, $b_0$~не ошибается на~всех объектах~$\CC_p$, то есть
\begin{align}
\label{def:corr_incorr_sets}
\left.
\begin{array}{cl}
\CC_p &= \{x \in \LL_p \colon I(b_0, x) = 0 \}, \\
\I_p  &= \{x \in \LL_p \colon I(b_0, x) = 1 \}.
\end{array}
\right.
\end{align}
Тогда верно, что~${e' = |X \cap \I_p|}$, а~${|X \cap \CC_p| = t' - e'}$.

Заметим, что, поскольку классификаторы левой цепи различимы только на~объектах множества~$\LL_p$, то для~любого классификатора~$b$~из~левой цепи верно
\[
\Delta_0(b, X) = \Delta_0(b, X \cap \LL_p), \forall X \subseteq \X.
\]
Отсюда следует, что, зафиксировав разбиение множества~$\LL_p$, мы определим запас ошибок на~всех соответствующих обучающих выборках~$X$.

%\begin{lemma}
%\label{lem:delta_values_dependence_on_edges_in_train}
%Зафиксируем разбиение выборки~$\LL_p.$
%Пусть классификаторы~$b_d$~и~$b_{d + 1}$~соединены ребром~$x$.
%Тогда
%\begin{enumerate}[label=\arabic*)]
%\item если $x \in \valid$, то $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X);$
%\item если $x \in X \cap \CC_p$, то $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X) + 1;$
%\item если $x \in X \cap \I_p$, то $\Delta_0(b_{d + 1}, X) = \Delta_0(a_{d}, X) - 1.$
%\end{enumerate}
%Другими словами, при~движении по~ребрам цепи от~классификатора $b_0$~к~$b_p$ запас ошибок классификаторов изменяется следующим образом: при переходе по~ребру из~$\valid$ запас ошибок не изменяется, при переходе по~ребру из~${X \cap \CC_p}$~увеличивается~на~$1$, при переходе по~ребру из~${X \cap \I_p}$~-- уменьшается на~$1$.
%\end{lemma}
%
%\begin{proof}
%Если $x \in \valid, $ то $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X)$, так как запас ошибок зависит только от~$X$.
%
%Пусть $x$ лежит в~$X$.
%Если~$x$~лежит в~возрастающей цепи, то есть в~$\CC_p$, то~$b_{d}$~не ошибается на~этом ребре, тогда как~$b_{d + 1}$~ошибается.
%Тогда $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X) + 1$.
%Если же~$x$~лежит в убывающей цепи, то~$b_{d}$~ошибается на~этом объекте, а~$b_{d + 1}$~-- нет.
%Значит, $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X) - 1$.
%\end{proof}

%\begin{Corollary}
%\label{coroll:delta_limits}
%Для~каждого ${d = 0, \dots, p}$ выполнено $|\Delta_0(b_{d}, X)| \leqslant |\LL_p|.$
%\end{Corollary}
%\begin{proof}
%Начальное значение запаса ошибок, то есть при~$d = 0$, равно нулю.
%Согласно лемме,  откуда следует утверждение следствия.
%\end{proof}


%\begin{Corollary}
%\label{coroll:match_split_to_path_2D}
%Каждое разбиение выборки~$\LL_p$~однозначно задает на~двумерной сетке~${\{0, \dots, |\LL_p|\} \times \{-|\LL_p|, \dots, |\LL_p|\}}$ траекторию, выходящую из~точки~$(0, 0)$~и образованную переходами трех видов:
%\begin{enumerate}[label=\arabic*)]
%\item из~точки $(d, \Delta)$ в~точку~$(d + 1, \Delta)$~-- <<вправо>>;
%\item из~точки $(d, \Delta)$ в~точку~$(d + 1, \Delta + 1)$~-- <<вправо-вверх>>;
%\item из~точки $(d, \Delta)$ в~точку~$(d + 1, \Delta - 1)$~-- <<вправо-вниз>>.
%\end{enumerate}
%Траектория, соответствующая разбиению~${(X \cap \LL_p, \valid \cap \LL_p)}$, проходит через точки~$(d, \Delta)$, где~${\Delta = \Delta_0(b_{d}, X)}$ для~каждого~${d = 0, \dots, p.}$
%\end{Corollary}
%
%\begin{proof}
%Зафиксируем разбиение~$(X \cap \LL_p, \valid \cap \LL_p)$~выборки~$\LL_p$.
%Будем идти по~ребрам цепи от~классификатора~$b_0$~к~$b_p$ и каждому переходу по~ребру ставить в~соответствие переход на~двумерной сетке.
%Так мы опишем траекторию, соответствующую разбиению ребер цепи.
%
%Выходим из~точки $(0, 0)$.
%Пусть классификаторы~$b_d$~и~$b_{d + 1}$~соединены ребром~$x$, тогда если~${x \in \valid}$, то вдоль траектории выполняется переход вида
%<<вправо>>; если~${x \in X \cap \CC_p}$, то выполняется переход вида <<вправо-вверх>>, и <<вправо-вниз>>, если~${x \in X \cap \I_p}$.
%Данные правила однозначно определяют траекторию, которая, согласно лемме \ref{lem:delta_values_dependence_on_edges_in_train}, действительно проходит через~точки~${(d, \Delta)},$ где~${\Delta = \Delta_0(b_{d}, X)}$, значит, по~следствию~\ref{coroll:delta_limits} и тождеству ${p = |\LL_p|}$, целиком лежит на~двумерной сетке~${\{0, \dots, |\LL_p|\} \times \{-|\LL_p|, \dots, |\LL_p|\}}$.
%\end{proof}


%В предыдущем примере было показано, что можно отсеять часть разбиений, запретив траекториям, им соответствующим, проходить через точки, не~удовлетворяющие лемме~\ref{lem:necessary_and_sufficient_conditions_delta}.

%Кроме того, необходимо следить за~выполнением ограничений, накладываемых параметрами~$t'$ и~$e'$~задачи.
%Для~этого



\begin{definition}
Определим на~трехмерной сетке $\Omega_p = \{0, \dots, |\LL_p|\} \times \{-|\LL_p|, \dots, |\LL_p|\} \times \{0, \dots, |\LL_p|\}$ множество~$\TT_p$~траекторий, выходящих из~точки~${(0, 0, 0)}$ и образованных переходами~трех видов:
\begin{enumerate}[label=\arabic*)]
	\item из~точки $(d, \Delta, i)$ в точку~$(d + 1, \Delta, i)$~-- <<вправо>>;
	\item из~точки $(d, \Delta, i)$ в~точку~$(d + 1, \Delta + 1, i)$~-- <<вправо-вверх>>;
	\item из~точки $(d, \Delta, i)$ в~точку~$(d + 1, \Delta - 1, i + 1)$~-- <<вправо-вниз>>;
\end{enumerate}
причем для~каждого~$d$~переход из~точки $(d, \Delta, i)$ удовлетворяет условию: пусть классификаторы~$b_{d}$~и~$b_{d + 1}$~соединены ребром~$x$, тогда
\begin{enumerate}[label=\arabic*)]
	\item если $x \in \CC_p$, то это переход вида <<вправо>> или <<вправо-вверх>>;
	\item если $x \in \I_p$, то это переход вида <<вправо>> или <<вправо-вниз>>.
\end{enumerate}
\end{definition}

%Обобщением следствия~\ref{coroll:match_split_to_path_2D} на~трехмерный случай является
\begin{theorem}
\label{theo:match_split_to_path_3D}
Между разбиениями множества~$\LL_p$ и траекториями из~множества~$\TT_p$ имеется взаимно-однозначное соответствие.

Траектория, соответствующая разбиению~${(X \cap \LL_p, \valid \cap \LL_p)}$, проходит через точки ${(d, \Delta, i)}$, где для~каждого~${d = 0, \dots, p}$ координата~${\Delta = \Delta_0(b_{d}, X)}$, а координата~$i$~равна числу ребер из~$X \cap \I_p$ между~$b_0$ и~$b_{d}.$
\end{theorem}

\begin{proof}
Пусть классификаторы~$b_{d - 1}$~и~$b_{d}$~соединены ребром~$x$.

Если $x \in \valid, $ то $\Delta_0(b_{d}, X) = \Delta_0(b_{d - 1}, X)$, так как запас ошибок зависит только от~$X$.

Пусть $x$ лежит в~$X$.
Если~$x$~лежит в~возрастающей цепи, то~$b_{d - 1}$~не ошибается на~этом ребре, тогда как~$b_{d}$~ошибается.
Тогда $\Delta_0(b_{d}, X) = \Delta_0(b_{d - 1}, X) + 1$.
Если же~$x$~лежит в~$\I_p$, то~$b_{d - 1}$~ошибается на~этом объекте, а~$b_{d}$~-- нет.
Значит, $\Delta_0(b_{d}, X) = \Delta_0(b_{d - 1}, X) - 1$.

Поставим в соответствие разбиению множества~$\LL_p$ траекторию по~следующему правилу.
Пусть траектория проходит через~точку~${(d, \Delta, i)}.$
При~$d = 0$~полагаем, что это точка~${(0, 0, 0).}$
Из~этой точки вдоль траектории выполняется переход вида <<вправо>>, если $x \in \valid$; <<вправо-вверх>>, если $x \in X \cap \CC_p$; <<вправо-вниз>>, если $x \in X \cap \I_p$.

Тогда для~каждого~$d$ координаты ${\Delta}$ и $i$ имеют смысл, указанный в условии теоремы,
и при~описанных переходах изменяются не~более, чем на~$1$.
Значит, траектория действительно целиком лежит на~сетке~$\Omega_p$ и, следовательно, во~множестве~$\TT_p$ и однозначно определена.

По~тем же правилам каждой траектории из~$\TT_p$ можно однозначно поставить в~соответствие разбиение множества~$\LL_p$.
Значит, отображение из~множества разбиений во~множество траекторий~$\TT_p$ сюръективно и инъективно, то есть оно биективно.
\end{proof}

\begin{example}
На~рисунке~\ref{fig:splits_and_paths_match}~на~нижнем графике изображена цепь, где выделены ребра, попавшие в~обучающую выборку.
Такому разбиению ребер цепи соответствует траектория, проекция которой на плоскость $(d, \Delta)$ изображена на~верхнем графике.
В~данном примере траектория проходит через точки, у~которых координата~$\Delta$~отрицательна.
Значит, в~цепи имеются классификаторы с~отрицательным запасом ошибок.
Следовательно, по~лемме~\ref{lem:necessary_and_sufficient_conditions_delta} и условиям~\eqref{necessary_and_sufficient_conditions_succ}, при~таком разбиении классификатор~$b_0$~не~будет выбран методом обучения.
Исключив из~рассмотрения траектории, не удовлетворяющие лемме \ref{lem:necessary_and_sufficient_conditions_delta}, мы отбросим и разбиения, не являющиеся допустимыми.
\end{example}

\begin{figure}[t]
\begin{center}
       \newlength{\mylength}
\setlength{\fboxsep}{6pt}
\setlength{\mylength}{\linewidth}
\addtolength{\mylength}{-\fboxsep}
\addtolength{\mylength}{-\fboxrule}

        \fbox{
        \parbox{\mylength}{
        \setlength{\belowdisplayskip}{0cm}
        \begin{xy}<0.7ex,0ex>:
            \POS(2,64)\ar@{.}(38,64)*{}
            \POS(2,66)\ar@{.}(38,66)*{}
            \POS(2,68)\ar@{.}(38,68)*{}
            \POS(2,70)\ar@{.}(38,70)*{}
            \POS(2,72)\ar@{.}(38,72)*{}
            \POS(2, 62)\ar@{.}(2, 90)*{}
            \POS(8, 62)\ar@{.}(8, 90)*{}
            \POS(14, 62)\ar@{.}(14, 90)*{}
            \POS(20, 62)\ar@{.}(20, 90)*{}
            \POS(26, 62)\ar@{.}(26, 90)*{}
            \POS(32, 62)\ar@{.}(32, 90)*{}
			\POS(38, 62)\ar@{.}(38, 90)*{}
            %
            \POS(0.5,68)*@{}, +/d0.5em/*++{b_0}
            \POS(40,64)*@{}, +/u0.5em/*++{b_p}
            \POS(2,68)*@{*}="a0"
            \POS(8,66)*@{*}="a1"\ar@{=}"a0"
            \POS(14,68)*@{*}="a2"\ar@{-}"a1"
            \POS(20,70)*@{*}="a3"\ar@{=}"a2"
            \POS(26,72)*@{*}="a4"\ar@{=}"a3"
            \POS(32,70)*@{*}="a5"\ar@{-}"a4"
            \POS(38,68)*@{*}="a6"\ar@{-}"a5"
            %
            \POS(0,62)\ar@{->}(44,62)*{}
            \POS(43,62),+/u0.75em/*{d}
            \POS(2,62)*@{|}="t0"
            \POS(8,62)*@{|}="t1"
            \POS(14,62)*@{|}="t2"
            \POS(20,62)*@{|}="t3"
            \POS(26,62)*@{|}="t4"
            \POS(32,62)*@{|}="t5"
            \POS(38,62)*@{|}="t6"
            %
            \POS(-5,70)*{\scriptstyle n(b_d,\X)}
            %
            \POS(0,84)\ar@{->}(44,84)*{}
            \POS(43,84),+/u0.75em/*{d}
            \POS(8,84)*@{|}
            \POS(14,84)*@{|}
            \POS(26,84)*@{|}
            \POS(32,84)*@{|}
            \POS(2,80)\ar@{.}(38,80)*{}
            \POS(2,82)\ar@{.}(38,82)*{}
            \POS(2,84)\ar@{.}(38,84)*{}
            \POS(2,86)\ar@{.}(38,86)*{}
            \POS(2,88)\ar@{.}(38,88)*{}
            \POS(2,90)\ar@{.}(38,90)*{}
            %
            \POS(2,84)*@{*}="a0"
            \POS(8,82)*@{*}="a1"\ar@{-}"a0"
            \POS(14,82)*@{*}="a2"\ar@{-}"a1"
            \POS(20,84)*@{*}="a3"\ar@{-}"a2"
            \POS(26,86)*@{*}="a4"\ar@{-}"a3"
            \POS(32,86)*@{*}="a5"\ar@{-}"a4"
            \POS(38,86)*@{*}="a6"\ar@{-}"a5"
            \POS(-5,86)*{\scriptstyle \Delta_0(b_d,X)}
        \end{xy}
        }
        }

\end{center}
\caption{Соответствие разбиения цепи~(нижний график) проекции траектории~(верхний график). Двойными линиями выделены ребра цепи, попавшие в~обучающую выборку.}
\label{fig:splits_and_paths_match}
\end{figure}

Определим множество
\begin{align}
\Omega'_p = \{(d, \Delta, i) \in \Omega_p & \cond  0 \leqslant i \leqslant d \,\, \text{и} \,\, |\Delta| \leqslant d \,\, \text{и} \label{eq:left_paths_domain} \\
&\bigl( \text{либо} \,\, \Delta > 0, \,\, \text{либо} \,\, \left(\Delta = 0 \,\, \text{и} \,\, n(b_{d}, \X) \leqslant n(b_0, \X)\right) \bigr) \}. \nonumber
\end{align}

\begin{lemma}
\label{lem:left_chain_coordinates_constraints}
Пусть дано допустимое разбиение множества~$\LL_p.$
Тогда всякая точка~${(d, \Delta, i)}$ траектории из~$\TT_p$, соответствующей данному разбиению, принадлежит множеству~${\Omega'_p \subseteq \Omega_p}.$
%\begin{enumerate}[label=\arabic*)]
%\item $0 \leqslant i \leqslant d;$
%\item $\Delta \leqslant d;$
%\item Либо $\Delta > 0$, либо $\Delta = 0$ и $n(b_{d}, \X) \leqslant n(b_0, \X).$
%\end{enumerate}
\end{lemma}
\begin{proof}
Выполнение первые двух условий из определения~\eqref{eq:left_paths_domain} является следствием теоремы~\ref{theo:match_split_to_path_3D}.
Третье условие есть повторение условий леммы~\ref{lem:necessary_and_sufficient_conditions_delta}.
\end{proof}

Пусть $T_p(d,\, \Delta,\, i)$ есть число траекторий из~$\TT_p$, соединяющих точку~${(0,\, 0,\, 0)}$ с~${(d,\, \Delta,\, i)}$ и проходящих только через~точки множества~$\Omega'_p$.
Из~правил построения траектории по разбиению множества~$\LL_p$ следует
\begin{lemma}
\label{lem:left_chain_paths_count}
В каждой точке~${(d,\, \Delta,\, i)}$~на~трехмерной сетке~$\Omega_p$ величина~${T_p(d,\, \Delta,\, i)}$ вычисляется рекуррентно.
\begin{enumerate}[label=\arabic*)]
\item Начальное условие $T_p(0,\, 0,\, 0) = 1.$
\item Если $(d,\, \Delta,\, i) \notin \Omega'_p$, то $T_p(d,\, \Delta,\, i) = 0.$
\item Пусть $b_{d - 1}$ и $b_{d}$ соединены ребром $x$. Тогда
    \begin{align}
    T_p(d,\, \Delta,\, i) =
	\left\{
	\begin{array}{lll}
	&T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta - 1,\, i), & \text{если} \,\, x \in \CC_p, \\
	&T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta + 1,\, i - 1), & \text{если} \,\, x \in \I_p,
	\end{array}
	\right.
	\label{eq:paths_count}
	\end{align}
    где множества $\CC_p$ и $\I_p$ определяются по~\eqref{def:corr_incorr_sets}.
%	\begin{itemize}
%	\item Если $x \in \CC_p$, то
%		\[
%		T_p(d,\, \Delta,\, i) = T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta - 1,\, i).
%		\]
%	\item  Если $x \in \I_p$, то
%		\[
%		T_p(d,\, \Delta,\, i) = T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta + 1,\, i - 1).
%		\]
%	\end{itemize}
\end{enumerate}
\end{lemma}

\begin{theorem}
\label{theo:left_chain_splits_count}
Пусть даны метод обучения~$\mu$ ПМЭР, множество~$\X$~мощности~$L,$~объем обучающей выборки~$l$ и прямая цепь~$\A = \{a_0, \dots, a_P\}.$
Тогда для~каждого $p = 1, \dots, P$ в каждой точке $(t', e')$ множества $\Psi'_p$, определенного в~\eqref{train_errors_left_right_upper_bounds}, число~${L_p(t', e')}$ допустимых разбиений множества~$\LL_p$, определяемое по~\eqref{left_chain_splits_count}, равно
\begin{align}
%\label{eq:left_splits_eq_paths}
L_p(t',\, e') = T_p(|\LL_p|,\, t' - 2e',\, e')
\end{align}
и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, где $b_d = a_{p - d}$ для~каждого~$d,$ при краевых условиях~${L_0(0, 0) = 1}.$
\end{theorem}
\begin{proof}
Из теоремы~\ref{theo:match_split_to_path_3D} следует, что
\[
\Delta_p(a_0, X) = {|X \cap \CC_p| -} {|X \cap \I_p|} = {t' - 2e'}.
\]

Между разбиениями множества ребер левой цепи и траекториями из~$\TT_p$ имеется биекция.
Таким образом, число траекторий, проходящих через точку ${(p,\, t' - 2e',\, e')}$, равно числу разбиений, удовлетворяющих условию $t' = |X \cap \LL_p|$, ${e' = n(a_p, X \cap \LL_p)}$.
Оставив среди них те, которые проходят только через точки множества~$\Omega'_p(t', e')$, мы оставим траектории, соответствующие допустимым разбиениям.
Их число равно~$T_p(|\LL_p|,\, t' - 2e',\, e').$
\end{proof}

\begin{remark}
Заметим, что ограничения $i \leqslant e'$ и ${\Delta \leqslant t' - e'}$, являющиеся следствием теоремы~\ref{theo:match_split_to_path_3D}, выполняются автоматически для тех траекторий, которые соединяют точки~$(0, 0, 0)$ и $(p, t' - 2e', e')$.
Действительно, поскольку величины $i$ и $\Delta + i$ не возрастают, значит, не превосходят значений в конечной точке, то есть
$i \leqslant e'$ и
\[
\Delta + i \leqslant t' - 2e' + e' = t' - e'.
\]
Координата $i \geqslant 0$, значит, $\Delta \leqslant \Delta + i \leqslant t' - e'.$ В силу этого замечания, в определение множества $\Omega'_p$  данные ограничения не входят.

%\item $0 \leqslant i \leqslant \min \{ d, e' \};$
%\item $\Delta \leqslant \min \{ d, t' - e' \};$
%\item Либо $\Delta > 0$, либо $\Delta = 0$ и $n(b_{d}, \X) \leqslant n(b_0, \X).$
%
\end{remark}

Таким образом, мы научились решать задачу для левой цепи.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Нахождение допустимых разбиений  множества ребер правой цепи.}
Решаем задачу вычисления~$R_p(t'', e'')$ для~каждого~$p$~в каждой точке $(t'', e'') \in \Psi''_p$.
Решение практически повторяет решение задачи для~левой цепи после замены~$\LL_p$ на~$\RR_p$ и~точки~$(t', e')$ на~$(t'', e'').$
Также имеются краевые условия: при~$p = P$ множество $\Psi''_P = \{(0, 0)\}$ и ${R_P(0, 0) = 1}.$
Далее полагаем, что~${0 \leqslant p \leqslant P - 1}$.

Обозначим классификаторы цепи через $b_d = a_{p + d}$  для каждого ${d = 0, \ldots, P - p}$.
Из леммы~\ref{lem:necessary_and_sufficient_conditions_delta} следует, что для справедливости леммы~\ref{lem:left_chain_paths_count}
для правой цепи множество~$\Omega'_p$ необходимо заменить на~множество~$\Omega''_p$, определяемое следующим образом:
\begin{align}
\Omega''_p = \{(d, \Delta, i) \in \Omega_p &\cond 0 \leqslant i \leqslant d \,\, \text{и} \,\, |\Delta| \leqslant d \,\, \text{и}
\label{eq:right_paths_domain} \\
&\bigl( \text{либо} \,\, \Delta > 0, \,\, \text{либо} \,\, \left(\Delta = 0 \,\, \text{и} \,\, n(b_{d}, \X) < n(b_0, \X)\right) \bigr) \}. \nonumber
\end{align}

По аналогии с теоремой~\ref{theo:left_chain_splits_count}, для правой цепи верна следующая теорема:
\begin{theorem}
\label{theo:right_chain_splits_count}
Пусть даны метод обучения~$\mu$ ПМЭР, множество~$\X$~мощности~$L,$~объем обучающей выборки~$l$ и произвольная прямая цепь~$\A = \{a_0, \dots, a_P\}.$
Тогда для~каждого $p = 0, \dots, P - 1$ в каждой точке~$(t'', e'')$ множества $\Psi''_p$, определенного в~\eqref{train_errors_left_right_upper_bounds}, число~${R_p(t'', e'')}$ допустимых разбиений множества~$\RR_p$, определяемое по~(\ref{right_chain_splits_count}), равно
\begin{align}
R_p(t'', e'') = T_p(|\RR_p|, t'' - 2 e'', e'')
%\label{eq:right_splits_eq_paths}
\end{align}
и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, с~заменой множества~$\Omega'_p$ на~$\Omega''_p$ и $b_d$ на $a_{p + d}$ для~каждого~$d.$ Краевые условия $R_P(0, 0) = 1.$
\end{theorem}

\begin{remark}
По~лемме~\ref{lem:necessary_and_sufficient_conditions_delta}, для всех~${d = 0, \dots, P}$ запас ошибок классификатора~$a_d$~цепи должен быть неотрицателен для допустимых разбиений множества ребер левой и правой цепи.
В~частности, ${\Delta_p(a_0, X) = t' - 2 e' \geqslant 0}$ и ${\Delta_p(a_P, X) = t'' - 2 e'' \geqslant 0}.$
Значит, границы изменения вторых координат точек множеств $\Psi_p$, $\Psi'_p$, $\Psi''_p$ имеют вид
\noindent
\begin{align*}
0 \leqslant e \leqslant    \min \bigl\{\tfrac12t ,\, m_p\bigr\}, \qquad
0 \leqslant e' \leqslant   \min \bigl\{\tfrac12{t'}, n(a_p, \LL_p)\bigr\}, \qquad
0 \leqslant e'' \leqslant  \min \bigl\{\tfrac12{t''}, n(a_p, \RR_p)\bigr\}.
\end{align*}
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Нахождение числа допустимых разбиений множества ребер прямой последовательности.}

Рассмотрим общий случай прямой последовательности~$\A = \{a_0, \dots, a_P\}$.
Сведем задачу вычисления количества допустимых разбиений левой и правой последовательностей к аналогичным задачам для прямых цепей.

Для этого построим прямую цепь $\A_c$, такую, что $\A \subseteq \A_c$ и первый и последний классификаторы семейств совпадают, следующим образом: для каждого $i,$ такого, что $|G_i| > 1,$ объединим цепь $\A$ с цепью $\GG_i$
\[
\{a_0, \dots, a_{i - 1}\} \cup \GG_i \cup \{a_{i + 2}, \dots, a_P\},
\]
где прямая цепь $\GG_i$ такова, что первым классификатором цепи является $a_i$, последним~-- $a_{i + 1}$.
Для определенности будем считать, что~$\GG_i$ строится как прямая цепь, составленная из двух монотонных: убывающей цепи длины $n_{down}$ и возрастающей длины $n_{up}$, где
\begin{align*}
n_{down} = \# \{x \in G_i \cond I(a_i, x) = 1\}, \\
n_{up} = \# \{x \in G_i \cond I(a_i, x) = 0\}.
\end{align*}
Назовем построенную цепь $\A_c$ \emph{интерполяцией} последовательности~$\A$.
Ее длина равна~$|\D|$.

Для каждого ${a_p \in \A}$ рассмотрим левую последовательность ${\{a_p, \dots, a_0\} \subseteq \A}$ и левую цепь ${\{a_p, \dots, a_0\} \subseteq \A_c}$.
По построению множества ребер данных семейств совпадают, вследствие чего множества допустимых разбиений левой цепи и левой последовательности, определенные по \eqref{left_chain_splits_count}, также совпадают. Вычислим их количество по теоремам \ref{theo:left_chain_splits_count} и \ref{theo:right_chain_splits_count} с единственным отличием.

Согласно \eqref{necessary_and_sufficient_conditions_succ}, условие $a_p \succ_X a$ и лемма \ref{lem:necessary_and_sufficient_conditions_delta} должны быть выполнены только для $a \in \A$.
Данное ограничение определяет строение множеств $\Omega'_p$ и $\Omega''_p,$ задаваемых в \eqref{eq:left_paths_domain} и \eqref{eq:right_paths_domain}. Переопределим их для случая интерполяции последовательности $\A:$
\begin{align}
&
\left.
\begin{array}{rll}
\Omega'_p = \{(d, \Delta, i) \in \Omega_p \cond  & \Bigl( b_d \in \A \quad \textit{и} &0 \leqslant i \leqslant d \quad \text{и} \quad |\Delta| \leqslant d \,\,  \text{и}\\
& &\bigl( \Delta > 0 \,\,  \text{или} \,\, \left(\Delta = 0 \,\, \text{и} \,\, n(b_{d}, \X) \leqslant n(b_0, \X)\right) \bigr) \Bigr)\\
\text{или} & \,\,\, b_d \in \A_c \setminus \A \}. &
\end{array}
\right.
\label{def:left_paths_domain_interpolation} \\
&
\left.
\begin{array}{rll}
\Omega''_p = \{(d, \Delta, i) \in \Omega_p \cond & \Bigl( b_d \in \A \quad \textit{и} &0 \leqslant i \leqslant d \,\, \text{и} \,\, |\Delta| \leqslant d \,\, \text{и} \\
& &\bigl( \Delta > 0 \,\,  \text{или} \,\, \left(\Delta = 0 \,\, \text{и} \,\, n(b_{d}, \X) < n(b_0, \X)\right) \bigr) \Bigr) \\
\text{или} & \,\,\, b_d \in \A_c \setminus \A \}. &
\end{array}
\right.
\label{def:right_paths_domain_interpolation}
\end{align}

Таким образом, верна теорема
\begin{theorem}
\label{theo:interpolation_splits_count}
Пусть даны метод обучения~$\mu$ ПМЭР, множество~$\X$~мощности~$L,$~объем обучающей выборки~$l$ и прямая последовательность~$\A = \{a_0, \dots, a_P\}.$
Пусть прямая цепь $\A_c = \{c_0, \dots, c_{|\D|}\}$ является интерполяцией последовательности $\A.$ Каждому классификатору $a_p \in \A$ соответствует $c_{i_p} \in \A_c$.

Тогда для~каждого $p = 1, \dots, P$ в каждой точке $(t', e')$ множества $\Psi'_p$, определенного в~\eqref{train_errors_left_right_upper_bounds}, число~${L_p(t', e')}$ допустимых разбиений множества~$\LL_p$, определяемое по~\eqref{left_chain_splits_count}, равно
\begin{align}
\label{eq:left_splits_eq_paths}
L_p(t',\, e') = T_p(|\LL_p|,\, t' - 2e',\, e')
\end{align}
и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, где $b_d = c_{i_p - d}$ для~каждого~$d$ и множество $\Omega'_p$ определено по~\eqref{def:left_paths_domain_interpolation}. Краевые условия~${L_0(0, 0) = 1}.$

Для~каждого $p = 0, \dots, P - 1$ в каждой точке~$(t'', e'')$ множества $\Psi''_p$, определенного в~\eqref{train_errors_left_right_upper_bounds}, число~${R_p(t'', e'')}$ допустимых разбиений множества~$\RR_p$, определяемое по~(\ref{right_chain_splits_count}), равно
\begin{align}
R_p(t'', e'') = T_p(|\RR_p|, t'' - 2 e'', e'')
\label{eq:right_splits_eq_paths}
\end{align}
и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, с~заменой множества~$\Omega'_p$ на~$\Omega''_p$, определенного по \eqref{def:right_paths_domain_interpolation}, и $b_d$ на $c_{i_p + d}$ для~каждого~$d.$ Краевые условия $R_P(0, 0) = 1.$
\end{theorem}

\section{Алгоритм вычисления вероятности переобучения и полного скользящего контроля}
%Подставим~(\ref{splits_by_t_e_left_right}) в~формулу вероятности переобучения~(\ref{q_eps_arbitrary}):
%\begin{align*}
%Q_{\eps}
%=
%\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t = 0}^{\hat{t}} \sum_{e = 0}^{\hat{e}} N_p(t, e) \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} L_p(t', e') R_p(t - t', e - e').
%\end{align*}
%
%После перестановки знаков суммирования и замены переменных ${t'' = t - t', \: e'' = e - e'}$ получаем основную теорему для~вычисления вероятности переобучения:
%\begin{theorem}
%Для~метода обучения ПМЭР, генеральной выборки~$\X$~объема~$L,$~объема обучающей выборки~$l,$ произвольной прямой цепи~$\A$~длины~$P$, точности~$\eps \in (0,\, 1)$, выражение для вероятности переобучения имеет вид
%\begin{align*}
%Q_{\eps}
%=
%\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') \: N_p(t' + t'', e' + e''),
%\end{align*}
%где параметры~$\hat{t}', \: \hat{t}'', \: \hat{e}', \: \hat{e}''$ вычисляются по~(\ref{train_errors_left_right_upper_bounds}), а величины~$N_p(t, e)$,~$L_p(t', e')$ и~$R_p(t'', e'')$ вычисляются по~формуле~\eqref{n_p_arbitrary_chain}, теоремам~\ref{theo:left_chain_splits_count}~и~\ref{theo:right_chain_splits_count} соответственно.
%\label{theo:arbitrary_chain_optimized}
%\end{theorem}
%
%Аналогично, после подстановки~(\ref{splits_by_t_e_left_right}) в~формулу~(\ref{ccv_arbitrary}), перестановки знаков суммирования и замены переменных получаем основную теорему для~вычисления полного скользящего контроля:
%\begin{theorem}
%\label{theo:ccv_arbitrary_optimized}
%Для~метода обучения~{ПМЭР}, генеральной выборки~$\X$~объема~$L,$~объема обучающей выборки~$l$, произвольной прямой цепи~$\A$~длины~$P$ выражение для~полного скользящего контроля имеет вид
%\begin{align*}
%CCV
%=
%\frac{1}{(L - l) C_L^l}
%\sum_{p = 0}^{P}
%\sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') F_p(t' + t'', e' + e''),
%\end{align*}
%где величина~$F_p(t, e)$ равна
%\begin{align*}
%F_p(t, e) = \sum_{s = 0}^{\hat{s}} C_m^s C_{L - P - m}^{l - t - s}\bigl(n(a_p, \X) - s - e \bigr),
%\end{align*}
%параметры~$m, \hat{s}, \hat{t}', \: \hat{t}'', \: \hat{e}', \: \hat{e}''$ вычисляются по~формулам~(\ref{errors_count_definition}),~(\ref{s_upper_bound})~и~(\ref{train_errors_left_right_upper_bounds}), и величины~$L_p(t', e')$ и~$R_p(t'', e'')$ вычисляются по теоремам~\ref{theo:left_chain_splits_count}~и~\ref{theo:right_chain_splits_count} соответственно.
%\end{theorem}

Итак, в теореме \ref{theo:interpolation_splits_count} описан алгоритм нахождения количества допустимых разбиений множеств ребер правой и левой последовательностей для~каждого~$p$.
Остается подставить найденные значения в формулы~\eqref{splits_by_t_e_left_right}, \eqref{q_eps_arbitrary} и~\eqref{ccv_arbitrary}.
Для~сокращения вычислений по теоремам~\ref{theo:arbitrary_chain} и~\ref{theo:ccv_arbitrary} для~каждого~$p$ предлагается заранее вычислить $L_p(t', e')$, $R_p(t'', e'')$, $N_p(t, e)$ и $F_p(t, e)$, после чего сложить полученные значения. Схема вычисления изложена в алгоритме~\ref{alg:q_eps_and_ccv}.

%%\[
%%\left.
%%\begin{array}{rl}
%%Q_{\eps}
%%&=
%%\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') \: N_p(t' + t'', e' + e''), \\
%%CCV
%%&=
%%\frac{1}{(L - l) C_L^l}
%%\sum_{p = 0}^{P}
%%\sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') F_p(t' + t'', e' + e'').
%%\end{array}
%%\right.
%%\]
%\begin{align*}
%Q_{\eps}
%=
%&\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') \: N_p(t' + t'', e' + e''), \\
%CCV
%=
%&\frac{1}{(L - l) C_L^l}
%\sum_{p = 0}^{P}
%\sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') F_p(t' + t'', e' + e'').
%\end{align*}

%В~алгоритме~\ref{alg:q_eps_and_ccv} представлен псевдокод вычисления вероятности переобучения и функционала полного скользящего контроля.
%Алгоритм \ref{alg:chain_splits_count} вычисляет количество допустимых разбиений левой или правой цепи.
%В зависимости от вида $Type$ цепи~-- левая или правая~-- используется либо множество $\Omega'_p()$, либо $\Omega''_p()$.
%Все вычисления опираются на~доказанные ранее леммы и теоремы, и все обозначения сохраняются.

\SetAlgorithmName{Алгоритм}{algorithm2e}{Список алгоритмов}
\SetKwInput{Input}{\bf{Вход}}
\SetKwInput{Output}{\bf{Выход}}
\SetKwBlock{Begin}{}{}
\SetKwFor{For}{для}{}{endfor}
\SetKwFor{ForEach}{для всех}{}{endfall}
\SetKw{KwTo}{$\ldots$}
\SetKw{Return}{вернуть}
\SetKwIF{If}{ElseIf}{Else}{если}{то}{иначе если}{иначе}{endif}
%\SetKwFunction{ChainSplitsCount}{ChainSplitsCount}

\begin{algorithm2e}[h]
\Input{Прямая последовательность $\A = \{a_0, \dots, a_P\}$ и ее интерполяция~-- прямая цепь $\A_c$, мощность~$L$ множества~$\X$, объем~$l$ обучающей выборки, число ошибок ${m = n(a_0, \NN)}$ на нейтральном множестве, точность $\eps$.}
\Output{Вероятность переобучения $Q_{\eps}$ и полный скользящий контроль CCV.}
\dontprintsemicolon
%$Q_{\eps}, CCV \longleftarrow 0$\;
\ForEach{$p = 0, \ldots, P$}{
    \nllabel{alg:iterate_through_p}
	\ForEach{точек $(t', e')$ множества $\Psi'_p,$ определенного по \eqref{train_errors_left_right_upper_bounds}}{
        \nllabel{alg:left_splits}
        найти $L_p(t', e')$ число допустимых разбиений левой последовательности по~формулам~\eqref{eq:left_splits_eq_paths},~\eqref{eq:paths_count}~и~\eqref{def:left_paths_domain_interpolation}\;
		\nllabel{alg:left_splits_end}
    }
    \ForEach{точек $(t'', e'')$ множества $\Psi''_p,$ определенного по \eqref{train_errors_left_right_upper_bounds}}{	
        \nllabel{alg:right_splits_begin}
        найти $R_p(t'', e'')$ число допустимых разбиений правой цепи по~формулам~\eqref{eq:right_splits_eq_paths},~\eqref{eq:paths_count}~и~\eqref{def:right_paths_domain_interpolation}\;
        \nllabel{alg:right_splits}
	}
	\ForEach{точек $(t, e)$ множества $\Psi_p$, определенного в \eqref{train_errors_upper_bounds}}{
        \nllabel{alg:first_line_neutral}
		вычислить $N_p(t, e)$ по формуле \eqref{n_p_arbitrary_chain}\;
		вычислить $F_p(t, e)$ по формуле \eqref{neutral_error_rate_exp}\;
	}
	\ForEach{точек $(t, e)$ вне множества $\Psi_p$}{
        положить $N_p(t, e)$ и $F_p(t, e)$ равными нулю\;
        \nllabel{alg:last_line_neutral}
    }
}
	$Q_\eps := \frac{1}{C_L^l}  \sum\limits_{p = 0}^{P} \sum\limits_{\substack{(t', e') \in \Psi'_p, \\ (t'', e'') \in \Psi''_p}} L_p(t', e') R_p(t'', e'')  N_p(t' + t'', e' + e'') $\;
    \nllabel{alg:qeps}
	$CCV :=  \frac{1}{(L - l) C_L^l} \sum\limits_{p = 0}^{P} \sum\limits_{\substack{(t', e') \in \Psi'_p, \\ (t'', e'') \in \Psi''_p}} L_p(t', e') R_p(t'', e'')  F_p(t' + t'', e' + e'')$
    \nllabel{alg:ccv}
\caption{Вычисление вероятности переобучения и полного скользящего контро\rlap{ля}}
\label{alg:q_eps_and_ccv}
\end{algorithm2e}
%
%\begin{algorithm2e}[t]
%\ChainSplitsCount{$p, t', e', Type$}{\\
%	\Input{Прямая цепь $\A = \{a_0, \dots, a_P\}$, параметры $p, \; t'$  и $e'$, вид~$Type$ цепи.}
%	\Output{Число допустимых разбиений левой или правой цепи.}
%	\dontprintsemicolon
%    \lIf{$Type = Left$}{
%    		$\hat{P} \longleftarrow |\LL_p|; \{b_0, \ldots, b_{\hat{P}}\} \longleftarrow \{a_p, \ldots, a_0\}$
%    	}
%	\lElse{
%		$\hat{P} \longleftarrow |\RR_p|; \{b_0, \ldots, b_{\hat{P}}\} \longleftarrow \{a_p, \ldots, a_{P}\}$
%	}
%	\ForEach{$\Delta = -\hat{P} , \ldots, \hat{P} $ {\bf и} $i = 0, \ldots, \hat{P}$}{
%		$T[0, \Delta, i] \longleftarrow 0$
%	}
%	$T[0, 0, 0] \longleftarrow 1$\;
%	\ForEach{$d = 1, \ldots, \hat{P}$}{
%		$x \longleftarrow $ ребро между $b_{d - 1}$ и $b_{d}$\;
%		\ForEach{$\Delta = -\hat{P}, \ldots, \hat{P}$ {\bf и} $i = 0, \ldots, \hat{P}$}{
%			\If {
%			\vskip-3ex
%			\begin{tabular}{b{0.03\textwidth}b{0.97\textwidth}}
%			 &\(0 \displaystyle \leqslant i \leqslant \min\{d, e'\} \; \text{\bf и} \;  -d \leqslant \Delta \leqslant \min\{{d, t' - e'}\} \, \text{\bf и}\)\\
%			 &\( \displaystyle \bigl(\Delta > 0 \; \text{\bf или} \, Type = Left \; \text{\bf и} \; \Delta = 0 \; \text{\bf и} \; n(b_{d}, \X) \leqslant n(b_0, \X)\)\\
%			 &\( \displaystyle \text{\bf или} \; Type = Right \; \text{\bf и} \; \Delta = 0 \; \text{\bf и} \; n(b_{d}, \X) < n(b_0, \X) \bigr)\)\\
%			 \end{tabular}
%			}
%			{
%				\eIf {$b_0$ не ошибается на $x$}{
%					$T[d, \Delta, i] \longleftarrow T[d - 1, \Delta, i] + T[d - 1, \Delta - 1, i]$\;
%				}
%				{
%					$T[d, \Delta, i] \longleftarrow T[d - 1, \Delta, i] + T[d - 1, \Delta + 1, i - 1]$\;
%				}						
%			}
%			\lElse{
%				$T[d, \Delta, i] \longleftarrow 0$\;
%			}	
%			\vskip-3ex
%		}	
%	}
%	\Return{$T[\hat{P},  t' -2e', e']$}.
%}
%\caption{Нахождение допустимых разбиений левой и правой цепей}
%\label{alg:chain_splits_count}
%\end{algorithm2e}

\subsection{Сложность алгоритма.}
Оценим сложность выполнения шагов~\ref{alg:left_splits}--\ref{alg:last_line_neutral} алгоритма~\ref{alg:q_eps_and_ccv}.
%При вычислении~${L_p(t',\, e')}$ по~теореме~\ref{theo:left_chain_splits_count} значение $T_p(d, \, \Delta, \, i)$ вычисляется в~каждой точке множества~$\Omega'_p$, определенного в~лемме~\ref{lem:left_chain_coordinates_constraints}.

При вычислении $L_p(t', e')$ по теореме \ref{theo:left_chain_splits_count} на шагах~\ref{alg:left_splits}--\ref{alg:left_splits_end} один раз для всех $(d, \Delta, i) \in \Omega'_p$ вычисляются $T_p(d, \Delta, i), $ затем для каждого $(t', e') \in \Psi'_p$ величина $L_p(t', e')$ полагается равной $T_p(d, t' - 2 e', e')$.
Множество~$\Omega'_p$ вложено в~куб со~стороной~$O(|\LL_p|)$,~поскольку каждая координата ограничена по модулю количеством ребер в левой последовательности.
Следовательно, сложность выполнения шагов~\ref{alg:left_splits}--\ref{alg:left_splits_end} есть~$O(|\LL_p|^3)$.
Аналогично, сложность выполнения шагов \ref{alg:right_splits_begin}--\ref{alg:right_splits} равна $O(|\RR_p|)^3)$.

Для~нахождения ${N_p(t, e)}$ и~${F_p(t, e)}$ необходимо вычислить биномиальные коэффициенты $C_m^i$ и $C_{L - P - m}^i$ при~всех возможных $i$ за~${O(L)}$.
Биномиальные коэффициенты для каждого~$p$ не пересчитываются.
При~известных значениях биномиальных коэффициентов искомые~${N_p(t, e)}$ и~${F_p(t, e)}$ вычисляются за~$O(L)$.
Множество~$\Psi_p$ вложено в квадрат со стороной~$L,$ значит, выполнение шагов \ref{alg:first_line_neutral}--\ref{alg:last_line_neutral} выполняется за~$O(L^3)$.
Следовательно, сложность выполнения шагов~\ref{alg:left_splits}--\ref{alg:last_line_neutral} равна $O(|\D|^3 + L^3) = O(L^3)$ для каждого $p.$

Множества $\Psi'_p$ и $\Psi''_p$ вложены в квадрат со стороной $P$, значит, шаги \ref{alg:qeps}--\ref{alg:ccv} выполняются за~$O(L^5)$, и сложность алгоритма~\ref{alg:q_eps_and_ccv} равна сложности итогового суммирования~$O(L^5)$.

%\section{Вычислительные эксперименты}
%Напомним обозначения.
%Дана цепь $\A = \{a_0, \dots, a_P\}$ на~множестве~$\X$~мощности~$L$~с~параметром~$m$,~равным числу ошибок классификаторов на~нейтральном множестве.
%Объем обучающей выборки полагается равным~$l.$
%Точность~$\eps$.
%
%\subsection{Произвольная прямая цепь.}
%Для~произвольной прямой цепи~$\A$~был проведен точный расчет вероятности переобучения двумя способами: по~определению и с~помощью алгоритма.
%Поскольку первый способ расчета требует временных затрат, экспоненциально зависящих от~$L$, параметры принимали небольшие значения:~${L = 30, \, l = 10, \, m = 6}$.
%Точность~$\eps = 0.05.$
%Параметр~$P$~принимал значения от~$0$~до~${L - m.}$
%При~этом результаты расчетов совпали, что подтверждает корректность алгоритма.
%
%
%\subsection{Сравнение с существующими оценками.}
%%Рассмотрим частный случай цепи, для которого существующие оценки вероятности переобучения являются завышенными.
%Рассмотрим задачу из примера 1 со сбалансированными классами, то есть с классами равной мощности.
%Варьирование порога порождает цепь.
%Если порог пробегает не все возможные значения, а, например, только те, которые находятся на границе классов, то возможно возникновение объектов, на которых все классификаторы цепи допускают ошибки.
%Количество таких объектов равно~$m$.
%
%Покажем, что для данной задачи существующие оценки вероятности переобучения и скользящего контроля являются завышенными.
%%Прямая цепь называется \emph{цепью--пилой} с~параметром~$h$,~если она состоит из~чередующихся возрастающих и убывающих монотонных цепей одинаковой длины~$h$~(пример~-- правый верхний график на~рисунке~\ref{fig:various_simple_chains}).
%
%\subsection{Оценки вероятности переобучения.}
%Верна следующая теорема, называемая оценкой Вапника-Червоненкиса \cite{vapnik}:
%\begin{theorem}
%Для~любого множества~$\X$ мощности $L$, семейства классификаторов~$\A$, метода обучения~$\mu$, длины обучающей выборки~$l$ и порога~$\eps \in (0, 1)$
%\begin{align*}
%Q_{\eps} \leqslant |\A| \max\limits_{m = 1, \dots, L} H_{L}^{l, m} \bigl( \tfrac{l}{L}(m - \eps (L - l)) \bigr).
%\end{align*}
%\end{theorem}
%
%\noindent\textbf{Оценка Соколова.}
%В~рамках комбинаторного подхода была получена оценка, учитывающая \emph{расслоение} и \emph{связность} семейства~\cite{ivahnenko11premi}.
%Под~расслоением семейства понимается распределение классификаторов по~частоте ошибок на~множестве~$\X$.
%Связность предполагает, что для~каждого классификатора в~семействе найдется множество похожих классификаторов, отличающихся от~него только на~одном объекте выборки.
%В~\cite{sokolov} эта оценка была улучшена за~счет более тонкого анализа эффекта связности.
%
%Пусть дано семейство классификаторов ${\B = \{b_1, \dots, b_T\}}$ с~известной матрицей ошибок на~множестве~$\X$.
%На~множестве классификаторов, как~векторов ошибок, существует отношение лексикографического порядка~$\leqslant$.
%Будем говорить, что классификатор~$a$~\emph{предшествует}~$b$~и~записывать~$a \prec b$,~если~$a \leqslant b$~и~расстояние Хемминга между ними равно~$1$.
%Будем называть классификатор $s$ \emph{истоком}, если нет классификаторов~$b,$~таких что~$b \prec s$.
%
%Через~$u(a)$~будем обозначать
%\[u(a) = |\{b \in \B~|~ a \prec b\}|.\]
%
%Через~$n(a)$~будем обозначать число ошибок~$a$~на~множестве~$\X$.
%
%Пусть даны два классификатора~$a_i$~и~$a_j.$ Тогда через~$A_{ij}$~и~$B_{ij}$~будем обозначать множества
%\begin{align*}
%A_{ij} &= \{x \in \X|~ I(a_i, x) = 0, ~I(a_j, x) = 1\},\\
%B_{ij} &= \{x \in \X|~ I(a_i, x) = 1, ~ I(a_j, x) = 0\}.
%\end{align*}
%
%Верна следующая теорема, называемая оценкой Соколова~\cite{sokolov}:
%\begin{theorem}
%Пусть $S$~-- множество истоков семейства~$\B$. Тогда верна следующая оценка
%\begin{align} \label{sokolov_bound}
%Q_{\eps} \leqslant \sum_{t = 1}^{T} \min\limits_{s \in S} \Biggl\{ \sum_{i = 0}^{\min\{|A_{ts}|,~ |B_{ts}|\}} \frac{C_{|B_{ts}|}^{i} C_{L - u - |B_{ts}|}^{l - u - i}}{C_L^l} H_{L - u - |B_{ts}|}^{l - u - i,~ n - |B_{ts}|}\Bigl( \tfrac{l}{L}(n - \eps (L - l)) - i) \Bigr) \Biggr\}.
%\end{align}
%\end{theorem}
%
%На~рисунке \ref{fig:sokolov_and_vc_bound} в~логарифмической шкале отложены значения оценки Вапника--Червоненкиса и оценки Соколова в~сравнении с~точной верхней оценкой вероятности переобучения цепи.
%Оценка Соколова является точной только в одном случае, когда минимальное количество ошибок совпадает с параметром $m$.
%В~этом случае цепь является унимодальной (рисунок \ref{fig:combined_from_monot_chain}).
%С~увеличением минимального количества ошибок оценка~(\ref{sokolov_bound}) начинает превосходить реальное значение вероятности переобучения.
%Оценка Вапника--Червоненкиса для~рассматриваемой цепи оказывается завышенной при~любом значении минимального количества ошибок.
%
%\begin{figure}[t]
%\centering{
%\includegraphics[width=\linewidth]{probs_240_160.eps}}
%\caption{Сравнение точных значений вероятности переобучения и оценок Соколова и Вапника--Червоненкиса в логарифмической шкале. Горизонтальной линией указано значение $Q_{\eps} = 1$.
%Условия эксперимента: $L = 240$, $l = 160$, $m = 20$, $\eps = 0.05$.
%По~горизонтали отложено минимальное количество ошибок классификаторов.}
%\label{fig:sokolov_and_vc_bound}
%\end{figure}
%
%\subsection{Оценки скользящего контроля.}
%Для частного случая, когда порог пробегает все возможные значения, в работе Гуза~\cite{guz2011} были получены верхние и нижние оценки скользящего контроля с полиномиальной вычислительной сложность $O(L^3)$.
%На рисунке~\ref{fig:guz_bound} по вертикали отложены значения нижней и верхней оценок Гуза в сравнении с точными оценками.
%Погрешность оценок Гуза составляет не более~$0.05$, из чего можно сделать вывод о высокой точности оценок.
%
%\begin{figure}[t]
%\centering{
%\includegraphics[width=\linewidth]{ccvs_240_160.eps}}
%\caption{Сравнение точных значений скользящего контроля и нижней и верхней оценок Гуза.
%Условия эксперимента: $L = 240$, $l = 160$, $m = 0$.
%По~горизонтали отложено минимальное количество ошибок классификаторов.}
%\label{fig:guz_bound}
%\end{figure}
%
%%\subsection{Влияние связности на переобученность.}
%%Целью следующего эксперимента было показать, что переобученность классификатора из цепи, выбираемого методом обучения, усредненная по всем обучающим выборкам, существенно ниже переобученности несвязного семейства.
%%
%%Генерировалась прямая цепь $\A$ заданной формы.
%%Затем методом Монте-Карло вычислялась частота ошибок метода обучения ПМЭР на~обучающей и на~контрольной выборках.
%%Далее в каждом столбце матрицы ошибок~$\A$ случайным независимым образом переставлялись элементы, вследствие чего нарушалась связность семейства, то есть близость классификаторов в~смысле расстояния Хемминга.
%%Данный способ генерации несвязного семейства ранее использовался в \cite{Voron09pria-eng}.
%%Для~полученного семейства также вычислялась частота ошибок метода обучения на~обучающей и контрольной выборках.
%%
%%Результаты эксперимента изображены на~рисунке~\ref{fig:simple_shuffle_chains}.
%%В~каждом столбце верхний график соответствует прямой цепи, нижний график~-- семейству, полученному путем нарушения связности в цепи.
%%Переобученность резко возрастает при~нарушении связности семейства.
%%На~переобученность также влияет и~расслоение семейства.
%%В~каждом ряду в~направлении слева направо число классификаторов в нижних слоях возрастает, переобученность при~этом также возрастает.
%%У~случайной цепи после нарушения связности (график в~правом нижнем углу) переобученность максимальна.
%%
%%\begin{figure}[t]
%%	\centering{
%%	\includegraphics[width=\textwidth]{chains_and_shuffled_chains_with_min_error.eps}}
%%\caption{Влияние связности семейства на переобученность. Условия эксперимента~${L = 200,\, l = 100,\, m = 50.}$}
%%\label{fig:simple_shuffle_chains}
%%\end{figure}

\section{Заключение}
Разработан алгоритм вычисления вероятности переобучения и полного скользящего контроля для~прямых последовательностей классификаторов, порождаемых одномерными пороговыми классификаторами при варьировании порога вдоль значений признака.

Следующей задачей является применение данного алгоритма для~повышения обобщающей способности методов обучения, в~частности для~модификации критериев отбора признаков в жадных алгоритмах индукции конъюнктивных логических закономерностей и других логических алгоритмах классификации.

\bigskip
\begin{thebibliography}{11}
\bibitem{voron2004cmmp}
    \textit{Vorontsov K. V.} Combinatorial substantiation of learning algorithms //
    Comp. Maths Math. Phys. — 2004. — Vol. 44, No. 11. — P. 1997–2009.

\bibitem{gilbert}
	\textit{Gilbert J., Mosteller F.}
	Recognizing the maximum of a sequence~//
	J. of Amer. Stat. Assoc.~--- 1966.~--- Vol. 61.~--- P. 35–73.

\bibitem{vapnik}
	\textit{Вапник\;В.\,Н., Червоненкис\;А.\,Я.}
	{О равномерной сходимости частот появления событий к их вероятностям}~//
	Теория вероятности и ее применения.~--- 1971.~--- Т.\,16, №\,2.~--- С.\,264--280

\bibitem{kohavi}
	{\it Kohavi\,R.}
	A study of cross-validation and bootstrap for accuracy estimation and model selection~//
	Proceedings of International Joint Conference on Artificial Intelligence.~---  1995. ~--- P. 1137-1143.
	
\bibitem{vokov09ras}
	{\it Воронцов К.\,В.}
	Точные оценки вероятности переобучения~//
	Докл.~РАН, 2009.~--- Т.~429, №~1.~--- С.~15--18

\bibitem{ivahnenko11premi}
	\textit{Vorontsov\;K.\,V., Ivahnenko\;A.\,A.}
	{Tight combinatorial generalization bounds for threshold conjunction rules}~//
	4-th Int'l Conf. on Pattern Recognition and Machine Intelligence (PReMI'11), June~27~-- July~1, 2011. Lecture Notes in Computer Science. Springer-Verlag, 2011.~--- P.\,66--73.

\bibitem{sokolov}
	\textit{Воронцов\;К.\,В., Фрей\;А.\,И., Соколов\;Е.\,А.}
	{Вычислимые комбинаторные оценки вероятности переобучения}~//
	Машинное обучение и анализ данных.~--- 2013.~--- T.\,1, №\,6.~--- С.\,734-743.

\bibitem{Tolstikhin2013}
	{\it Фрей А.\,И., Толстихин И.\,О.}
	Комбинаторные оценки вероятности переобучения на основе кластеризации и покрытий множества алгоритмов~//
	Машинное обучение и анализ данных.~--- 2013.~--- T.\,1, №\,6.~--- С.\,761-778.

\bibitem{haussler}
    {\it Haussler D., Littlestone N., Warmuth M. K.}
    Predicting {0, 1}-functions on randomly drawn points~//
    Information and Computation.~--- 1994.~--- Vol. 115, No. 2.~--- P. 248–292.

\bibitem{Voron10pria-eng}
	\textit{Vorontsov~K.~V.}
	{Exact combinatorial bounds on the probability of overfitting for empirical risk minimization}~//
	Pattern Recognition and Image Analysis.~--- 2010.~--- Vol.~20, No.~3.~--- P.~269--285.

\bibitem{botov_pria}
	\textit{Botov P. V.}
	Exact estimates of the probability of overfitting for multidimensional modeling families of algorithms~//
	Pattern Recognition and Image Analysis.~--- 2010.~--- Vol. 20, No. 4.~--- P. 52–65.

\bibitem{Tolstikhin2010iip}
	{\it Толстихин И. О.}
	Вероятность переобучения некоторых разреженных семейств алгоритмов~//
	Междунар. конф. ИОИ-8. ~--- М:МАКС~Пресс, 2010. ~--- С. 83-86.

\bibitem{Frei10Sym}
	{\it Frei A.\,I.}
	Accurate estimates of the generalization ability for symmetric set of predictors and randomized learning algorithms~//
	Pattern Recognition and Image Analysis.~--- 2010.~--- Vol. 20, No. 3.~--- P. 241–250.

\bibitem{Botov2009MMPR}
	{\it Ботов П. В.}
	Точные оценки вероятности переобучения для монотонных и унимодальных семейств алгоритмов~//
	Всеросс. конф. Математические методы распознавания образов-14.~--- М.: МАКС Пресс, 2009.~--- С. 7–10.

\bibitem{zhuravlev_ryazanov}
	\textit{Журавлёв\;Ю.\,И., Рязанов\;В.\,В., Сенько\;О.\,В.}
	<<Распознавание>>. Математические методы. Программная система. Практические применения.~-- М.:~ФАЗИС, 2006.~-- 176~с.

\bibitem{zhuravlev}
	\textit{Журавлёв\;Ю.\,И.}
	{Об~алгебраическом подходе к~решению задач распознавания или классификации}~//
	Проблемы кибернетики:~Вып.\,33.~--- 1978.~--- С.\,5--68.

\bibitem{guz2011}
	\textit{Гуз\;И.\,С.}
	{Конструктивные оценки полного скользящего контроля для пороговой классификации}~//
	Математическая биология и биоинформатика.~--- 2011.~--- Т.\,6, №\,2.~--- С.\,173-189.

\bibitem{ivahnenko11mmro}
	\textit{Ивахненко\;А.\,А., Воронцов\;К.\,В.}
	{Критерии информативности пороговых логических правил с~поправкой на переобучение порогов}~//
	15-я Всеросс. конф. Математические методы распознавания образов.~--- М.:~МАКС Пресс, 2011.~--- С.~48--51.

%\bibitem{Voron09pria-eng}
%	\textit{Vorontsov K. V.}
%	Splitting and similarity phenomena in the sets of classifiers and their effect on the probability of overfitting //
%	Pattern Recognition and Image Analysis.~-- 2009.~-- Vol. 19, No.~3.~-- Pp. 412-420.
\end{thebibliography}

\end{document} 
