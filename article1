% \noindent{\bf Keywords:}  nonlinear hyperbolic equations, Laplace transformation, Backlund transformation.
\documentclass[12pt]{umj}
\usepackage[cp1251]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[english,russian]{babel}

\usepackage{url} %url
\usepackage{array} %in tabular
\usepackage{enumitem} %enumerate
\usepackage{graphicx} % \includegraphics
\usepackage{subfig} % figure environment
\usepackage[all]{xy} % xy environment
\usepackage[linesnumbered,ruled,vlined,algo2e,oldcommands]{algorithm2e} %algorithm2e

\def\udcs{519.25} %Здесь автор определяет УДК своей работы

\def\currentyear{2016}
\def\currentvolume{1}
\setcounter{tocdepth}{1}
\firstpage{1}
\subjclass{\udcs}

\newcommand{\eps}{\varepsilon}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\X}{\bar{X}}
%\newcommand{\X}{\mathbb{X}}
%\newcommand{\valid}{\bar{X}}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\Y}{\mathbb{Y}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\Expect}{\mathsf{E}}
\newcommand{\cond}{\mspace{3mu}{|}\mspace{3mu}}

\newtheorem{lemma}{Лемма}
\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Определение}
\newtheorem{corollary}{Следствие}
\newtheorem{proposition}{Предложение}

\theoremstyle{definition}
\newtheorem{example}{Пример}
\newtheorem{remark}{Замечание}

\begin{document}

%УДК \udcs
\thispagestyle{empty}

\title[Комбинаторные оценки переобучения пороговых решающих правил]
    {Комбинаторные оценки переобучения пороговых решающих правил}

\author{Ш.\,Х.~Ишкина}
%Указываем авторов
\address{Шаура Хабировна Ишкина,  %Имя, Отчество, Фамилия первого автора
\newline\hphantom{iii} ФИЦ <<Информатика и управление>> РАН,% Место работы
\newline\hphantom{iii} ул. Вавилова, д. 44/2 % Адрес (улица, дом, строение и т.п.)
\newline\hphantom{iii} 119333, г. Москва, Россия}%  Адрес (почтовый индекс, город, страна)  Адрес (почтовый индекс, город, страна)
\email{shaura-ishkina@yandex.ru}% Ваш электронный адрес для переписки

\thanks{\textsc{Sh.\,Kh.\,Ishkina,
    Combinatorial bounds of overfitting for threshold classifiers}}% название статьи на английском языке
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проекты №\,15-37-50350 мол\_нр и №\,14-07-00847.}
\thanks{\copyright \ \textsc{Ишкина Ш. Х.} 2016}
\thanks{\it Поступила 19 декабря 2016 г.}
% (указываем дату отправки, строка будет при получении изменена)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
{
\small
\begin{quote}
\noindent{\bf Аннотация.}
    Оценивание обобщающей способности является фундаментальной задачей теории статистического обучения.
    Тем не менее, точные и вычислительно эффективные оценки до сих пор не известны даже для многих простых частных случаев.
    В~данной работе исследуется семейство одномерных пороговых решающих правил.
    Применяется комбинаторная теория переобучения, основанная на единственном вероятностном допущении, что
    все разбиения множества объектов на обучающую и тестовую выборки равновероятны.
    Предлагается полиномиальный алгоритм для вычисления функционалов вероятности переобучения и полного скользящего контроля.

\noindent{\bf Ключевые слова: }{
    статистическое обучение, минимизации эмпирического риска, комбинаторная теория переобучения,
    вероятность переобучения, полный скользящий контроль, обобщающая способность, пороговое правило, вычислительная сложность}

\medskip
\noindent{\bf Abstract.}
    Tightening generalization bounds is a fundamental objective of statistical learning theory.
    However, accurate and computationally efficient bounds are still unknown even for very simple cases.
    In~this paper, we consider one-dimensional threshold decision rules.
    We~use the framework of combinatorial theory of overfitting,
    which is based on a single probabilistic assumption that
    all partitions of a~set of objects into an observed training sample and a~hidden test sample can occur with equal probability.
    We~propose a~polynomial algorithm for computing both probability of overfitting and complete cross-validation.

\noindent{\bf Keywords: }{
    computational learning theory, empirical risk minimization, combinatorial theory of overfitting,
    probability of overfitting, complete cross-validation, generalization bound, threshold classifier, computational complexity}

\bigskip
\noindent{\bf Mathematics Subject Classification:} 68Q32, 60C05

\end{quote}
}

\section{Введение}
\setcounter{section}{1}
\setcounter{equation}{0}

%% предлагаю заменить: классификатор -> правило

Рассмотрим следующую математическую модель принятия решений в~условиях неполноты информации.
Задана бинарная матрица, строки которой соответствуют объектам, столбцы~--- правилам принятия решений, называемым также классификаторами или гипотезами.
В~ячейке матрицы находится единица тогда и~только тогда, когда данный классификатор ошибается на~данном объекте.
Из~множества~$\XX$ всех строк матрицы случайно и~равновероятно выбирается наблюдаемая обучающая выборка~--- подмножество ${X\subset\XX}$ фиксированной мощности.
Затем из~множества~$\AA$ всех столбцов матрицы выбирается классификатор с~минимальной частотой ошибок на~$X$.
Требуется оценить частоту ошибок этого классификатора на скрытой контрольной выборке ${\X = \XX \backslash X}$.
Если разность частот ошибок на контрольной и~обучающей выборках превышает~$\eps$, то~говорят, что произошло переобучение.
Получение верхних оценок вероятности переобучения является одной из основных задач теории статистического обучения~%
\cite{vapnik71uniform,boucheron05theory,koltchinskii11oracle}.

Классические оценки Вапника--Червоненкиса~\cite{vapnik71uniform} зависят только от размера матрицы ошибок.
Будучи оценками <<худшего случая>>, они завышены на~порядки и~плохо согласуются с~результатами экспериментов~\cite{voron08pria-eng}.
Более тонкие оценки зависят от свойств отношения частичного порядка на~множестве вектор-столбцов матрицы ошибок~\cite{haussler94predicting}.
В~комбинаторной теории переобучения~\cite{voron04qualdan,voron09dan,voron09roai2008}
обосновывается необходимость сочетания двух свойств, расслоения и~связности~\cite{voron10pria-eng,voron11premi}.
Благодаря расслоению, классификаторы с~высокой вероятностью ошибки вносят пренебрежимо малый вклад в~переобучение.
Благодаря связности, у~классификаторов с~близкими векторами ошибок резко снижается вклад в~переобучение.

В~\cite{zhivotovskiy12iip} получены условия, при которых оценка расслоения--связности \mbox{является точной}.
Им~удовлетворяют, в~частности, монотонные и~унимодальные цепи классификаторов~\cite{voron10pria-eng}.
В~практических задачах статистического обучения
такие цепи могут порождаться элементарными пороговыми правилами~\cite{zhuravlev06recognition},
но~лишь при слабо реалистичном предположении о~существовании безошибочного правила.
В~общем случае пороговые правила порождают последовательности классификаторов, называемые прямыми цепями.
Ранее для них были известны лишь верхние оценки ожидаемой частоты ошибок на контрольной выборке~\cite{guz11mbb}.
Различные уточнения оценок расслоения--связности, например,
учитывающие попарную конкуренцию между классификаторами~\cite{sokolov13jmlda}
или послойную кластеризацию множества классификаторов~\cite{frey13jmlda,frey14dan},
также остаются завышенными для прямых цепей.

В~данной работе предлагается алгоритм полиномиальной сложности для вычисления вероятности переобучения произвольной прямой цепи.

\subsection{Основные определения.}

Задано
конечное множество ${\XX = \{ x_1, \dots, x_L \}}$, элементы которого называются \emph{объектами}, и
конечное множество~$\AA$, элементы которого называются \emph{классификаторами}.
Множество $\AA$ называется \emph{семейством классификаторов.}

Задана функция ${I \colon \AA \times \XX \rightarrow \{0, \, 1\}}$, называемая \emph{индикатором ошибки}.
Если~${I(a, x) = 1}$, то говорят, что классификатор~$a$ \emph{допускает ошибку} на~объекте~$x$.
%Если~${I(a, x) = 0}$, то говорят, что классификатор~$a$~\emph{не ошибается} на~данном объекте.
Бинарная матрица $\bigl( I(a, x) \colon x\in\XX, a\in\AA \bigr)$ размера $|\XX|{\times}|\AA|$ называется \emph{матрицей ошибок}.

Предполагается, что каждому классификатору~${a \in\AA}$~взаимно однозначно соответствует его вектор ошибок~${(I(a, x_i))_{i = 1}^L}$,
то есть в~матрице ошибок не~может быть двух равных столбцов.
Договоримся обозначать через~$a$ как классификатор, так~и его вектор ошибок.

\emph{Числом ошибок} классификатора~$a$~на~выборке $X \subset \XX$ называется величина
\[
    n(a, X) = \sum_{x \in X} I(a, x).
\]

\emph{Частотой ошибок} классификатора $a$ на выборке $X \subset \XX$ называется величина
\[
    \nu(a, X) = {n(a, X)} / {|X|}.
\]

Обозначим через $[X]^l$ множество всех подмножеств $\XX$ мощности~$l < L$.
Подмножества $X\in [X]^l$ будем называть \emph{обучающими выборками},
а~их дополнения ${\X = \XX \backslash X}$~--- \emph{контрольными выборками}.
Введем на~множестве $[X]^l$ равномерное распределение вероятностей:
\[
    \Prob(X) = {1} / {C_L^l},
    \qquad
    X \in [X]^l.
\]

\emph{Переобученностью} классификатора~$a$~на~разбиении~$(X, \X)$ называется величина
\[
    \delta(a, X) = \nu(a, \X) - \nu(a, X).
\]

Если $\delta(a, X)>\eps$, то будем говорить, что классификатор~$a$ переобучен на~$X$.

%\emph{Эмпирической оценкой} некоторой функции $\phi(X, \X)$, не зависящей от порядка элементов в~выборках $X$ и $\X$, называется величина $\hat{\Expect} \phi(X, \X)$, полученная методом Монте-Карло путем усреднения по некоторому случайному подмножеству выборок $N \subset [X]^l$
%\[
%\hat{\Expect} \phi(X, \X) = \frac{1}{|N|} \sum_{X \in N} \phi(X, \X).
%\]
%TODO найти подходящее место
%Недостаток эмпирических оценок заключается в~низкой точности и большой вычислительной сложности.

\emph{Методом обучения} называется отображение $\mu \colon [X]^l \rightarrow \AA$, которое
каждой обучающей выборке~$X$ ставит в~соответствие классификатор $a=\mu X$ из~семейства~$\AA$.

\emph{Пессимистичной минимизацией эмпирического риска (ПМЭР)} называется
метод обучения, который выбирает классификатор, допускающий наименьшее число ошибок на~обучающей выборке~$X$,
а~если таких классификаторов в~семействе несколько, то~выбирает из~них классификатор с~наибольшим числом ошибок на~контрольной выборке~$\X$~\cite{voron10pria-eng}.

Для фиксированного метода обучения~$\mu$, семейства классификаторов~$\AA$, множества~$\XX$ и~объема обучающей выборки~$l$
\emph{вероятностью переобучения} называется функционал
\[
    Q_{\eps}(\mu, \AA, \XX, l)
    = \Prob[\delta(\mu X, X) \geqslant \eps]
    = \frac{1}{C_L^l} \sum_{X \in [X]^l} [\delta(\mu X, X) \geqslant \eps].
\]

Здесь и далее квадратные скобки будут использоваться для преобразования логического условия в~числовое значение по~правилу
$[\text{истина}] = 1$, $[\text{ложь}] = 0$.

\emph{Полным скользящим контролем} (complete cross-validation, CCV)
называется функционал, равный математическому ожиданию числа ошибок на~контрольной выборке:
\[
    CCV(\mu, \AA, \XX, l)
    = \Expect \nu(\mu X, \X)
    = \frac{1}{C_L^l} \sum_{X \in [X]^l} \nu(\mu X, \X).
\]

Эффективное вычисление $Q_{\eps}$ и $CCV$ непосредственно по~определению
возможно только при малых~$|\X|=L-l$.
Если $l$~близко~к~$L/2$, то число слагаемых экспоненциально по~$L$.

%Для краткости будем опускать параметры, от~которых зависят данные показатели, и записывать~$Q_{\eps}$ и~$CCV$.

%Функционалы вероятности переобучения и полного скользящего контроля характеризуют обобщающую способность метода обучения.

%Получение оценок $Q_{\eps}$ и $CCV$ проводится в~два этапа.
%На~первом этапе выводятся комбинаторные оценки, зависящие от некоторых статистик $S(\XX)$, посчитанных по всей матрице ошибок.
%На~втором этапе статистики $S(\XX)$ оцениваются по случайной подматрице~$S(X)$.
%Эмпирический способ решения задачи второго этапа предложен в~\cite{ivahnenko11mmro}.
%В данной работе рассматривается только первый этап.

\subsection{Прямые последовательности классификаторов.}
%\begin{definition}
%\emph{Расстоянием между классификаторами}  называется расстояние Хемминга между их векторами ошибок:
%\[\rho(a, a') = \sum_{i = 1}^{L}[I(a, x_i) \neq I(a', x_i)], \quad \forall a, a' \in \AA,\]
%где квадратные скобки обозначают предикат [истина] = 1, [ложь] = 0.
%\end{definition}

Рассмотрим множества объектов, по которым различаются соседние классификаторы семейства $\AA = \{a_0, \dots, a_P\}$:
\begin{align}
    \label{eq:edgesdef}
    G_p = \bigl\{x \in \XX \cond I(a_p, x) \neq I(a_{p + 1}, x) \bigr\}, \quad p = 0, \dots, P - 1.
\end{align}

\begin{definition}
    Семейство классификаторов называется \textit{прямой последовательностью}, если множества $G_p$ попарно не~пересекаются.
\end{definition}

\begin{definition}
    Прямая последовательность ${\AA = \{a_0, \dots, a_P\}}$ называется \textit{прямой цепью},
    если каждая пара соседних классификаторов различается по одному объекту:
    $|G_p| = 1$,\; $p = 0, \dots, P - 1$.
    Число~$P$~называется \textit{длиной} прямой цепи~$\AA$.
\end{definition}

\begin{example}
    Пусть ${x_1 \leq x_2 \leq \cdots \leq x_L}$~--- точки числовой оси,
    и~каждому~$x_i$ соответствует правильное решение ${y_i\in\{0,1\}}$.
    Рассмотрим семейство пороговых правил
    ${a(x,\theta) = [x \geqslant \theta]}$ с~параметром ${\theta\in\RR}$.
    Введём индикатор ошибки
    ${I(a,x_i) = \bigl[ a(x_i,\theta) \neq y_i \bigr]}$.
    Варьирование~$\theta$ порождает не~более ${L+1}$ классификаторов с~попарно различными векторами ошибок.
%    Выберем значения порогов
%    \[
%        \theta_0 = x_1-1,\quad
%        \theta_p = \frac12(x_p+x_{p+1}),\; p=1,\dots,L-1,\quad
%        \theta_L = x_L+1.
%    \]
    Они~образуют прямую последовательность.
    Если все объекты~$x_i$ попарно различны, ${x_1 < x_2 < \cdots < x_L}$,
    то~прямая последовательность является прямой цепью.
    На~рис.~\ref{fig:simple_chain} показан пример прямой цепи.
    По~оси~$x$ отложены объекты~$x_i$.
    Правильные решения~$y_i$ показаны точками~$\circ$~и~$\bullet$.
    Пороги~$\theta$ выбраны посередине между соседними объектами.
    Ниже показан график числа ошибок классификаторов и~матрица ошибок.

\end{example}

\begin{figure}[t]
    \shorthandoff{"}\small
	\begin{center}
	%\subfloat[Прямая цепь общего вида]
%	{
	\fbox{
    \begin{xy}<0.7ex,0ex>:
        \POS(0,100)\ar@{->}(49,100)*{}
        \POS(49,100),+/u1em/*{x}
        \POS(5,100)*@{*},+/u1em/*{x_1}
        \POS(11,100)*@{*},+/u1em/*{x_2}
        \POS(17,100)*@{o},+/u1em/*{x_3}
        \POS(23,100)*@{o},+/u1em/*{x_4}
        \POS(29,100)*@{o},+/u1em/*{x_5}
        \POS(35,100)*@{*},+/u1em/*{x_6}
        \POS(41,100)*@{o},+/u1em/*{x_7}
        \POS(2,100)*@{|},+/d1em/*++{a_0}="t0"
        \POS(8,100)*@{|},+/d1em/*++{a_1}="t1"
        \POS(14,100)*@{|},+/d1em/*++{a_2}="t2"
        \POS(20,100)*@{|},+/d1em/*++{a_3}="t3"
        \POS(26,100)*@{|},+/d1em/*++{a_4}="t4"
        \POS(32,100)*@{|},+/d1em/*++{a_5}="t5"
        \POS(38,100)*@{|},+/d1em/*++{a_6}="t6"
        \POS(44,100)*@{|},+/d1em/*++{a_7}="t7"
        %
        \POS(2,84)\ar@{.}(44,84)*{}
        \POS(2,86)\ar@{.}(44,86)*{}
        \POS(2,88)\ar@{.}(44,88)*{}
        \POS(2,90)\ar@{.}(44,90)*{}
        \POS(2,92)\ar@{.}(44,92)*{}
        %
        \POS(2,90)*@{*}="a0"
        \POS(8,88)*@{*}="a1"\ar@{-}"a0"
        \POS(14,86)*@{*}="a2"\ar@{-}"a1"
        \POS(20,88)*@{*}="a3"\ar@{-}"a2"
        \POS(26,90)*@{*}="a4"\ar@{-}"a3"
        \POS(32,92)*@{*}="a5"\ar@{-}"a4"
        \POS(38,90)*@{*}="a6"\ar@{-}"a5"
        \POS(44,92)*@{*}="a7"\ar@{-}"a6"
        %
        \POS(2,80)*++{1}\ar @{.} "t0"
        \POS(8,80)*++{0}\ar @{.} "t1"
        \POS(14,80)*++{0}\ar @{.} "t2"
        \POS(20,80)*++{0}\ar @{.} "t3"
        \POS(26,80)*++{0}\ar @{.} "t4"
        \POS(32,80)*++{0}\ar @{.} "t5"
        \POS(38,80)*++{0}\ar @{.} "t6"
        \POS(44,80)*++{0}\ar @{.} "t7"
        %
        \POS(2,76)*{1}\POS(8,76)*{1}\POS(14,76)*{0}\POS(20,76)*{0}\POS(26,76)*{0}\POS(32,76)*{0}\POS(38,76)*{0}\POS(44,76)*{0}
        \POS(2,72)*{0}\POS(8,72)*{0}\POS(14,72)*{0}\POS(20,72)*{1}\POS(26,72)*{1}\POS(32,72)*{1}\POS(38,72)*{1}\POS(44,72)*{1}
        \POS(2,68)*{0}\POS(8,68)*{0}\POS(14,68)*{0}\POS(20,68)*{0}\POS(26,68)*{1}\POS(32,68)*{1}\POS(38,68)*{1}\POS(44,68)*{1}
        \POS(2,64)*{0}\POS(8,64)*{0}\POS(14,64)*{0}\POS(20,64)*{0}\POS(26,64)*{0}\POS(32,64)*{1}\POS(38,64)*{1}\POS(44,64)*{1}
        \POS(2,60)*{1}\POS(8,60)*{1}\POS(14,60)*{1}\POS(20,60)*{1}\POS(26,60)*{1}\POS(32,60)*{1}\POS(38,60)*{0}\POS(44,60)*{0}
        \POS(2,56)*{0}\POS(8,56)*{0}\POS(14,56)*{0}\POS(20,56)*{0}\POS(26,56)*{0}\POS(32,56)*{0}\POS(38,56)*{0}\POS(44,56)*{1}
        %
        \POS(-6,92)*{\scriptstyle n(a_p,\XX)}
        \POS(-6,80)*{x_1}\POS(-6,76)*{x_2}\POS(-6,72)*{x_3}\POS(-6,68)*{x_4}\POS(-6,64)*{x_5}\POS(-6,60)*{x_6}\POS(-6,56)*{x_7}
    \end{xy}}
%	}
	%\\
%	\subfloat[Монотонная возрастающая цепь]
%	{\label{fig:monot_chain}
%	\fbox{\begin{xy}<0.7ex,0ex>:
%            \POS(0,100)\ar@{->}(43,100)*{}
%            \POS(42,100),+/u1em/*{x}
%            \POS(5,100)*@{*},+/u1em/*{x_1}
%            \POS(11,100)*@{*},+/u1em/*{x_2}
%            \POS(17,100)*@{o},+/u1em/*{x_3}
%            \POS(23,100)*@{o},+/u1em/*{x_4}
%            \POS(29,100)*@{o},+/u1em/*{x_5}
%            \POS(35,100)*@{o},+/u1em/*{x_6}
%            \POS(14,100)*@{|},+/d1em/*++{a_2}="t0"
%            \POS(20,100)*@{|},+/d1em/*++{a_3}="t1"
%            \POS(26,100)*@{|},+/d1em/*++{a_4}="t2"
%            \POS(32,100)*@{|},+/d1em/*++{a_5}="t3"
%            \POS(38,100)*@{|},+/d1em/*++{a_6}="t4"
%            %
%            \POS(14,80)*++{0}\ar @{.} "t0"
%            \POS(20,80)*++{0}\ar @{.} "t1"
%            \POS(26,80)*++{0}\ar @{.} "t2"
%            \POS(32,80)*++{0}\ar @{.} "t3"
%            \POS(38,80)*++{0}\ar @{.} "t4"
%            %
%            \POS(14,84)\ar@{.}(38,84)*{}\POS(14,84)*@{*}="a0"
%            \POS(14,86)\ar@{.}(38,86)*{}\POS(20,86)*@{*}="a1"\ar@{-}"a0"
%            \POS(14,88)\ar@{.}(38,88)*{}\POS(26,88)*@{*}="a2"\ar@{-}"a1"
%            \POS(14,90)\ar@{.}(38,90)*{}\POS(32,90)*@{*}="a3"\ar@{-}"a2"
%            \POS(14,92)\ar@{.}(38,92)*{}\POS(38,92)*@{*}="a4"\ar@{-}"a3"
%            %
%            \POS(14,76)*{0}\POS(20,76)*{0}\POS(26,76)*{0}\POS(32,76)*{0}\POS(38,76)*{0}
%            \POS(14,72)*{0}\POS(20,72)*{1}\POS(26,72)*{1}\POS(32,72)*{1}\POS(38,72)*{1}
%            \POS(14,68)*{0}\POS(20,68)*{0}\POS(26,68)*{1}\POS(32,68)*{1}\POS(38,68)*{1}
%            \POS(14,64)*{0}\POS(20,64)*{0}\POS(26,64)*{0}\POS(32,64)*{1}\POS(38,64)*{1}
%            \POS(14,60)*{0}\POS(20,60)*{0}\POS(26,60)*{0}\POS(32,60)*{0}\POS(38,60)*{1}
%            %
%            \POS(6,92)*{\scriptstyle n(a_p,\XX)}
%            \POS(4,80)*{x_1}\POS(4,76)*{x_2}\POS(4,72)*{x_3}\POS(4,68)*{x_4}\POS(4,64)*{x_5}\POS(4,60)*{x_6}
%        \end{xy}}
%
%    }
%    \qquad
%    \subfloat[Унимодальная цепь~-- прямая цепь, состоящая из~двух участков монотонности]
%    {
%    \label{fig:combined_from_monot_chain}
%        \fbox{\begin{xy}<0.7ex,0ex>:
%            \POS(0,100)\ar@{->}(43,100)*{}
%            \POS(42,100),+/u1em/*{x}
%            \POS(5,100)*@{*},+/u1em/*{x_1}
%            \POS(11,100)*@{*},+/u1em/*{x_2}
%            \POS(17,100)*@{o},+/u1em/*{x_3}
%            \POS(23,100)*@{o},+/u1em/*{x_4}
%            \POS(29,100)*@{o},+/u1em/*{x_5}
%            \POS(35,100)*@{o},+/u1em/*{x_6}
%            \POS(2,100)*@{|},+/d1em/*++{a_0}="t0"
%            \POS(8,100)*@{|},+/d1em/*++{a_1}="t1"
%            \POS(14,100)*@{|},+/d1em/*++{a_2}="t2"
%            \POS(20,100)*@{|},+/d1em/*++{a_3}="t3"
%            \POS(26,100)*@{|},+/d1em/*++{a_4}="t4"
%            \POS(32,100)*@{|},+/d1em/*++{a_5}="t5"
%            \POS(38,100)*@{|},+/d1em/*++{a_6}="t6"
%            %
%            \POS(2,84)\ar@{.}(38,84)*{}
%            \POS(2,86)\ar@{.}(38,86)*{}
%            \POS(2,88)\ar@{.}(38,88)*{}
%            \POS(2,90)\ar@{.}(38,90)*{}
%            \POS(2,92)\ar@{.}(38,92)*{}
%            %
%            \POS(2,88)*@{*}="a0"
%            \POS(8,86)*@{*}="a1"\ar@{-}"a0"
%            \POS(14,84)*@{*}="a2"\ar@{-}"a1"
%            \POS(20,86)*@{*}="a3"\ar@{-}"a2"
%            \POS(26,88)*@{*}="a4"\ar@{-}"a3"
%            \POS(32,90)*@{*}="a5"\ar@{-}"a4"
%            \POS(38,92)*@{*}="a6"\ar@{-}"a5"
%            %
%            \POS(2,80)*++{1}\ar @{.} "t0"
%            \POS(8,80)*++{0}\ar @{.} "t1"
%            \POS(14,80)*++{0}\ar @{.} "t2"
%            \POS(20,80)*++{0}\ar @{.} "t3"
%            \POS(26,80)*++{0}\ar @{.} "t4"
%            \POS(32,80)*++{0}\ar @{.} "t5"
%            \POS(38,80)*++{0}\ar @{.} "t6"
%            %
%            \POS(2,76)*{1}\POS(8,76)*{1}\POS(14,76)*{0}\POS(20,76)*{0}\POS(26,76)*{0}\POS(32,76)*{0}\POS(38,76)*{0}
%            \POS(2,72)*{0}\POS(8,72)*{0}\POS(14,72)*{0}\POS(20,72)*{1}\POS(26,72)*{1}\POS(32,72)*{1}\POS(38,72)*{1}
%            \POS(2,68)*{0}\POS(8,68)*{0}\POS(14,68)*{0}\POS(20,68)*{0}\POS(26,68)*{1}\POS(32,68)*{1}\POS(38,68)*{1}
%            \POS(2,64)*{0}\POS(8,64)*{0}\POS(14,64)*{0}\POS(20,64)*{0}\POS(26,64)*{0}\POS(32,64)*{1}\POS(38,64)*{1}
%            \POS(2,60)*{0}\POS(8,60)*{0}\POS(14,60)*{0}\POS(20,60)*{0}\POS(26,60)*{0}\POS(32,60)*{0}\POS(38,60)*{1}
%            %
%            \POS(-6,92)*{\scriptstyle n(a_p,\XX)}
%            \POS(-6,80)*{x_1}\POS(-6,76)*{x_2}\POS(-6,72)*{x_3}\POS(-6,68)*{x_4}\POS(-6,64)*{x_5}\POS(-6,60)*{x_6}
%        \end{xy}}}
	\end{center}
	\caption{Пример прямой цепи}
    \label{fig:simple_chain}
\end{figure}

\begin{definition}
    Прямая цепь $\AA = \{a_0, \dots, a_P\}$ называется \textit{возрастающей} (\textit{убывающей}),
    если каждый классификатор~$a_p$~допускает~$m + p$ (соответственно, ${m - p}$) ошибок на~множестве~$\XX$ при некотором значении~$m$.
    Прямую цепь~$\AA$~будем называть \textit{монотонной}, если она является убывающей или возрастающей.
\end{definition}

Прямая цепь~$\AA$ может состоять из~нескольких участков монотонности.
%то~убывание или возрастание каждого участка договоримся определять
%в~направлении от~первого классификатора цепи~$\AA$~к~последнему.
Например, в~цепи, показанной на рис.~\ref{fig:simple_chain},
имеется четыре участка монотонности:
$\{a_0, a_1, a_2\}$ и $\{a_5, a_6\}$~--- убывающие,
$\{a_2, a_3, a_4, a_5\}$ и $\{a_6, a_7\}$~--- возрастающие.

%\begin{example}
%\label{ex:overfitting_comparison}
%На~рисунке \ref{fig:various_simple_chains} изображены прямые цепи различной формы и горизонтальными линиями показаны эмпирические оценки частоты ошибок ПМЭР на~обучающей выборке $\hat{\Expect} \nu (\mu X, X)$ и на контрольной $\hat{\Expect} \nu (\mu X, \X)$.
%Чем больше разность между ними, равная $\hat{\Expect} \delta (\mu X, X)$, тем сильнее переобучается семейство.
%Условия эксперимента:~${L = 200, \, l = 100, \, \eps = 0.05}$.
%Эмпирические оценки вычислялись методом Монте-Карло по~$10^5$ разбиениям.
%
%\emph{Слоем} называется множество классификаторов, допускающих на~генеральной выборке равное число ошибок.
%Чем меньше ошибок допускает классификатор, тем ниже его слой.
%Данный эксперимент показывает, что одни семейства переобучаются значительно сильнее, чем другие: переобучение тем выше, чем больше классификаторов находится в нижних слоях семейства и чем более они различны.
%
%Вследствие этого ставится задача точного вычисления вероятности переобучения и полного скользящего контроля прямой цепи общего вида.
%\end{example}
%
%\begin{figure}[t]
%\begin{center}
%\includegraphics[width=\textwidth]{chain_examples.eps}
%\end{center}
%\caption{Эмпирические оценки частоты ошибок ПМЭР на~обучающей и контрольной выборках для~прямых цепей различной формы, вычисленные методом Монте-Карло по~$10^5$ разбиениям.
%Условия эксперимента:~${L = 200, \, l = 100, \, \eps = 0.05.}$
%Минимальная частота ошибок равна~${0.25}$ для~всех цепей. }
%\label{fig:various_simple_chains}
%\end{figure}

\subsection{Постановка задачи.}

Найти способ вычисления функционалов
вероятности переобучения~$Q_{\eps}$ и~полного скользящего контроля~$CCV$
за полиномиальное по~$L$ время
для ПМЭР~$\mu$ и~произвольной прямой последовательности~$\AA$.

\section{Переобучение произвольного семейства}

Пусть дано произвольное подмножество ${\D \subseteq \XX}$ множества~$\XX$.
Каждое разбиение~${(X, \X)}$ множества~${\XX = X \sqcup \X}$ индуцирует разбиение~${(X \cap \D, \X \cap \D)}$ подмножества~$\D$.
Также любая пара разбиений~${(D', \bar{D}')}$ и~$(D'', \bar{D}'')$
подмножеств~${\D' \subseteq \XX}$ и~${\D'' = \XX \backslash \D'}$ соответственно
определяет разбиение~$(X, \X)$ множества~$\XX$ по~правилу
${X = D' \cup D''}$ и~${\X = \bar{D}' \cup \bar{D}''}$.

Назовем пару классификаторов $a$ и $a'$ \emph{неразличимыми на~подмножестве} $X_0 \subseteq \XX$, если
$I(a, x) = I(a', x)$ для всех $x \in X_0$.

Обозначим через~$\D$ подмножество объектов, по которым классификаторы семейства $\AA = \{a_0, \ldots, a_P\}$ различимы:
\begin{align}
    \label{d_definition}
    \D =  G_0 \sqcup \dots \sqcup G_{P-1}
    = \bigl\{x \in \XX \cond \exists \, a, a' \in \AA \colon  I(a, x) \neq I(a', x) \bigr\},
\end{align}
где множества $G_p$ определяются согласно~\eqref{eq:edgesdef}.

Объекты множества ${\N = \XX \setminus \D}$ назовем \emph{нейтральными}.
На~множестве~$\N$ классификаторы семейства неразличимы и допускают одинаковое число ошибок~$m$.
Через~$m_p$ обозначим число ошибок классификатора~$a_p$ на~множестве~$\D$:
\begin{align}
    \label{errors_count_definition}
    &m = n(a, \N),\quad \forall a \in \AA; \\
    &m_p = n(a_p, \D). \nonumber
\end{align}
%Таким образом, классификатор~$a_p$~допускает~${n(a_p, \XX) = m + m_p}$ ошибок на~выборке~$\XX$.

%\subsection{Сведение задачи к нахождению числа разбиений.}
Сведем задачу вычисления вероятности переобучения~$Q_\eps$ и полного скользящего контроля~$CCV$
к~нахождению числа разбиений множества~$\D$ с~некоторыми ограничениями.

Будем обозначать через~$t$ число объектов из~$\D$, попавших в~обучающую выборку~$X$,
а~через~$e$~--- число ошибок классификатора~$a_p$ на~этих объектах.
Введём две функции от~$t$~и~$e$:
число разбиений множества~$\N$, таких, что классификатор $a_p$ переобучен на~$X$
\[
    N_p(t, e) = \# \bigl\{
        (X \cap \N, \X \cap \N) \bigm|
        \delta(a_p, X) \geqslant \eps,\;
        t = |X \cap \D|,\;
        e = n(a_p, X \cap \D)
    \bigr\}
\]
и число разбиений множества~$\D$, таких, что $a_p$ является результатом обучения: ${\mu X = a_p}$:
\[
    D_p(t, e) = \# \bigl\{
        (X \cap \D, \X \cap \D) \bigm|
        \mu X = a_p,\;
        t = |X \cap \D|,\;
        e = n(a_p, X \cap \D)
    \bigr\}.
\]

Введём \emph{гипергеометрическую функцию распределения}
\[
    H^{l, m}_L(s)
    = \frac{1}{C_L^l}
        \sum_{i = 0}^{\min \{\lfloor s \rfloor, l, m\} }
        C_m^i C_{L - m}^{l - i},
\]
где~$\lfloor x \rfloor$~--- целая часть~$x$, то есть наибольшее целое число, не~превосходящее~$x$.
Гипергеометрическая функция распределения $H^{l, m}_L(s)$ для~данного множества~$\XX$~мощности~$L$~и выборки $X_0 \subset \XX$ объема~$m$~равна доле выборок множества~$\XX$ объема~$l$, содержащих не более~$s$~элементов из $X_0$.
Будем полагать~${C_n^i = 0}$ при~невыполнении условия $0 \leqslant i \leqslant n$.


\begin{theorem}
    \label{theo:arbitrary_chain}
    Для произвольного семейства классификаторов~$\AA = \{a_0, \ldots, a_P\}$,
    метода ПМЭР $\mu$,
    множества~$\XX$ мощности~$L$, объема обучающей выборки~$l$,
    точности~$\eps \in (0,\, 1)$ вероятность переобучения имеет вид
    \begin{align}
        \label{q_eps_arbitrary}
        Q_{\eps}
        =
        \frac{1}{C_L^l}
            \sum_{p = 0}^{P}
            \sum_{(t, e) \in \Psi_p}
                D_p(t, e) N_p(t, e), %\sum_{e = 0}^{\hat{e}}
    \end{align}
    где множество~$\D$, параметры $m_p$ и $m$~определяются по~\eqref{d_definition} и~\eqref{errors_count_definition}
    \begin{align}
        &\Psi_p = \bigl\{(t, e) \, |\, 0 \leqslant t \leqslant \min\{l, |\D|\},  \; 0 \leqslant e \leqslant \min\{t, m_p\} \bigr\};
            \label{train_errors_upper_bounds}\\
        &N_p(t, e) = C_{L - |\D|}^{l - t} \: H_{L - |\D|}^{l - t, m}(s_{p}(e));
            \label{n_p_arbitrary_chain} \\
        &s_{p}(e) = \frac{l}{L}(n(a_p, \XX) - \eps (L - l)) - e. \nonumber
    \end{align}
\end{theorem}

\begin{proof}
Представим вероятность переобучения в~виде
\begin{align*}
    Q_{\eps} = \sum_{p = 0}^{P} \Prob \bigl[\mu X = a_p\bigr] \bigl[\delta(a_p, X) \geqslant \eps \bigr].
\end{align*}

Рассмотрим множество разбиений~${(X, \X)}$ с~фиксированными значениями~$t$ и~$e$:
\begin{align}
    \label{t_e_definition}
    &t = |X \cap \D|, \quad e = n(a_p, X \cap \D).
\end{align}

Множество допустимых значений $(t, e)$ есть $\Psi_p$, согласно~\eqref{train_errors_upper_bounds}.

Для~таких разбиений
выполнение условия $\delta(a_p, X) \geqslant \eps$ не зависит от~выбора разбиения множества~$\D$, а
выполнение условия $\mu X = a_p$, не~зависит от~выбора разбиения множества~$\N$,
поскольку классификаторы неразличимы на~множестве~$\N$.
Поэтому для каждой тройки параметров $p,\, t,\, e$ число разбиений множества~$\XX$, таких, что
одновременно выполнены условия ${\mu X = a_p}$ и ${\delta(\mu X, X) \geqslant \eps}$,
равно произведению $N_p(t, e) D_p(t, e)$.

Докажем~\eqref{n_p_arbitrary_chain}.
Пусть~${n(a_p, X \cap \N) = s}$,~тогда~${n(a_p, X) = e + s}$.
Условие $\delta(a_p, X) \geqslant \eps$ эквивалентно условию $n(a_p, X) \leqslant \frac{l}{L}(n(a_p, \XX) - \eps (L - l))$,
значит, $s \leq s_{p}(e)$.
Число разбиений множества~$\N$ при~данных~$t$~и~$s$ равно $C_m^s C_{L - |\D| - m}^{l - t - s}$, откуда следует
\begin{align*}
    N_p(t, e)
    = \sum_{s = 0}^{s_{p}(e)} C_{m}^{s} C_{L - |\D| - m}^{l - t - s}
    = C_{L - |\D|}^{l - t} \frac{1}{C_{L - |\D|}^{l - t}} \sum_{s = 0}^{s_{p}(e)} C_{m}^{s} C_{L - |\D| - m}^{l - t - s}
    = C_{L - |\D|}^{l - t} \: H_{L - |\D|}^{l - t, m}(s_{p}(e)).
\end{align*}
%Заметим, что, разделив и~домножив каждое слагаемое на~$C_{L - P}^{l - t}$,~получим в~точности~\eqref{n_p_arbitrary_chain}.
\vskip-3ex
\end{proof}

Для функционала полного скользящего контроля имеет место аналогичная теорема.
\begin{theorem}
    \label{theo:ccv_arbitrary}
    Для произвольного семейства классификаторов~$\AA = \{a_0, \ldots, a_P\}$,
    метода ПМЭР $\mu$,
    множества~$\XX$ мощности~$L$, объема обучающей выборки~$l$,
    функционал полного скользящего контроля имеет вид
    \begin{align}
    CCV = \frac{1}{(L - l) C_L^l} \sum_{p = 0}^{P} \sum_{(t, e) \in \Psi_p} D_p(t, e) F_p(t, e),  %^{\hat{t}} \sum_{e = 0}^{\hat{e}}
    \label{ccv_arbitrary}
    \end{align}
    где
    \begin{align}
    &F_p(t, e) = \sum_{s = 0}^{\min \{ l - t, m\}} C_m^s C_{L - |\D| - m}^{l - t - s}\bigl(n(a_p, \XX) - s - e \bigr),\label{neutral_error_rate_exp}
    \end{align}
    множества~$\D$ и $\Psi_p$ определяются по~\eqref{d_definition} и \eqref{train_errors_upper_bounds}, параметры $m_p$ и $m$~определяются по~\eqref{errors_count_definition}.
\end{theorem}

\begin{proof}
Запишем формулу полного скользящего контроля и переставим в~ней знаки суммирования:
\begin{align*}
CCV
= \frac{1}{C_L^l}  \sum_{X \in [X]^l} \sum_{p = 0}^{P} [\mu X = a_p] \: \nu(a_p, \X)
= \frac{1}{C_L^l}  \sum_{p = 0}^{P} \sum_{X \in [X]^l} [\mu X = a_p] \: \nu(a_p, \X).
\end{align*}

Выполнение условия ${\mu X = a_p}$ не зависит от~выбора разбиения множества~$\N$.

Представим число ошибок классификатора $a_p$ на~контрольной выборке в виде
\[
    n(a_p, \X) = n(a_p, \XX) - n(a_p, X) = n(a_p, \XX) - n(a_p, X \cap \D) - n(a_p, X \cap \N).
\]

Определим параметры~$t$~и~$e$ по~формулам~(\ref{t_e_definition}).
Обозначим $s = {n(a_p, X \cap \N)}$.
Из~ограничений $s + t \leqslant l$ и $s \leqslant m$ следует верхняя оценка параметра~$s$~в~\eqref{neutral_error_rate_exp}.

Легко проверить, что число разбиений множества~$\N$~при~данных~$t$~и~$s$ равно $C_m^s C_{L - |\D| - m}^{l - t - s}$, откуда следует утверждение теоремы.
\end{proof}

Таким образом, задача сводится к~вычислению для~каждого~$p$ значений~$D_p(t, e)$ на всем множестве $\Psi_p$.
Для случая прямой последовательности далее будет описан рекуррентный алгоритм вычисления $D_p(t, e)$.

\section{Вычисление количества разбиений множества ребер прямой последовательности}
Пусть теперь семейство $\AA = \{a_0, \ldots, a_P\}$ является прямой последовательностью.
Объекты множества $\D$ будем называть \emph{ребрами прямой последовательности}~$\AA$.
%Поставим ему в~соответствие помеченный мультиграф $G_{\AA} = \langle V, E \rangle$, множество вершин~$V$~которого совпадает с~$\AA$,~а множество ребер есть
%\[
%E = \{e = (a, a') \cond \exists x \in \D \colon I(a, x) \neq I(a', x)\}.
%\]
%Для прямой цепи $\AA$ мультиграф $G_{\AA}$ является цепью.
%
%Каждому ребру мультиграфа взаимно однозначно соответствует объект~${x \in \D}$, поэтому
%далее ребра мультиграфа будем отождествлять с~соответствующими объектами множества~$\D$.
%Мощность множества $\D$ равна $P$.

\subsection{Сведение к задачам на левой и правой последовательностях.}
Рассмотрим классификатор~$a_p$ и~зафиксируем точку $(t, e)\in \Psi_p$.
Относительно $a_p$ прямая последовательность~$\AA$ разбивается на~две:
левую $a_0, a_1, \dots, a_p$ и
правую $a_p, a_{p + 1}, \dots, a_P$.

Сведем задачу вычисления~$D_p(t, e)$ к~нахождению числа разбиений множества ребер левой и правой последовательностей с~некоторыми ограничениями.

\begin{definition}
    Запасом ошибок классификатора~$a$ относительно~$a_p$~на~выборке~$X$~назовем величину~${\Delta_p(a, X) = n(a, X) - n(a_p, X).}$
\end{definition}

\begin{definition}
Будем говорить, что на выборке $X$ классификатор $a_p$ \textit{лучше, чем} $a$, и записывать $a_p \succ_X a$, если из \textit{двух} классификаторов $a$ и $a_p$ метод обучения выбирает~$a_p$.
\end{definition}

Тогда требование к~разбиению $(X,\X)$, чтобы метод обучения из всех классификаторов семейства выбирал $a_p$, записывается в~следующем виде:
\begin{align}
    \label{necessary_and_sufficient_conditions_succ}
    \mu X = a_p  \qquad \Leftrightarrow \qquad \forall a \neq a_p \quad a_p \succ_X a
\end{align}

Будем считать, что из~классификаторов, минимизирующих число ошибок на~обучающей выборке~$X$
и допускающих равное число ошибок на~контрольной выборке~$\X$,
выбирается классификатор с~наибольшим номером.

\begin{lemma}
    \label{lem:necessary_and_sufficient_conditions_delta}
    Классификатор $a_p$ лучше, чем классификатор $a$, на выборке $X$, если и только если выполнено одно из следующих условий:
    \begin{enumerate}[label=\arabic*)]
    \item \label{delta0} $\Delta_p(a, X) > 0$;
    \item \label{deltal} $\Delta_p(a, X) = 0$ и $a$ находится в~левой последовательности и $n(a, \XX) \leqslant n(a_p, \XX)$;
    \item \label{deltar} $\Delta_p(a, X) = 0$ и $a$ находится в~правой последовательности и $n(a, \XX) < n(a_p, \XX)$.
    \end{enumerate}
\end{lemma}


%\begin{lemma}
%Для каждого ${a_p \in \AA}$  необходимым и достаточным условием выполнения события~${[\mu X = a_p]}$, является справедливость отношения $a_p \succ_X a$ для~каждого $a \neq a_p$.
%\end{lemma}
%\begin{proof}
%Зафиксируем обучающую выборку~$X$. Если для всех классификаторов выполнен пункт \ref{delta0} определения отношения $\succ_X$, то утверждение очевидно.
%
%Пусть $a_p$ минимизирует число ошибок на обучающей выборке и существует $a \neq a_p$ с нулевым запасом ошибок на~$X$.
%Если он допускает на контрольной выборке большее число ошибок, чем $a_p$, то метод обучения не выберет $a_p$ по определению ПМЭР.
%Если он находится в левой цепи и $n(a, \XX) \leqslant n(a_p, \XX)$, то, по нашему предположению, метод обучения выберет классификатор с большим номером, то есть $a_p$.
%Если же классификатор $a$ находится в правой цепи, то метод обучения выберет $a$.
%\end{proof}

Обозначим через~$\LL_p$~и~$\RR_p$ множества ребер левой и правой последовательностей соответственно.
Множества~$\LL_p$~и~$\RR_p$~не пересекаются, классификаторы левой последовательности неразличимы на~$\RR_p$,~классификаторы правой последовательности неразличимы на~$\LL_p$.
Выполнение условий леммы~\ref{lem:necessary_and_sufficient_conditions_delta}
не зависит от выбора разбиения множества~$\RR_p$ для классификаторов левой последовательности и
не зависит от~выбора разбиения множества~$\LL_p$ для классификаторов правой последовательности.
Значит, общее число разбиений множества~$\D$,~в~которых метод обучения выбирает~$a_p$,~является произведением числа разбиений множеств~$\LL_p$~и~$\RR_p$,~в~которых~$a_p$~лучше всех классификаторов левой и правой последовательностей соответственно.

Следовательно, верна следующая
\begin{theorem}
    \label{theo:chain_splits_count}
    Для каждого~$p$ для всех $(t, e)\in\Psi_p$ число разбиений множества~$\D$, таких что ${t = |X \cap \D|}$, ${e = n(a_p, X \cap \D)}$ и ${\mu X = a_p}$, равно
    \begin{align}\label{splits_by_t_e_left_right}
        D_p(t, e) = \sum_{t' + t'' = t} \sum_{e' + e'' = e} L_p(t', e') R_p(t'', e''),
    \end{align}
    где
    \begin{align}
    \label{left_chain_splits_count}
        L_p(t', e') &= \# \biggl\{
            (X \cap \LL_p, \X \cap \LL_p) \biggm|
            \begin{array}{l}
                \forall d = 0, \ldots, p,\;\; a_p \succ_X a_d,\; \\
                t' = |X \cap \LL_p|,\;
                e' = n(a_p,\, X \cap \LL_p)
            \end{array}
        \biggr\}, \\
    \label{right_chain_splits_count}
        R_p(t'', e'') &= \# \biggr\{
            (X \cap \RR_p, \X \cap \RR_p) \biggm|
            \begin{array}{l}
                \forall d = p + 1, \ldots, P,\;\; a_p \succ_X a_d,\; \\
                t'' = |X \cap \RR_p|,\;
                e'' = n(a_p,\, X \cap \RR_p)
            \end{array}
        \biggr\},
    \end{align}
    точки $(t', e')$ и $(t'', e'')$ являются элементами множеств $\Psi'_p$ и $\Psi''_p$ соответственно, где
    \begin{align}
        \label{train_errors_left_right_upper_bounds'}
        \Psi'_p &= \bigl\{
            (t', e') \bigm|
            0 \leqslant t' \leqslant \min \{l, |\LL_p| \},
            0 \leqslant e' \leqslant \min \{t',\, n(a_p, \LL_p) \}
        \bigr\}, \\
        \label{train_errors_left_right_upper_bounds''}
        \Psi''_p &= \bigl\{
            (t'', e'') \bigm|
            0 \leqslant t'' \leqslant \min \{l, |\RR_p|\},
            0 \leqslant e'' \leqslant \min \{t'',\, n(a_p, \RR_p) \}
        \bigr\}.
    \end{align}

\end{theorem}

%Обозначим через~$\mu'_p$ метод обучения, задаваемый следующим образом:
%\[
%\forall X \subseteq \XX \quad \mu'_p X =
%\left\{
%\begin{array}{ll}
%\mu X, & \text{если}\; \mu X \; \text{из левой цепи}, \\
%a_p, & \text{иначе}.
%\end{array}
%\right.
%\]
%
%Другими словами, это ПМЭР, который каждой обучающей выборке ставит в~соответствие классификатор левой цепи.
%При~этом в~случае неоднозначности, когда несколько классификаторов допускают наименьшее число ошибок на~обучающей выборке и равное число ошибок на~контрольной, метод обучения~$\mu'_p$ выбирает классификатор с~\emph{наибольшим} номером.
%
%Аналогично, через~$\mu''_p$ обозначим метод обучения, задаваемый следующим образом:
%\[
%\forall X \subset \XX \quad \mu''_p X =
%\left\{
%\begin{array}{ll}
%\mu X, & \text{если}\; \mu X \; \text{из правой цепи}, \\
%a_p, & \text{иначе}.
%\end{array}
%\right.
%\]
%
%Заметим, что метод обучения~$\mu''_p$ в~случае неоднозначности также выбирает классификатор с~наибольшим номером.

%Следовательно, события~${[\mu'_p X = a_p]}$~и~${[\mu''_p X = a_p]}$ независимы и верно
%\[
%[\mu X = a_p] = [\mu'_p X = a_p][\mu''_p X = a_p].
%\]

%\begin{proof}
%
%\end{proof}



%Так мы разбили задачу на две:
%\begin{enumerate}[label=\arabic*)]
%\item найти число разбиений выборки $\LL_p$,~таких, что выполнено $\mu'_p X = a_p$;
%\item найти число разбиений выборки $\RR_p$,~таких, что выполнено $\mu''_p X = a_p$.
%\end{enumerate}

%Напомним, что у~нас имеется дополнительное ограничение на~число элементов из~$X$~в~$\D$ ($|X \cap \D| = t$), а также на~число ошибок $n(a_p,\, X \cap \D) = e$.
%
%Введем обозначения
%\[
%\left.
%\begin{array}{ll}
%t' = |X \cap \LL_p|,  & e' = n(a_p,\, X \cap \LL_p), \\
% t'' = |X \cap \RR_p|, & e'' = n(a_p,\, X \cap \RR_p).
%\end{array}
%\right.
%\]
%
%Очевидно, что $t = t' + t''$, $e = e' + e''$.
%
%Аналогично (\ref{train_errors_upper_bounds}), определим границы изменения параметров:


Назовем разбиения множеств $\LL_p$ и $\RR_p$, удовлетворяющие условиям \eqref{left_chain_splits_count} и \eqref{right_chain_splits_count} соответственно, \emph{допустимыми}.
Итак, для каждого~$p$ задача свелась к~вычислению числа допустимых разбиений~${L_p(t', e')}$ и ${R_p(t'', e'')}$ для всех точек множеств $\Psi'_p$ и $\Psi''_p$.

\medskip
Далее рассматривается случай, когда прямая последовательность~$\AA$ является прямой цепью.
Тогда левая и правая последовательности $\LL_p$ и $\RR_p$ также являются цепями.
%Аналогично, для каждого~$p$~задача для~правой цепи заключается в~нахождении числа~${R_p(t'', e'')}$ допустимых разбиений выборки~$\RR_p$ для~каждой пары параметров~$t''$~и~$e''$, удовлетворяющих неравенствам~(\ref{train_errors_left_right_upper_bounds''}):


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Нахождение числа допустимых разбиений левой и правой цепей.}
%Для данного~$p$ и для~каждой пары параметров~$t'$, $e'$ и $t''$, $e''$, удовлетворяющих неравенствам~(\ref{train_errors_left_right_upper_bounds}), необходимо вычислить~$L_p(t', e')$~и~$R_p(t'', e'')$.
%Перепишем в~эквивалентной форме требование к~разбиению, чтобы метод обучения выбирал классификатор $a_p$.
%
%\begin{definition}
%\emph{Запасом ошибок} классификатора~$a$ относительно~$a_p$~на~выборке~$X$~назовем величину~${\Delta_p(a, X) = n(a, X) - n(a_p, X).}$
%\end{definition}
%
%Подчеркнем, в~чем отличие задач для~левой и правой цепей.
%Мы положили, что из~классификаторов, допускающих наименьшее число ошибок на~обучающей выборке и равное число ошибок на~генеральной выборке, выбирается классификатор с~наибольшим номером.
%Значит, в~левой цепи разрешено, чтобы классификатор, допускающий столько же ошибок, что и~$a_p$,~на~обучающей выборке, допускал столько же ошибок и на~генеральной выборке (в~этом случае будет выбран~$a_p$), тогда как в~правой цепи это запрещено.
%
%Таким образом, получаем
%\begin{lemma}\label{lem:necessary_and_sufficient_conditions_delta}
%Для каждого $a_p \in \AA$ необходимым и достаточным условием выбора классификатора~$a_p$~методом обучения, то есть события $[\mu X = a_p]$, является выполнение для~каждого $a \neq a_p$ одного из~следующих условий:
%\begin{enumerate}[label=\arabic*)]
%\item либо $\Delta_p(a, X) > 0$;
%\item либо $\Delta_p(a, X) = 0$ и $a$ находится в~левой цепи и $n(a, \XX) \leqslant n(a_p, \XX)$;
%\item либо $\Delta_p(a, X) = 0$ и $a$ находится в~правой цепи и $n(a, \XX) < n(a_p, \XX)$.
%\end{enumerate}
%\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Нахождение числа допустимых разбиений множества ребер левой цепи.}
Найдём~$L_p(t', e')$ для~каждого~$p$~в каждой точке~$(t', e') \in \Psi'_p$.
Заметим, что при~${p = 0}$~решение задачи тривиально:
множество $\Psi'_0$ состоит из одной точки~${(0, 0)}$~и~${L_0(0, 0) = 1}$.
Всюду далее считаем $1 \leqslant p \leqslant P$.

Перенумеруем классификаторы так, чтобы последовательность начиналась в~$a_p$ и заканчивалась в~$a_0$.
Обозначим  $\{b_0, \ldots, b_p\}$, где $b_d = a_{p - d}$ для каждого $d = 0, \ldots, p$.
Запас ошибок относительно $a_p$ запишем как $\Delta_0(b_d, X) = \Delta_p(a_{p - d}, X)$ для каждого~$d$.

Левая цепь $\LL_p$ составлена из~возрастающих и убывающих монотонных участков.
Обозначим множество всех ребер
возрастающих монотонных участков цепи через~$\CC_p$,
убывающих монотонных участков цепи~--- через~$\I_p$.
Разумеется, ${\CC_p \sqcup \I_p = \LL_p}$.

Цепь прямая, следовательно, $b_0$~не ошибается на~всех объектах~$\CC_p$, то есть
\begin{equation}
    \label{def:corr_incorr_sets}
    \begin{aligned}
    \CC_p &= \{x \in \LL_p \colon I(b_0, x) = 0 \}, \\
    \I_p  &= \{x \in \LL_p \colon I(b_0, x) = 1 \}.
    \end{aligned}
\end{equation}
Тогда верно, что~${e' = |X \cap \I_p|}$, а~${|X \cap \CC_p| = t' - e'}$.

Заметим, что, поскольку классификаторы левой цепи различимы только на~объектах множества~$\LL_p$, то для~любого классификатора~$b$~из~левой цепи верно
\[
    \Delta_0(b, X) = \Delta_0(b, X \cap \LL_p),
    \quad
    \forall X \subseteq \XX.
\]
Отсюда следует, что, зафиксировав разбиение множества~$\LL_p$, мы определим запас ошибок на~всех соответствующих обучающих выборках~$X$.

%\begin{lemma}
%\label{lem:delta_values_dependence_on_edges_in_train}
%Зафиксируем разбиение выборки~$\LL_p$.
%Пусть классификаторы~$b_d$~и~$b_{d + 1}$~соединены ребром~$x$.
%Тогда
%\begin{enumerate}[label=\arabic*)]
%\item если $x \in \X$, то $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X)$;
%\item если $x \in X \cap \CC_p$, то $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X) + 1$;
%\item если $x \in X \cap \I_p$, то $\Delta_0(b_{d + 1}, X) = \Delta_0(a_{d}, X) - 1$.
%\end{enumerate}
%Другими словами, при~движении по~ребрам цепи от~классификатора $b_0$~к~$b_p$ запас ошибок классификаторов изменяется следующим образом: при переходе по~ребру из~$\X$ запас ошибок не изменяется, при переходе по~ребру из~${X \cap \CC_p}$~увеличивается~на~$1$, при переходе по~ребру из~${X \cap \I_p}$~-- уменьшается на~$1$.
%\end{lemma}
%
%\begin{proof}
%Если $x \in \X, $ то $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X)$, так как запас ошибок зависит только от~$X$.
%
%Пусть $x$ лежит в~$X$.
%Если~$x$~лежит в~возрастающей цепи, то есть в~$\CC_p$, то~$b_{d}$~не ошибается на~этом ребре, тогда как~$b_{d + 1}$~ошибается.
%Тогда $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X) + 1$.
%Если же~$x$~лежит в убывающей цепи, то~$b_{d}$~ошибается на~этом объекте, а~$b_{d + 1}$~-- нет.
%Значит, $\Delta_0(b_{d + 1}, X) = \Delta_0(b_{d}, X) - 1$.
%\end{proof}

%\begin{Corollary}
%\label{coroll:delta_limits}
%Для~каждого ${d = 0, \dots, p}$ выполнено $|\Delta_0(b_{d}, X)| \leqslant |\LL_p|$.
%\end{Corollary}
%\begin{proof}
%Начальное значение запаса ошибок, то есть при~$d = 0$, равно нулю.
%Согласно лемме,  откуда следует утверждение следствия.
%\end{proof}


%\begin{Corollary}
%\label{coroll:match_split_to_path_2D}
%Каждое разбиение выборки~$\LL_p$~однозначно задает на~двумерной сетке~${\{0, \dots, |\LL_p|\} \times \{-|\LL_p|, \dots, |\LL_p|\}}$ траекторию, выходящую из~точки~$(0, 0)$~и образованную переходами трех видов:
%\begin{enumerate}[label=\arabic*)]
%\item из~точки $(d, \Delta)$ в~точку~$(d + 1, \Delta)$~-- <<вправо>>;
%\item из~точки $(d, \Delta)$ в~точку~$(d + 1, \Delta + 1)$~-- <<вправо-вверх>>;
%\item из~точки $(d, \Delta)$ в~точку~$(d + 1, \Delta - 1)$~-- <<вправо-вниз>>.
%\end{enumerate}
%Траектория, соответствующая разбиению~${(X \cap \LL_p, \X \cap \LL_p)}$, проходит через точки~$(d, \Delta)$, где~${\Delta = \Delta_0(b_{d}, X)}$ для~каждого~${d = 0, \dots, p.}$
%\end{Corollary}
%
%\begin{proof}
%Зафиксируем разбиение~$(X \cap \LL_p, \X \cap \LL_p)$~выборки~$\LL_p$.
%Будем идти по~ребрам цепи от~классификатора~$b_0$~к~$b_p$ и каждому переходу по~ребру ставить в~соответствие переход на~двумерной сетке.
%Так мы опишем траекторию, соответствующую разбиению ребер цепи.
%
%Выходим из~точки $(0, 0)$.
%Пусть классификаторы~$b_d$~и~$b_{d + 1}$~соединены ребром~$x$, тогда если~${x \in \X}$, то вдоль траектории выполняется переход вида
%<<вправо>>; если~${x \in X \cap \CC_p}$, то выполняется переход вида <<вправо-вверх>>, и <<вправо-вниз>>, если~${x \in X \cap \I_p}$.
%Данные правила однозначно определяют траекторию, которая, согласно лемме \ref{lem:delta_values_dependence_on_edges_in_train}, действительно проходит через~точки~${(d, \Delta)}$, где~${\Delta = \Delta_0(b_{d}, X)}$, значит, по~следствию~\ref{coroll:delta_limits} и тождеству ${p = |\LL_p|}$, целиком лежит на~двумерной сетке~${\{0, \dots, |\LL_p|\} \times \{-|\LL_p|, \dots, |\LL_p|\}}$.
%\end{proof}


%В предыдущем примере было показано, что можно отсеять часть разбиений, запретив траекториям, им соответствующим, проходить через точки, не~удовлетворяющие лемме~\ref{lem:necessary_and_sufficient_conditions_delta}.

%Кроме того, необходимо следить за~выполнением ограничений, накладываемых параметрами~$t'$ и~$e'$~задачи.
%Для~этого

Введём трехмерную сетку
$\Omega_p = \{0, \dots, |\LL_p|\} \times \{-|\LL_p|, \dots, |\LL_p|\} \times \{0, \dots, |\LL_p|\}$.

\begin{definition}
Определим на~$\Omega_p$ множество~$\TT_p$~траекторий, выходящих из~точки~${(0, 0, 0)}$ и образованных переходами~трех видов:
\begin{enumerate}[label=\arabic*)]
	\item из~точки $(d, \Delta, i)$ в точку~$(d + 1, \Delta, i)$~-- <<вправо>>;
	\item из~точки $(d, \Delta, i)$ в~точку~$(d + 1, \Delta + 1, i)$~-- <<вправо-вверх>>;
	\item из~точки $(d, \Delta, i)$ в~точку~$(d + 1, \Delta - 1, i + 1)$~-- <<вправо-вниз>>;
\end{enumerate}
причем для~каждого~$d$~переход из~точки $(d, \Delta, i)$ удовлетворяет условию: пусть классификаторы~$b_{d}$~и~$b_{d + 1}$~соединены ребром~$x$, тогда
\begin{enumerate}[label=\arabic*)]
	\item если $x \in \CC_p$, то это переход вида <<вправо>> или <<вправо-вверх>>;
	\item если $x \in \I_p$, то это переход вида <<вправо>> или <<вправо-вниз>>.
\end{enumerate}
\end{definition}

%Обобщением следствия~\ref{coroll:match_split_to_path_2D} на~трехмерный случай является
\begin{theorem}
    \label{theo:match_split_to_path_3D}
    Между разбиениями множества~$\LL_p$ и траекториями из~множества~$\TT_p$ имеется взаимно однозначное соответствие.
    Траектория, соответствующая разбиению~${(X \cap \LL_p, \X \cap \LL_p)}$, проходит через точки ${(d, \Delta, i)}$,
    где для~каждого~${d = 0, \dots, p}$ координата~${\Delta = \Delta_0(b_{d}, X)}$,
    а~координата~$i$~равна числу ребер из~$X \cap \I_p$ между~$b_0$ и~$b_{d}$.
\end{theorem}

\begin{proof}
Пусть классификаторы~$b_{d - 1}$~и~$b_{d}$~соединены ребром~$x$.

Если $x \in \X, $ то $\Delta_0(b_{d}, X) = \Delta_0(b_{d - 1}, X)$, так как запас ошибок зависит только от~$X$.

Пусть $x$ лежит в~$X$.
Если~$x$~лежит в~возрастающей цепи, то~$b_{d - 1}$~не ошибается на~этом ребре, тогда как~$b_{d}$~ошибается.
Тогда $\Delta_0(b_{d}, X) = \Delta_0(b_{d - 1}, X) + 1$.
Если же~$x$~лежит в~$\I_p$, то~$b_{d - 1}$~ошибается на~этом объекте, а~$b_{d}$~--- нет.
Значит, $\Delta_0(b_{d}, X) = \Delta_0(b_{d - 1}, X) - 1$.

Поставим в соответствие разбиению множества~$\LL_p$ траекторию по~следующему \mbox{правилу}.
Пусть траектория проходит через~точку~${(d, \Delta, i)}$.
При~${d = 0}$~полагаем, что это точка $(0, 0, 0)$.
Из~этой точки вдоль траектории выполняется переход вида <<вправо>>, если $x \in \X$; <<вправо-вверх>>, если $x \in X \cap \CC_p$; <<вправо-вниз>>, если $x \in X \cap \I_p$.

Тогда для~каждого~$d$ координаты ${\Delta}$ и $i$ имеют смысл, указанный в условии теоремы,
и при~описанных переходах изменяются не~более, чем на~$1$.
Значит, траектория действительно целиком лежит на~сетке~$\Omega_p$ и, следовательно, во~множестве~$\TT_p$ и однозначно определена.

По~тем же правилам каждой траектории из~$\TT_p$ можно однозначно поставить в~соответствие разбиение множества~$\LL_p$.
Значит, отображение из~множества разбиений во~множество траекторий~$\TT_p$ сюръективно и инъективно, то есть оно биективно.
\end{proof}

\begin{example}
На~рис.~\ref{fig:splits_and_paths_match}~на~нижнем графике изображена цепь, где выделены ребра, попавшие в~обучающую выборку.
Такому разбиению ребер цепи соответствует траектория, проекция которой на плоскость $(d, \Delta)$ изображена на~верхнем графике.
В~данном примере траектория проходит через точки, у~которых координата~$\Delta$~отрицательна.
Значит, в~цепи имеются классификаторы с~отрицательным запасом ошибок.
Следовательно, по~лемме~\ref{lem:necessary_and_sufficient_conditions_delta} и условиям~\eqref{necessary_and_sufficient_conditions_succ}, при~таком разбиении классификатор~$b_0$~не~будет выбран методом обучения.
Исключив из~рассмотрения траектории, не удовлетворяющие лемме \ref{lem:necessary_and_sufficient_conditions_delta}, мы отбросим и разбиения, не являющиеся допустимыми.
\end{example}

\begin{figure}[t]
    \shorthandoff{"}
    \begin{center}
        \newlength{\mylength}
        \setlength{\fboxsep}{6pt}
        \setlength{\mylength}{\linewidth}
        \addtolength{\mylength}{-\fboxsep}
        \addtolength{\mylength}{-\fboxrule}

        \fbox{
        \parbox{\mylength}{
        \setlength{\belowdisplayskip}{0cm}
        \begin{xy}<0.7ex,0ex>:
            \POS(2,64)\ar@{.}(38,64)*{}
            \POS(2,66)\ar@{.}(38,66)*{}
            \POS(2,68)\ar@{.}(38,68)*{}
            \POS(2,70)\ar@{.}(38,70)*{}
            \POS(2,72)\ar@{.}(38,72)*{}
            \POS(2, 62)\ar@{.}(2, 90)*{}
            \POS(8, 62)\ar@{.}(8, 90)*{}
            \POS(14, 62)\ar@{.}(14, 90)*{}
            \POS(20, 62)\ar@{.}(20, 90)*{}
            \POS(26, 62)\ar@{.}(26, 90)*{}
            \POS(32, 62)\ar@{.}(32, 90)*{}
			\POS(38, 62)\ar@{.}(38, 90)*{}
            %
            \POS(0.5,68)*@{}, +/d0.5em/*++{b_0}
            \POS(40,64)*@{}, +/u0.5em/*++{b_p}
            \POS(2,68)*@{*}="a0"
            \POS(8,66)*@{*}="a1"\ar@{=}"a0"
            \POS(14,68)*@{*}="a2"\ar@{-}"a1"
            \POS(20,70)*@{*}="a3"\ar@{=}"a2"
            \POS(26,72)*@{*}="a4"\ar@{=}"a3"
            \POS(32,70)*@{*}="a5"\ar@{-}"a4"
            \POS(38,68)*@{*}="a6"\ar@{-}"a5"
            %
            \POS(0,62)\ar@{->}(44,62)*{}
            \POS(43,62),+/u0.75em/*{d}
            \POS(2,62)*@{|}="t0"
            \POS(8,62)*@{|}="t1"
            \POS(14,62)*@{|}="t2"
            \POS(20,62)*@{|}="t3"
            \POS(26,62)*@{|}="t4"
            \POS(32,62)*@{|}="t5"
            \POS(38,62)*@{|}="t6"
            %
            \POS(-5,70)*{\scriptstyle n(b_d,\XX)}
            %
            \POS(0,84)\ar@{->}(44,84)*{}
            \POS(43,84),+/u0.75em/*{d}
            \POS(8,84)*@{|}
            \POS(14,84)*@{|}
            \POS(26,84)*@{|}
            \POS(32,84)*@{|}
            \POS(2,80)\ar@{.}(38,80)*{}
            \POS(2,82)\ar@{.}(38,82)*{}
            \POS(2,84)\ar@{.}(38,84)*{}
            \POS(2,86)\ar@{.}(38,86)*{}
            \POS(2,88)\ar@{.}(38,88)*{}
            \POS(2,90)\ar@{.}(38,90)*{}
            %
            \POS(2,84)*@{*}="a0"
            \POS(8,82)*@{*}="a1"\ar@{-}"a0"
            \POS(14,82)*@{*}="a2"\ar@{-}"a1"
            \POS(20,84)*@{*}="a3"\ar@{-}"a2"
            \POS(26,86)*@{*}="a4"\ar@{-}"a3"
            \POS(32,86)*@{*}="a5"\ar@{-}"a4"
            \POS(38,86)*@{*}="a6"\ar@{-}"a5"
            \POS(-5,86)*{\scriptstyle \Delta_0(b_d,X)}
        \end{xy}
        }
        }

    \end{center}
    \caption{Соответствие разбиения цепи~(нижний график) проекции траектории~(верхний график). Двойными линиями выделены ребра цепи, попавшие в~обучающую выборку.}
    \label{fig:splits_and_paths_match}
\end{figure}

Определим множество
\begin{equation}
    \label{eq:left_paths_domain}
    \Omega'_p = \biggl\{
        (d, \Delta, i) \in \Omega_p \biggm|
        \begin{array}{l}
            0 \leqslant i \leqslant d \,\, \text{и} \,\, |\Delta| \leqslant d \,\, \text{и} \\
            \bigl(
                \text{либо} \,\, \Delta > 0, \,\,
                \text{либо} \,\,
                    \left(
                        \Delta = 0 \,\, \text{и} \,\, n(b_{d}, \XX) \leqslant n(b_0, \XX)
                    \right)
            \bigr)\\
        \end{array}
    \biggr\}.
\end{equation}

\begin{lemma}
\label{lem:left_chain_coordinates_constraints}
    Всякая точка~${(d, \Delta, i)}$ траектории из~$\TT_p$,
    соответствующей допустимому разбиению множества~$\LL_p$,
    принадлежит множеству~${\Omega'_p \subseteq \Omega_p}$.
    %\begin{enumerate}[label=\arabic*)]
    %\item $0 \leqslant i \leqslant d$;
    %\item $\Delta \leqslant d$;
    %\item Либо $\Delta > 0$, либо $\Delta = 0$ и $n(b_{d}, \XX) \leqslant n(b_0, \XX)$.
    %\end{enumerate}
\end{lemma}

\begin{proof}
Выполнение первые двух условий из определения~\eqref{eq:left_paths_domain} является следствием теоремы~\ref{theo:match_split_to_path_3D}.
Третье условие есть повторение условий леммы~\ref{lem:necessary_and_sufficient_conditions_delta}.
\end{proof}

Пусть $T_p(d,\, \Delta,\, i)$ есть число траекторий из~$\TT_p$, соединяющих точку~${(0,\, 0,\, 0)}$ с~${(d,\, \Delta,\, i)}$ и проходящих только через~точки множества~$\Omega'_p$.
Из~правил построения траектории по разбиению множества~$\LL_p$ следует
\begin{lemma}
    \label{lem:left_chain_paths_count}
    В каждой точке~${(d,\, \Delta,\, i)}$~на~трехмерной сетке~$\Omega_p$ величина~${T_p(d,\, \Delta,\, i)}$ вычисляется рекуррентно.
    \begin{enumerate}[label=\arabic*)]
    \item Начальное условие $T_p(0,\, 0,\, 0) = 1$.
    \item Если $(d,\, \Delta,\, i) \notin \Omega'_p$, то $T_p(d,\, \Delta,\, i) = 0$.
    \item Пусть $b_{d - 1}$ и $b_{d}$ соединены ребром $x$. Тогда
        \begin{align}
        T_p(d,\, \Delta,\, i) =
    	\begin{cases}
        	T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta - 1,\, i), & \text{если} \,\, x \in \CC_p, \\
        	T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta + 1,\, i - 1), & \text{если} \,\, x \in \I_p,
    	\end{cases}
    	\label{eq:paths_count}
    	\end{align}
        где множества $\CC_p$ и $\I_p$ определяются по~\eqref{def:corr_incorr_sets}.
    %	\begin{itemize}
    %	\item Если $x \in \CC_p$, то
    %		\[
    %		T_p(d,\, \Delta,\, i) = T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta - 1,\, i).
    %		\]
    %	\item  Если $x \in \I_p$, то
    %		\[
    %		T_p(d,\, \Delta,\, i) = T_p(d - 1,\, \Delta,\, i) + T_p(d - 1,\, \Delta + 1,\, i - 1).
    %		\]
    %	\end{itemize}
    \end{enumerate}
\end{lemma}

\begin{theorem}
    \label{theo:left_chain_splits_count}
    Пусть даны метод ПМЭР $\mu$, множество~$\XX$~мощности~$L$,~объем обучающей выборки~$l$ и прямая цепь~$\AA = \{a_0, \dots, a_P\}$.
    Тогда для~каждого $p = 1, \dots, P$ в каждой точке $(t', e')$ множества $\Psi'_p$,
    определенного в~\eqref{train_errors_left_right_upper_bounds'},
    число~${L_p(t', e')}$ допустимых разбиений множества~$\LL_p$, определяемое по~\eqref{left_chain_splits_count}, равно
    \begin{align*}
        %\label{eq:left_splits_eq_paths}
        L_p(t',\, e') = T_p(|\LL_p|,\, t' - 2e',\, e')
    \end{align*}
    и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, где $b_d = a_{p - d}$ для~каждого~$d$, при краевых условиях~${L_0(0, 0) = 1}$.
\end{theorem}

\begin{proof}
Из теоремы~\ref{theo:match_split_to_path_3D} следует, что
\[
    \Delta_p(a_0, X) = {|X \cap \CC_p| -} {|X \cap \I_p|} = {t' - 2e'}.
\]

Между разбиениями множества ребер левой цепи и траекториями из~$\TT_p$ имеется биекция.
Таким образом, число траекторий, проходящих через точку ${(p,\, t' - 2e',\, e')}$,
равно числу разбиений, удовлетворяющих условиям $t' = |X \cap \LL_p|$ и ${e' = n(a_p, X \cap \LL_p)}$.
Оставив среди них те, которые проходят только через точки множества~$\Omega'_p(t', e')$, мы оставим траектории, соответствующие допустимым разбиениям.
Их число равно~$T_p(|\LL_p|,\, t' - 2e',\, e')$.
\end{proof}

\begin{remark}
Ограничения $i \leqslant e'$ и ${\Delta \leqslant t' - e'}$, являющиеся следствием теоремы~\ref{theo:match_split_to_path_3D},
выполняются автоматически для тех траекторий, которые соединяют точки~$(0, 0, 0)$ и $(p, t' - 2e', e')$.
Действительно, поскольку величины $i$ и $\Delta + i$ не возрастают, значит, не превосходят значений в конечной точке, то есть
$i \leqslant e'$ и
\[
    \Delta + i \leqslant t' - 2e' + e' = t' - e'.
\]
Координата $i \geqslant 0$, значит, $\Delta \leqslant \Delta + i \leqslant t' - e'$. В силу этого замечания, в определение множества $\Omega'_p$  данные ограничения не входят.

%\item $0 \leqslant i \leqslant \min \{ d, e' \}$;
%\item $\Delta \leqslant \min \{ d, t' - e' \}$;
%\item Либо $\Delta > 0$, либо $\Delta = 0$ и $n(b_{d}, \XX) \leqslant n(b_0, \XX)$.
%
\end{remark}

Таким образом, мы научились решать задачу для левой цепи.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Нахождение допустимых разбиений  множества ребер правой цепи.}
Решаем задачу вычисления~$R_p(t'', e'')$ для~каждого~$p$~в каждой точке $(t'', e'') \in \Psi''_p$.
Решение практически повторяет решение задачи для~левой цепи после замены~$\LL_p$ на~$\RR_p$ и~точки~$(t', e')$ на~$(t'', e'')$.
Также имеются краевые условия: при~$p = P$ множество $\Psi''_P = \{(0, 0)\}$ и ${R_P(0, 0) = 1}$.
Далее полагаем, что~${0 \leqslant p \leqslant P - 1}$.

Обозначим классификаторы цепи через $b_d = a_{p + d}$  для каждого ${d = 0, \ldots, P - p}$.
Из леммы~\ref{lem:necessary_and_sufficient_conditions_delta} следует, что для справедливости леммы~\ref{lem:left_chain_paths_count}
для правой цепи множество~$\Omega'_p$ необходимо заменить на~множество~$\Omega''_p$, определяемое следующим образом:
\begin{equation}
    \label{eq:right_paths_domain} \\
    \Omega''_p = \biggl\{
        (d, \Delta, i) \in \Omega_p \biggm|
        \begin{array}{l}
            0 \leqslant i \leqslant d \,\, \text{и} \,\, |\Delta| \leqslant d \,\, \text{и} \\
            \bigl(
                \text{либо} \,\, \Delta > 0, \,\,
                \text{либо} \,\, \left(\Delta = 0 \,\, \text{и} \,\, n(b_{d}, \XX) < n(b_0, \XX)\right)
            \bigr)
        \end{array}
    \biggr\}.
\end{equation}

По аналогии с теоремой~\ref{theo:left_chain_splits_count}, для правой цепи верна следующая теорема.
\begin{theorem}
    \label{theo:right_chain_splits_count}
    Пусть даны метод ПМЭР $\mu$, множество~$\XX$~мощности~$L$,~объем обучающей выборки~$l$ и произвольная прямая цепь~$\AA = \{a_0, \dots, a_P\}$.
    Тогда для~каждого $p = 0, \dots, P - 1$ в каждой точке~$(t'', e'')$ множества $\Psi''_p$,
    определенного в~\eqref{train_errors_left_right_upper_bounds''},
    число~${R_p(t'', e'')}$ допустимых разбиений множества~$\RR_p$, определяемое по~(\ref{right_chain_splits_count}), равно
    \begin{align*}
        R_p(t'', e'') = T_p(|\RR_p|, t'' - 2 e'', e'')
        %\label{eq:right_splits_eq_paths}
    \end{align*}
    и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, с~заменой множества~$\Omega'_p$ на~$\Omega''_p$ и $b_d$ на $a_{p + d}$ для~каждого~$d$. Краевые условия $R_P(0, 0) = 1$.
\end{theorem}

\begin{remark}
    По~лемме~\ref{lem:necessary_and_sufficient_conditions_delta},
    для всех~${d = 0, \dots, P}$ запас ошибок классификатора~$a_d$~цепи должен быть неотрицателен
    для допустимых разбиений множества ребер левой и правой цепи.
    В~частности,
    ${\Delta_p(a_0, X) = t' - 2 e' \geqslant 0}$ и
    ${\Delta_p(a_P, X) = t''- 2 e'' \geqslant 0}$.
    Значит, границы изменения вторых координат точек множеств $\Psi_p$, $\Psi'_p$, $\Psi''_p$ имеют вид
    \noindent
    \begin{align*}
        0 \leqslant e \leqslant    \min \bigl\{\tfrac12t ,\, m_p\bigr\}, \qquad
        0 \leqslant e' \leqslant   \min \bigl\{\tfrac12{t'}, n(a_p, \LL_p)\bigr\}, \qquad
        0 \leqslant e'' \leqslant  \min \bigl\{\tfrac12{t''}, n(a_p, \RR_p)\bigr\}.
    \end{align*}
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Нахождение числа допустимых разбиений множества ребер прямой последовательности.}

Рассмотрим общий случай прямой последовательности~$\AA = \{a_0, \dots, a_P\}$.
Сведем задачу вычисления количества допустимых разбиений левой и правой последовательностей к аналогичным задачам для прямых цепей.

Для этого построим прямую цепь $\AA_c$, такую, что $\AA \subseteq \AA_c$ и первый и последний классификаторы семейств совпадают, следующим образом: для каждого $i$, такого, что $|G_i| > 1$, добавим в последовательность $\AA$ прямую цепь $\GG_i$
\[
    \{a_0, \dots, a_{i - 1}\} \cup \GG_i \cup \{a_{i + 2}, \dots, a_P\},
\]
где прямая цепь $\GG_i$ такова, что первым классификатором цепи является $a_i$, последним~--- $a_{i + 1}$.
Для определенности будем считать, что~$\GG_i$ строится как прямая цепь, составленная из двух монотонных: убывающей цепи длины $n_1$ и возрастающей длины $n_0$, где
\begin{align*}
    n_1 = \# \{x \in G_i \cond I(a_i, x) = 1\}, \\
    n_0 = \# \{x \in G_i \cond I(a_i, x) = 0\}.
\end{align*}
Назовем построенную цепь $\AA_c$ \emph{интерполяцией} последовательности~$\AA$.
Ее длина равна~$|\D|$.

Для каждого ${a_p \in \AA}$ рассмотрим
левую последовательность ${\{a_p, \dots, a_0\} \subseteq \AA}$ и
левую цепь ${\{a_p, \dots, a_0\} \subseteq \AA_c}$.
По построению множества ребер данных семейств совпадают,
вследствие чего множества допустимых разбиений левой цепи и левой последовательности, определенные по \eqref{left_chain_splits_count}, также совпадают.
Вычислим их количество по теоремам \ref{theo:left_chain_splits_count} и \ref{theo:right_chain_splits_count} с единственным отличием.

Согласно \eqref{necessary_and_sufficient_conditions_succ},
условие $a_p \succ_X a$ и условие леммы~\ref{lem:necessary_and_sufficient_conditions_delta}
должны быть выполнены только для $a \in \AA$.
Данное ограничение определяет строение множеств $\Omega'_p$ и $\Omega''_p$,
задаваемых в~\eqref{eq:left_paths_domain}~и~\eqref{eq:right_paths_domain}.
Переопределим их для случая интерполяции последовательности $\AA$:
\begin{align}
    \arraycolsep=0pt
    \label{def:left_paths_domain_interpolation}
    \Omega'_p &= \biggl\{
        (d, \Delta, i) \in \Omega_p \biggm|
        \begin{array}{l}
            b_d \in \AA_c \backslash \AA
            \text{ или }
            \bigl(
                b_d \in \AA \text{ и }
                0 \leqslant i \leqslant d \text{ и }
                |\Delta| \leqslant d \\\qquad
                \text{ и }
                \bigl(
                    \Delta > 0 \text{ или }
                    \left(
                        \Delta = 0 \text{ и }
                        n(b_{d}, \XX) \leqslant n(b_0, \XX)
                    \right)
                \bigr)
            \bigr)
        \end{array}
    \biggr\};
    \\
    \label{def:right_paths_domain_interpolation}
    \Omega''_p &= \biggl\{
        (d, \Delta, i) \in \Omega_p \biggm|
        \begin{array}{l}
            b_d \in \AA_c \backslash \AA
            \text{ или }
            \bigl(
                b_d \in \AA \text{ и }
                0 \leqslant i \leqslant d \text{ и }
                |\Delta| \leqslant d \\\qquad
                \text{ и }
                \bigl(
                    \Delta > 0 \text{ или }
                    \left(
                        \Delta = 0 \text{ и }
                        n(b_{d}, \XX) < n(b_0, \XX)
                    \right)
                \bigr)
            \bigr)
        \end{array}
    \biggr\}.
\end{align}

\begin{theorem}
    \label{theo:interpolation_splits_count}
    Пусть даны метод ПМЭР~$\mu$, множество~$\XX$ мощности~$L$, объем обучающей выборки~$l$ и прямая последовательность~$\AA = \{a_0, \dots, a_P\}$.
    Пусть прямая цепь $\AA_c = \{c_0, \dots, c_{|\D|}\}$ является интерполяцией последовательности $\AA$. Каждому классификатору $a_p \in \AA$ соответствует $c_{i_p} \in \AA_c$.

    Тогда для~каждого $p = 1, \dots, P$ в каждой точке $(t', e')$ множества $\Psi'_p$,
    определенного в~\eqref{train_errors_left_right_upper_bounds'},
    число~${L_p(t', e')}$ допустимых разбиений множества~$\LL_p$, определяемое по~\eqref{left_chain_splits_count}, равно
    \begin{align}
    \label{eq:left_splits_eq_paths}
    L_p(t',\, e') = T_p(|\LL_p|,\, t' - 2e',\, e')
    \end{align}
    и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, где $b_d = c_{i_p - d}$ для~каждого~$d$ и множество $\Omega'_p$ определено по~\eqref{def:left_paths_domain_interpolation}. Краевые условия~${L_0(0, 0) = 1}$.

    Для~каждого $p = 0, \dots, P - 1$ в каждой точке~$(t'', e'')$ множества $\Psi''_p$,
    определенного в~\eqref{train_errors_left_right_upper_bounds''},
    число~${R_p(t'', e'')}$ допустимых разбиений множества~$\RR_p$, определяемое по~\eqref{right_chain_splits_count}, равно
    \begin{align}
    R_p(t'', e'') = T_p(|\RR_p|, t'' - 2 e'', e'')
    \label{eq:right_splits_eq_paths}
    \end{align}
    и вычисляется рекуррентно по правилам, описанным в лемме \ref{lem:left_chain_paths_count}, с~заменой множества~$\Omega'_p$ на~$\Omega''_p$, определенного по \eqref{def:right_paths_domain_interpolation}, и $b_d$ на $c_{i_p + d}$ для~каждого~$d$. Краевые условия $R_P(0, 0) = 1$.
\end{theorem}

\section{Алгоритм вычисления вероятности переобучения и полного скользящего контроля}
%Подставим~(\ref{splits_by_t_e_left_right}) в~формулу вероятности переобучения~(\ref{q_eps_arbitrary}):
%\begin{align*}
%Q_{\eps}
%=
%\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t = 0}^{\hat{t}} \sum_{e = 0}^{\hat{e}} N_p(t, e) \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} L_p(t', e') R_p(t - t', e - e').
%\end{align*}
%
%После перестановки знаков суммирования и замены переменных ${t'' = t - t', \: e'' = e - e'}$ получаем основную теорему для~вычисления вероятности переобучения:
%\begin{theorem}
%Для~метода обучения ПМЭР, генеральной выборки~$\XX$~объема~$L$,~объема обучающей выборки~$l$, произвольной прямой цепи~$\AA$~длины~$P$, точности~$\eps \in (0,\, 1)$, выражение для вероятности переобучения имеет вид
%\begin{align*}
%Q_{\eps}
%=
%\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') \: N_p(t' + t'', e' + e''),
%\end{align*}
%где параметры~$\hat{t}', \: \hat{t}'', \: \hat{e}', \: \hat{e}''$ вычисляются по~(\ref{train_errors_left_right_upper_bounds}), а величины~$N_p(t, e)$,~$L_p(t', e')$ и~$R_p(t'', e'')$ вычисляются по~формуле~\eqref{n_p_arbitrary_chain}, теоремам~\ref{theo:left_chain_splits_count}~и~\ref{theo:right_chain_splits_count} соответственно.
%\label{theo:arbitrary_chain_optimized}
%\end{theorem}
%
%Аналогично, после подстановки~(\ref{splits_by_t_e_left_right}) в~формулу~(\ref{ccv_arbitrary}), перестановки знаков суммирования и замены переменных получаем основную теорему для~вычисления полного скользящего контроля:
%\begin{theorem}
%\label{theo:ccv_arbitrary_optimized}
%Для~метода обучения~{ПМЭР}, генеральной выборки~$\XX$~объема~$L$,~объема обучающей выборки~$l$, произвольной прямой цепи~$\AA$~длины~$P$ выражение для~полного скользящего контроля имеет вид
%\begin{align*}
%CCV
%=
%\frac{1}{(L - l) C_L^l}
%\sum_{p = 0}^{P}
%\sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') F_p(t' + t'', e' + e''),
%\end{align*}
%где величина~$F_p(t, e)$ равна
%\begin{align*}
%F_p(t, e) = \sum_{s = 0}^{\hat{s}} C_m^s C_{L - P - m}^{l - t - s}\bigl(n(a_p, \XX) - s - e \bigr),
%\end{align*}
%параметры~$m, \hat{s}, \hat{t}', \: \hat{t}'', \: \hat{e}', \: \hat{e}''$ вычисляются по~формулам~(\ref{errors_count_definition}),~(\ref{s_upper_bound})~и~(\ref{train_errors_left_right_upper_bounds}), и величины~$L_p(t', e')$ и~$R_p(t'', e'')$ вычисляются по теоремам~\ref{theo:left_chain_splits_count}~и~\ref{theo:right_chain_splits_count} соответственно.
%\end{theorem}

Итак, в теореме \ref{theo:interpolation_splits_count} описан алгоритм нахождения
количества допустимых разбиений множеств ребер правой и левой последовательностей для~каждого~$p$.
Остается подставить найденные значения в формулы~\eqref{splits_by_t_e_left_right}, \eqref{q_eps_arbitrary} и~\eqref{ccv_arbitrary}.
Для~сокращения вычислений по теоремам~\ref{theo:arbitrary_chain} и~\ref{theo:ccv_arbitrary}
для~каждого~$p$ предлагается заранее вычислить $L_p(t', e')$, $R_p(t'', e'')$, $N_p(t, e)$ и $F_p(t, e)$,
после чего сложить полученные значения.
Схема вычислений показана в алгоритме~\ref{alg:q_eps_and_ccv}.

%%\[
%%\left.
%%\begin{array}{rl}
%%Q_{\eps}
%%&=
%%\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') \: N_p(t' + t'', e' + e''), \\
%%CCV
%%&=
%%\frac{1}{(L - l) C_L^l}
%%\sum_{p = 0}^{P}
%%\sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') F_p(t' + t'', e' + e'').
%%\end{array}
%%\right.
%%\]
%\begin{align*}
%Q_{\eps}
%=
%&\frac{1}{C_L^l} \sum_{p = 0}^{P} \sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') \: N_p(t' + t'', e' + e''), \\
%CCV
%=
%&\frac{1}{(L - l) C_L^l}
%\sum_{p = 0}^{P}
%\sum_{t' = 0}^{\hat{t}'} \sum_{e' = 0}^{\hat{e}'} \sum_{t'' = 0}^{\hat{t}''} \sum_{e'' = 0}^{\hat{e}''} L_p(t', e') R_p(t'', e'') F_p(t' + t'', e' + e'').
%\end{align*}

%В~алгоритме~\ref{alg:q_eps_and_ccv} представлен псевдокод вычисления вероятности переобучения и функционала полного скользящего контроля.
%Алгоритм \ref{alg:chain_splits_count} вычисляет количество допустимых разбиений левой или правой цепи.
%В зависимости от вида $Type$ цепи~-- левая или правая~-- используется либо множество $\Omega'_p()$, либо $\Omega''_p()$.
%Все вычисления опираются на~доказанные ранее леммы и теоремы, и все обозначения сохраняются.

\SetAlgorithmName{Алгоритм}{algorithm2e}{Список алгоритмов}
\SetKwInput{Input}{\bf{Вход}}
\SetKwInput{Output}{\bf{Выход}}
\SetKwBlock{Begin}{}{}
\SetKwFor{For}{для}{}{endfor}
\SetKwFor{ForEach}{для всех}{}{endfall}
\SetKw{KwTo}{$\ldots$}
\SetKw{Return}{вернуть}
\SetKwIF{If}{ElseIf}{Else}{если}{то}{иначе если}{иначе}{endif}
%\SetKwFunction{ChainSplitsCount}{ChainSplitsCount}

\begin{algorithm2e}[h]
\Input{матрица ошибок прямой последовательности $\AA = \{a_0, \dots, a_P\}$, параметры $l$, $\eps$.}
\Output{вероятность переобучения $Q_{\eps}$ и полный скользящий контроль CCV.}
%\dontprintsemicolon
%$Q_{\eps}, CCV \longleftarrow 0$\;
%$\D \longleftarrow$
\medskip
построить прямую цепь $\AA_c$~--- интерполяцию последовательности $\AA$\;
определить $m$ по \eqref{errors_count_definition}\;
\ForEach{$p = 0, \ldots, P$}{
    разделить цепь $\AA_c$ на две~--- левую $\{a_p, \dots, a_0\}$ и правую $\{a_p, \dots, a_P\}$\;
    \nllabel{alg:iterate_through_p}
	\ForEach{точек $(t', e')$ множества $\Psi'_p$, определенного по \eqref{train_errors_left_right_upper_bounds'}}{
        \nllabel{alg:left_splits}
        найти $L_p(t', e')$ %число допустимых разбиений множества ребер левой цепи
        по~формулам~\eqref{eq:left_splits_eq_paths},~\eqref{eq:paths_count}~и~\eqref{def:left_paths_domain_interpolation}\;
		\nllabel{alg:left_splits_end}
    }
    \ForEach{точек $(t'', e'')$ множества $\Psi''_p$, определенного по \eqref{train_errors_left_right_upper_bounds''}}{	
        \nllabel{alg:right_splits_begin}
        найти $R_p(t'', e'')$ %число допустимых разбиений множества ребер правой цепи
        по~формулам~\eqref{eq:right_splits_eq_paths},~\eqref{eq:paths_count}~и~\eqref{def:right_paths_domain_interpolation}\;
        \nllabel{alg:right_splits}
	}
	\ForEach{точек $(t, e)$ множества $\Psi_p$, определенного в \eqref{train_errors_upper_bounds}}{
        \nllabel{alg:first_line_neutral}
		вычислить $N_p(t, e)$ по формуле \eqref{n_p_arbitrary_chain}\;
		вычислить $F_p(t, e)$ по формуле \eqref{neutral_error_rate_exp}\;
	}
	\ForEach{точек $(t, e)$ вне множества $\Psi_p$}{
        $N_p(t, e):=0$;~
        $F_p(t, e):=0$\;
        \nllabel{alg:last_line_neutral}
    }
}
	$Q_\eps := \dfrac{1}{C_L^l}
        \sum\limits_{p = 0}^{P} \:
        \sum\limits_{(t', e') \in \Psi'_p} \:
        \sum\limits_{(t'',e'') \in \Psi''_p}
            L_p(t', e') R_p(t'', e'')  N_p(t' + t'', e' + e'') $\;
    \nllabel{alg:qeps}
	$CCV :=  \dfrac{1}{(L - l) C_L^l}
        \sum\limits_{p = 0}^{P} \:
        \sum\limits_{(t', e') \in \Psi'_p} \:
        \sum\limits_{(t'',e'') \in \Psi''_p}
            L_p(t', e') R_p(t'', e'')  F_p(t' + t'', e' + e'')$\;
    \nllabel{alg:ccv}
\caption{Вычисление вероятности переобучения и полного скользящего контро\rlap{ля}}
\label{alg:q_eps_and_ccv}
\end{algorithm2e}
%
%\begin{algorithm2e}[t]
%\ChainSplitsCount{$p, t', e', Type$}{\\
%	\Input{Прямая цепь $\AA = \{a_0, \dots, a_P\}$, параметры $p, \; t'$  и $e'$, вид~$Type$ цепи.}
%	\Output{Число допустимых разбиений левой или правой цепи.}
%	\dontprintsemicolon
%    \lIf{$Type = Left$}{
%    		$\hat{P} \longleftarrow |\LL_p|; \{b_0, \ldots, b_{\hat{P}}\} \longleftarrow \{a_p, \ldots, a_0\}$
%    	}
%	\lElse{
%		$\hat{P} \longleftarrow |\RR_p|; \{b_0, \ldots, b_{\hat{P}}\} \longleftarrow \{a_p, \ldots, a_{P}\}$
%	}
%	\ForEach{$\Delta = -\hat{P} , \ldots, \hat{P} $ {\bf и} $i = 0, \ldots, \hat{P}$}{
%		$T[0, \Delta, i] \longleftarrow 0$
%	}
%	$T[0, 0, 0] \longleftarrow 1$\;
%	\ForEach{$d = 1, \ldots, \hat{P}$}{
%		$x \longleftarrow $ ребро между $b_{d - 1}$ и $b_{d}$\;
%		\ForEach{$\Delta = -\hat{P}, \ldots, \hat{P}$ {\bf и} $i = 0, \ldots, \hat{P}$}{
%			\If {
%			\vskip-3ex
%			\begin{tabular}{b{0.03\textwidth}b{0.97\textwidth}}
%			 &\(0 \displaystyle \leqslant i \leqslant \min\{d, e'\} \; \text{\bf и} \;  -d \leqslant \Delta \leqslant \min\{{d, t' - e'}\} \, \text{\bf и}\)\\
%			 &\( \displaystyle \bigl(\Delta > 0 \; \text{\bf или} \, Type = Left \; \text{\bf и} \; \Delta = 0 \; \text{\bf и} \; n(b_{d}, \XX) \leqslant n(b_0, \XX)\)\\
%			 &\( \displaystyle \text{\bf или} \; Type = Right \; \text{\bf и} \; \Delta = 0 \; \text{\bf и} \; n(b_{d}, \XX) < n(b_0, \XX) \bigr)\)\\
%			 \end{tabular}
%			}
%			{
%				\eIf {$b_0$ не ошибается на $x$}{
%					$T[d, \Delta, i] \longleftarrow T[d - 1, \Delta, i] + T[d - 1, \Delta - 1, i]$\;
%				}
%				{
%					$T[d, \Delta, i] \longleftarrow T[d - 1, \Delta, i] + T[d - 1, \Delta + 1, i - 1]$\;
%				}						
%			}
%			\lElse{
%				$T[d, \Delta, i] \longleftarrow 0$\;
%			}	
%			\vskip-3ex
%		}	
%	}
%	\Return{$T[\hat{P},  t' -2e', e']$}.
%}
%\caption{Нахождение допустимых разбиений левой и правой цепей}
%\label{alg:chain_splits_count}
%\end{algorithm2e}

\subsection{Сложность алгоритма.}
Оценим сложность выполнения шагов~\ref{alg:left_splits}--\ref{alg:last_line_neutral} алгоритма~\ref{alg:q_eps_and_ccv}.
%При вычислении~${L_p(t',\, e')}$ по~теореме~\ref{theo:left_chain_splits_count} значение $T_p(d, \, \Delta, \, i)$ вычисляется в~каждой точке множества~$\Omega'_p$, определенного в~лемме~\ref{lem:left_chain_coordinates_constraints}.

При вычислении $L_p(t', e')$ по теореме \ref{theo:left_chain_splits_count}
на шагах~\ref{alg:left_splits}--\ref{alg:left_splits_end}
один раз для всех $(d, \Delta, i) \in \Omega'_p$ вычисляются $T_p(d, \Delta, i)$,
затем для каждого $(t', e') \in \Psi'_p$ величина $L_p(t', e')$ полагается равной $T_p(d, t' - 2 e', e')$.
Множество~$\Omega'_p$ вложено в~куб со~стороной~$O(|\LL_p|)$,
поскольку каждая координата ограничена по модулю количеством ребер в левой последовательности.
Следовательно, сложность выполнения шагов~\ref{alg:left_splits}--\ref{alg:left_splits_end} составляет~$O(|\LL_p|^3)$.
Аналогично, сложность выполнения шагов \ref{alg:right_splits_begin}--\ref{alg:right_splits} составляет~$O(|\RR_p|)^3)$.

Для~нахождения ${N_p(t, e)}$ и~${F_p(t, e)}$ необходимо вычислить
биномиальные коэффициенты $C_m^i$ и $C_{L - P - m}^i$ при~всех возможных $i$ за~${O(L)}$.
Биномиальные коэффициенты для каждого~$p$ не пересчитываются.
При~известных значениях биномиальных коэффициентов искомые~${N_p(t, e)}$ и~${F_p(t, e)}$ вычисляются за~$O(L)$.
Множество~$\Psi_p$ вложено в квадрат со стороной~$L$, значит,
выполнение шагов \ref{alg:first_line_neutral}--\ref{alg:last_line_neutral} выполняется за~$O(L^3)$.
Следовательно, сложность выполнения шагов~\ref{alg:left_splits}--\ref{alg:last_line_neutral}
составляет $O(|\D|^3 + L^3) = O(L^3)$ для каждого $p$.

Множества $\Psi'_p$ и $\Psi''_p$ вложены в квадрат со стороной $P$,
значит, шаги \ref{alg:qeps}--\ref{alg:ccv} выполняются за~$O(L^5)$,
и сложность алгоритма~\ref{alg:q_eps_and_ccv} также составляет~$O(L^5)$.

%\section{Вычислительные эксперименты}
%Напомним обозначения.
%Дана цепь $\AA = \{a_0, \dots, a_P\}$ на~множестве~$\XX$~мощности~$L$~с~параметром~$m$,~равным числу ошибок классификаторов на~нейтральном множестве.
%Объем обучающей выборки полагается равным~$l$.
%Точность~$\eps$.
%
%\subsection{Произвольная прямая цепь.}
%Для~произвольной прямой цепи~$\AA$~был проведен точный расчет вероятности переобучения двумя способами: по~определению и с~помощью алгоритма.
%Поскольку первый способ расчета требует временных затрат, экспоненциально зависящих от~$L$, параметры принимали небольшие значения:~${L = 30, \, l = 10, \, m = 6}$.
%Точность~$\eps = 0.05$.
%Параметр~$P$~принимал значения от~$0$~до~${L - m.}$
%При~этом результаты расчетов совпали, что подтверждает корректность алгоритма.
%
%
%\subsection{Сравнение с существующими оценками.}
%%Рассмотрим частный случай цепи, для которого существующие оценки вероятности переобучения являются завышенными.
%Рассмотрим задачу из примера 1 со сбалансированными классами, то есть с классами равной мощности.
%Варьирование порога порождает цепь.
%Если порог пробегает не все возможные значения, а, например, только те, которые находятся на границе классов, то возможно возникновение объектов, на которых все классификаторы цепи допускают ошибки.
%Количество таких объектов равно~$m$.
%
%Покажем, что для данной задачи существующие оценки вероятности переобучения и скользящего контроля являются завышенными.
%%Прямая цепь называется \emph{цепью--пилой} с~параметром~$h$,~если она состоит из~чередующихся возрастающих и убывающих монотонных цепей одинаковой длины~$h$~(пример~-- правый верхний график на~рисунке~\ref{fig:various_simple_chains}).
%
%\subsection{Оценки вероятности переобучения.}
%Верна следующая теорема, называемая оценкой Вапника-Червоненкиса \cite{vapnik71uniform}:
%\begin{theorem}
%Для~любого множества~$\XX$ мощности $L$, семейства классификаторов~$\AA$, метода обучения~$\mu$, длины обучающей выборки~$l$ и порога~$\eps \in (0, 1)$
%\begin{align*}
%Q_{\eps} \leqslant |\AA| \max\limits_{m = 1, \dots, L} H_{L}^{l, m} \bigl( \tfrac{l}{L}(m - \eps (L - l)) \bigr).
%\end{align*}
%\end{theorem}
%
%\noindent\textbf{Оценка Соколова.}
%В~рамках комбинаторного подхода была получена оценка, учитывающая \emph{расслоение} и \emph{связность} семейства~\cite{voron11premi}.
%Под~расслоением семейства понимается распределение классификаторов по~частоте ошибок на~множестве~$\XX$.
%Связность предполагает, что для~каждого классификатора в~семействе найдется множество похожих классификаторов, отличающихся от~него только на~одном объекте выборки.
%В~\cite{sokolov13jmlda} эта оценка была улучшена за~счет более тонкого анализа эффекта связности.
%
%Пусть дано семейство классификаторов ${\B = \{b_1, \dots, b_T\}}$ с~известной матрицей ошибок на~множестве~$\XX$.
%На~множестве классификаторов, как~векторов ошибок, существует отношение лексикографического порядка~$\leqslant$.
%Будем говорить, что классификатор~$a$~\emph{предшествует}~$b$~и~записывать~$a \prec b$,~если~$a \leqslant b$~и~расстояние Хемминга между ними равно~$1$.
%Будем называть классификатор $s$ \emph{истоком}, если нет классификаторов~$b$,~таких что~$b \prec s$.
%
%Через~$u(a)$~будем обозначать
%\[u(a) = |\{b \in \B~|~ a \prec b\}|.\]
%
%Через~$n(a)$~будем обозначать число ошибок~$a$~на~множестве~$\XX$.
%
%Пусть даны два классификатора~$a_i$~и~$a_j$. Тогда через~$A_{ij}$~и~$B_{ij}$~будем обозначать множества
%\begin{align*}
%A_{ij} &= \{x \in \XX|~ I(a_i, x) = 0, ~I(a_j, x) = 1\},\\
%B_{ij} &= \{x \in \XX|~ I(a_i, x) = 1, ~ I(a_j, x) = 0\}.
%\end{align*}
%
%Верна следующая теорема, называемая оценкой Соколова~\cite{sokolov13jmlda}:
%\begin{theorem}
%Пусть $S$~-- множество истоков семейства~$\B$. Тогда верна следующая оценка
%\begin{align} \label{sokolov_bound}
%Q_{\eps} \leqslant \sum_{t = 1}^{T} \min\limits_{s \in S} \Biggl\{ \sum_{i = 0}^{\min\{|A_{ts}|,~ |B_{ts}|\}} \frac{C_{|B_{ts}|}^{i} C_{L - u - |B_{ts}|}^{l - u - i}}{C_L^l} H_{L - u - |B_{ts}|}^{l - u - i,~ n - |B_{ts}|}\Bigl( \tfrac{l}{L}(n - \eps (L - l)) - i) \Bigr) \Biggr\}.
%\end{align}
%\end{theorem}
%
%На~рисунке \ref{fig:sokolov_and_vc_bound} в~логарифмической шкале отложены значения оценки Вапника--Червоненкиса и оценки Соколова в~сравнении с~точной верхней оценкой вероятности переобучения цепи.
%Оценка Соколова является точной только в одном случае, когда минимальное количество ошибок совпадает с параметром $m$.
%В~этом случае цепь является унимодальной (рисунок \ref{fig:combined_from_monot_chain}).
%С~увеличением минимального количества ошибок оценка~(\ref{sokolov_bound}) начинает превосходить реальное значение вероятности переобучения.
%Оценка Вапника--Червоненкиса для~рассматриваемой цепи оказывается завышенной при~любом значении минимального количества ошибок.
%
%\begin{figure}[t]
%\centering{
%\includegraphics[width=\linewidth]{probs_240_160.eps}}
%\caption{Сравнение точных значений вероятности переобучения и оценок Соколова и Вапника--Червоненкиса в логарифмической шкале. Горизонтальной линией указано значение $Q_{\eps} = 1$.
%Условия эксперимента: $L = 240$, $l = 160$, $m = 20$, $\eps = 0.05$.
%По~горизонтали отложено минимальное количество ошибок классификаторов.}
%\label{fig:sokolov_and_vc_bound}
%\end{figure}
%
%\subsection{Оценки скользящего контроля.}
%Для частного случая, когда порог пробегает все возможные значения, в работе Гуза~\cite{guz11mbb} были получены верхние и нижние оценки скользящего контроля с полиномиальной вычислительной сложность $O(L^3)$.
%На рисунке~\ref{fig:guz_bound} по вертикали отложены значения нижней и верхней оценок Гуза в сравнении с точными оценками.
%Погрешность оценок Гуза составляет не более~$0.05$, из чего можно сделать вывод о высокой точности оценок.
%
%\begin{figure}[t]
%\centering{
%\includegraphics[width=\linewidth]{ccvs_240_160.eps}}
%\caption{Сравнение точных значений скользящего контроля и нижней и верхней оценок Гуза.
%Условия эксперимента: $L = 240$, $l = 160$, $m = 0$.
%По~горизонтали отложено минимальное количество ошибок классификаторов.}
%\label{fig:guz_bound}
%\end{figure}
%
%%\subsection{Влияние связности на переобученность.}
%%Целью следующего эксперимента было показать, что переобученность классификатора из цепи, выбираемого методом обучения, усредненная по всем обучающим выборкам, существенно ниже переобученности несвязного семейства.
%%
%%Генерировалась прямая цепь $\AA$ заданной формы.
%%Затем методом Монте-Карло вычислялась частота ошибок метода обучения ПМЭР на~обучающей и на~контрольной выборках.
%%Далее в каждом столбце матрицы ошибок~$\AA$ случайным независимым образом переставлялись элементы, вследствие чего нарушалась связность семейства, то есть близость классификаторов в~смысле расстояния Хемминга.
%%Данный способ генерации несвязного семейства ранее использовался в \cite{voron09roai2008}.
%%Для~полученного семейства также вычислялась частота ошибок метода обучения на~обучающей и контрольной выборках.
%%
%%Результаты эксперимента изображены на~рисунке~\ref{fig:simple_shuffle_chains}.
%%В~каждом столбце верхний график соответствует прямой цепи, нижний график~-- семейству, полученному путем нарушения связности в цепи.
%%Переобученность резко возрастает при~нарушении связности семейства.
%%На~переобученность также влияет и~расслоение семейства.
%%В~каждом ряду в~направлении слева направо число классификаторов в нижних слоях возрастает, переобученность при~этом также возрастает.
%%У~случайной цепи после нарушения связности (график в~правом нижнем углу) переобученность максимальна.
%%
%%\begin{figure}[t]
%%	\centering{
%%	\includegraphics[width=\textwidth]{chains_and_shuffled_chains_with_min_error.eps}}
%%\caption{Влияние связности семейства на переобученность. Условия эксперимента~${L = 200,\, l = 100,\, m = 50.}$}
%%\label{fig:simple_shuffle_chains}
%%\end{figure}

\section{Заключение}
Разработан алгоритм вычисления вероятности переобучения и полного скользящего контроля
для~прямых последовательностей классификаторов,
порождаемых элементарными пороговыми правилами при варьировании параметра порога.

Задачей будущего исследования является применение данного алгоритма
для повышения обобщающей способности методов статистического обучения,
в~частности, для совершенствования критериев отбора признаков,
методов поиска логических закономерностей в~данных,
линейных и~логических алгоритмов классификации.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\BibAuthor#1{\emph{#1}}
\def\BibTitle#1{#1}
%\def\BibUrl#1.{\relax}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{gost71uv}
%\bibliography{MachLearn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{10}

\bibitem{vapnik71uniform}
    \BibAuthor{Вапник~В.~Н., Червоненкис~А.~Я.}
    \BibTitle{О равномерной сходимости частот появления событий к~их~вероятностям}~//
    {Теория вероятностей и~ее применения}. "---
    \newblock 1971. "---
    \newblock \CYRT.~16, {\cyr\textnumero}~2. "---
    \newblock {\cyr\CYRS.}~264--280.

\bibitem{boucheron05theory}
    \BibAuthor{Boucheron~S., Bousquet~O., Lugosi~G.}
    \BibTitle{Theory of classification: A survey of some recent advances}~//
    {ESAIM: Probability and Statistics}. "---
    \newblock 2005. "---
    \newblock Vol.~9. "---
    \newblock Pp.~323--375.

\bibitem{koltchinskii11oracle}
    \BibAuthor{Koltchinskii~V.}
    \BibTitle{Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems:
    {\'E}cole d’{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XXXVIII-2008}.
    Lecture Notes in Mathematics. "---
\newblock Springer, 2011.

\bibitem{voron08pria-eng}
    \BibAuthor{Vorontsov~K.~V.}
    \BibTitle{Combinatorial probability and the tightness of generalization bounds}~//
    {Pattern Recognition and Image Analysis}. "---
    \newblock 2008. "---
    \newblock Vol.~18, no.~2. "---
    \newblock Pp.~243--259.

\bibitem{haussler94predicting}
    \BibAuthor{Haussler~D., Littlestone~N., Warmuth~M.~K.}
    \BibTitle{Predicting $\{0,1\}$-functions on randomly drawn points}~//
    {Inf. Comput.} "---
    \newblock December 1994. "---
    \newblock Vol.~115. "---
    \newblock Pp.~248--292.

\bibitem{voron04qualdan}
    \BibAuthor{Воронцов~К.~В.}
    \BibTitle{Комбинаторные оценки качества обучения по прецедентам}~//
    {Доклады РАН}. "---
    \newblock 2004. "---
    \newblock \CYRT.~394, {\cyr\textnumero}~2. "---
    \newblock {\cyr\CYRS.}~175--178.

\bibitem{voron09dan}
    \BibAuthor{Воронцов~К.~В.}
    \BibTitle{Точные оценки вероятности переобучения}~//
    {Доклады РАН}. "---
    \newblock 2009. "---
    \newblock \CYRT.~429, {\cyr\textnumero}~1. "---
    \newblock {\cyr\CYRS.}~15--18.

\bibitem{voron09roai2008}
    \BibAuthor{Vorontsov~K.~V.}
    \BibTitle{Splitting and similarity phenomena in the sets of classifiers and their effect on the probability of overfitting}~//
    {Pattern Recognition and Image Analysis}. "---
    \newblock 2009. "---
    \newblock Vol.~19, no.~3. "---
    \newblock Pp.~412--420.

\bibitem{voron10pria-eng}
    \BibAuthor{Vorontsov~K.~V.}
    \BibTitle{Exact combinatorial bounds on the probability of overfitting for empirical risk minimization}~//
    {Pattern Recognition and Image Analysis}. "---
    \newblock 2010. "---
    \newblock Vol.~20, no.~3. "---
    \newblock Pp.~269--285.

\bibitem{voron11premi}
    \BibAuthor{Vorontsov~K.~V., Ivahnenko~A.~A.}
    \BibTitle{Tight combinatorial generalization bounds for threshold conjunction rules}~//
    4th International Conference on Pattern Recognition and Machine Intelligence (PReMI'11).
    June~27 -- July~1, 2011. "---
    \newblock Lecture Notes in Computer Science. Springer-Verlag, 2011. "---
    \newblock Pp.~66--73.

\bibitem{zhivotovskiy12iip}
    \BibAuthor{Животовский~Н.~К., Воронцов~К.~В.}
    \BibTitle{Критерии точности комбинаторных оценок обобщающей способности}~//
    Интеллектуализация обработки информации (ИОИ-2012): Докл. "---
    \newblock Москва: Торус Пресс, 2012. "---
    \newblock {\cyr\CYRS.}~25--28.

\bibitem{zhuravlev06recognition}
    \BibAuthor{Журавлёв~Ю.~И., Рязанов~В.~В., Сенько~О.~В.}
    \BibTitle{<<Распознавание>>. Математические методы. Программная система. Практические применения}. "---
    \newblock М.:~Фазис, 2006. "---
    \newblock 176~{\cyr\cyrs.}

\bibitem{guz11mbb}
    \BibAuthor{Гуз~И.~С.}
    \BibTitle{Конструктивные оценки полного скользящего контроля для пороговой классификации}~//
    {Математическая биология и биоинформатика}. "---
    \newblock 2011. "---
    \newblock \CYRT.~6, {\cyr\textnumero}~2. "---
    \newblock {\cyr\CYRS.}~173--189.

\bibitem{sokolov13jmlda}
    \BibAuthor{Воронцов~К.~В., Фрей~А.~И., Соколов~Е.~А.}
    \BibTitle{Вычислимые комбинаторные оценки вероятности переобучения}~//
    {Машинное обучение и анализ данных}. "---
    \newblock 2013. "---
    \newblock \CYRT.~1, {\cyr\textnumero}~6. "---
    \newblock {\cyr\CYRS.}~734--743.

\bibitem{frey13jmlda}
    \BibAuthor{Фрей~А.~И., Толстихин~И.~О.}
    \BibTitle{Комбинаторные оценки вероятности переобучения на основе кластеризации и покрытий множества алгоритмов}~//
    {Машинное обучение и анализ данных}. "---
    \newblock 2013. "---
    \newblock \CYRT.~1, {\cyr\textnumero}~6. "---
    \newblock {\cyr\CYRS.}~761--778.

\bibitem{frey14dan}
    \BibAuthor{Фрей~А.~И., Толстихин~И.~О.}
    \BibTitle{Комбинаторные оценки вероятности переобучения на основе покрытий множества алгоритмов}~//
    {Доклады РАН}. "---
    \newblock 2014. "---
    \newblock \CYRT.~455, {\cyr\textnumero}~3. "---
    \newblock {\cyr\CYRS.}~265--268.

\end{thebibliography}


%%%\bigskip
%%%\begin{thebibliography}{11}
%%%%\bibitem{gilbert}
%%%%	\textit{Gilbert J., Mosteller F.}
%%%%	Recognizing the maximum of a sequence~//
%%%%	J. of Amer. Stat. Assoc.~--- 1966.~--- Vol.\,61.~--- P.\,35–73.
%%%
%%%\bibitem{vapnik71uniform}
%%%	\textit{Вапник\;В.\,Н., Червоненкис\;А.\,Я.}
%%%	О равномерной сходимости частот появления событий к их вероятностям~//
%%%	Теория вероятности и ее применения.~--- 1971.~--- Т.\,16, №\,2.~--- С.\,264--280
%%%
%%%\bibitem{boucheron05theory}
%%%    \textit{Boucheron\;S., Bousquet\;O., Lugosi\;G.}
%%%    Theory of classification: A survey of some recent advances~//
%%%    ESAIM: probability and statistics.~---  2005.~--- Vol.\,9(1), P.\,323--375.
%%%
%%%\bibitem{koltchinskii11oracle},
%%%    \textit{Koltchinskii\;V.}
%%%    Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems:
%%%    {\'E}cole d’{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XXXVIII-2008~//
%%%    Lecture Notes in Mathematics, Springer.~--- 2011.
%%%
%%%\bibitem{Voron09pria-eng}
%%%	\textit{Vorontsov K. V.}
%%%	Splitting and similarity phenomena in the sets of classifiers and their effect on the probability of overfitting //
%%%	Pattern Recognition and Image Analysis.~-- 2009.~-- Vol. 19, No.~3.~-- Pp. 412-420.
%%%
%%%%\bibitem{voron2004cmmp}
%%%%    \textit{Vorontsov K. V.}
%%%%    Combinatorial substantiation of learning algorithms //
%%%%    Comp. Maths Math. Phys. — 2004. — Vol. 44, No. 11. — P. 1997–2009.
%%%
%%%\bibitem{haussler94predicting}
%%%    \textit{Haussler\;D., Littlestone\;N., Warmuth\;M.\,K.}
%%%    Predicting $\{0, 1\}$-functions on randomly drawn points~//
%%%    Information and Computation.~--- 1994.~--- Vol.\,115(2).~--- P.\,248--292.
%%%
%%%%\bibitem{kohavi}
%%%%	\textit{Kohavi\,R.}
%%%%	A study of cross-validation and bootstrap for accuracy estimation and model selection~//
%%%%	Proceedings of International Joint Conference on Artificial Intelligence.~---  1995. ~--- P. 1137-1143.
%%%
%%%\bibitem{voron09dan}
%%%	\textit{Воронцов К.\,В.}
%%%	Точные оценки вероятности переобучения~//
%%%	Докл.~РАН, 2009.~--- Т.~429, №~1.~--- С.~15--18
%%%
%%%\bibitem{voron11premi}
%%%	\textit{Vorontsov\;K.\,V., Ivahnenko\;A.\,A.}
%%%	Tight combinatorial generalization bounds for threshold conjunction rules~//
%%%	4-th Int'l Conf. on Pattern Recognition and Machine Intelligence (PReMI'11), June~27~-- July~1, 2011.
%%%    Lecture Notes in Computer Science. Springer-Verlag, 2011.~--- P.\,66--73.
%%%
%%%\bibitem{zhuravlev_ryazanov}
%%%	\textit{Журавлёв\;Ю.\,И., Рязанов\;В.\,В., Сенько\;О.\,В.}
%%%	<<Распознавание>>. Математические методы. Программная система. Практические применения.~--- М.:~ФАЗИС, 2006.~--- 176~с.
%%%
%%%\bibitem{zhivotovskiy12iip}
%%%    \textit{Животовский\;Н.\,К., Воронцов\;К.\,В.}
%%%    Критерии точности комбинаторных оценок обобщающей способности~//
%%%    Интеллектуализация обработки информации (ИОИ-2012): Докл.~--- Москва: Торус Пресс, 2012.~--- С.\,25--28.
%%%
%%%\bibitem{sokolov13jmlda}
%%%	\textit{Воронцов\;К.\,В., Фрей\;А.\,И., Соколов\;Е.\,А.}
%%%	{Вычислимые комбинаторные оценки вероятности переобучения}~//
%%%	Машинное обучение и анализ данных.~--- 2013.~--- T.\,1, №\,6.~--- С.\,734-743.
%%%
%%%\bibitem{frey13jmlda}
%%%	{\it Фрей А.\,И., Толстихин И.\,О.}
%%%	Комбинаторные оценки вероятности переобучения на основе кластеризации и покрытий множества алгоритмов~//
%%%	Машинное обучение и анализ данных.~--- 2013.~--- T.\,1, №\,6.~--- С.\,761-778.
%%%
%%%%\bibitem{haussler}
%%%%    {\it Haussler D., Littlestone N., Warmuth M. K.}
%%%%    Predicting {0, 1}-functions on randomly drawn points~//
%%%%    Information and Computation.~--- 1994.~--- Vol. 115, No. 2.~--- P. 248–292.
%%%
%%%\bibitem{Voron10pria-eng}
%%%	\textit{Vorontsov~K.~V.}
%%%	{Exact combinatorial bounds on the probability of overfitting for empirical risk minimization}~//
%%%	Pattern Recognition and Image Analysis.~--- 2010.~--- Vol.~20, No.~3.~--- P.~269--285.
%%%
%%%%\bibitem{botov_pria}
%%%%	\textit{Botov P. V.}
%%%%	Exact estimates of the probability of overfitting for multidimensional modeling families of algorithms~//
%%%%	Pattern Recognition and Image Analysis.~--- 2010.~--- Vol. 20, No. 4.~--- P. 52–65.
%%%%
%%%%\bibitem{Tolstikhin2010iip}
%%%%	{\it Толстихин И. О.}
%%%%	Вероятность переобучения некоторых разреженных семейств алгоритмов~//
%%%%	Междунар. конф. ИОИ-8. ~--- М:МАКС~Пресс, 2010. ~--- С. 83-86.
%%%%
%%%%\bibitem{Frei10Sym}
%%%%	{\it Frei A.\,I.}
%%%%	Accurate estimates of the generalization ability for symmetric set of predictors and randomized learning algorithms~//
%%%%	Pattern Recognition and Image Analysis.~--- 2010.~--- Vol. 20, No. 3.~--- P. 241–250.
%%%%
%%%%\bibitem{Botov2009MMPR}
%%%%	{\it Ботов П. В.}
%%%%	Точные оценки вероятности переобучения для монотонных и унимодальных семейств алгоритмов~//
%%%%	Всеросс. конф. Математические методы распознавания образов-14.~--- М.: МАКС Пресс, 2009.~--- С. 7–10.
%%%%
%%%%\bibitem{zhuravlev}
%%%%	\textit{Журавлёв\;Ю.\,И.}
%%%%	{Об~алгебраическом подходе к~решению задач распознавания или классификации}~//
%%%%	Проблемы кибернетики:~Вып.\,33.~--- 1978.~--- С.\,5--68.
%%%
%%%\bibitem{guz11mbb}
%%%	\textit{Гуз\;И.\,С.}
%%%	{Конструктивные оценки полного скользящего контроля для пороговой классификации}~//
%%%	Математическая биология и биоинформатика.~--- 2011.~--- Т.\,6, №\,2.~--- С.\,173-189.
%%%
%%%\bibitem{ivahnenko11mmro}
%%%	\textit{Ивахненко\;А.\,А., Воронцов\;К.\,В.}
%%%	{Критерии информативности пороговых логических правил с~поправкой на переобучение порогов}~//
%%%	15-я Всеросс. конф. Математические методы распознавания образов.~--- М.:~МАКС Пресс, 2011.~--- С.~48--51.
%%%
%%%\end{thebibliography}

\end{document} 
